Location,Job Title,Salary Estimate,Employer Name,Job Description,Rating,Company Size,Founded,Type,Industry,Sector,Revenue
"Milwaukee, WI",Senior Data Engineer,$87K - $114K (Glassdoor est.),AE Business Solutions,"AE Business Solutions is seeking a Senior Data Engineer that can deliver high-quality, maintainable, and scalable data solutions. The Senior Data Engineer will work with Solution Architects and other data engineers to develop an enterprise analytics data platform through new and updated data pipelines leveraging shared components and aligning to standards and best practices. This role is focused on collecting data from internal and external sources and transforming it into usable information for the business including data scientists and data analysts. If you have strong experience in AWS services and leveraging data pipelines, this role has a lot to offer.  This position is a 6-month contract to hire position that requires 3 days onsite in Milwaukee. Benefits including PTO and health insurance can be provided throughout the contract duration. How will you make an impact? Deliver high quality data assets to be used by the business to transform business processes and to enable leaders to make data-driven decisions Continuously improve data solutions to increase quality, speed of delivery and trust of data engineering team’s deliverables to enable business outcomes Reduce total cost of ownership of solutions by developing shared components and implementing best practices and coding standards What are we looking for? Ability to translate data engineering designs into working code Data analysis and data engineering pipeline experience including design, development, and support Experience with AWS services including S3, Lambda, EMR, RDS, Glue, Data Pipeline, Redshift and/or other big data technologies. Experience with coding in Python, PySpark, and Terraform Experience with DevOps practices including Continuous Integration, Continuous Delivery, and Infrastructure as Code to deliver end-to-end automation of data delivery. Ability to train and mentor junior data engineers. Experience with Agile engineering practices including the scrum framework You might be a good fit if you like to: Leverage technology to deliver data solutions to drive business outcomes Drive for continuous improvement of processes and solutions Bring energy and commitment to excellence to drive delivery of high-quality solutions Share knowledge and mentor team members on technologies and solutions What you can learn on the job: Gain experience and expertise using AWS cloud technologies through hands-on solution execution alongside solution architects and data engineers Grow practical understanding of agile work practices through application of the scrum framework to deliver work products Partner with others to deliver solutions and build knowledge around other data functions including Information Governance, Information Architecture, and Data Science What does success look like? First 30 days: Understand the existing analytics data platform and technologies Understand team’s work management processes First 6 months: Develop data solutions for the business including shared components and specific data sets Learn business processes and develop necessary contacts throughout the organization First year: Continue to develop data solutions for the business Become expert on the team for other data engineers to go to for mentoring in completing their work Partner with solution architects and lead data engineers to advance capabilities of the platform AE Business Solutions is seeking a Senior Data Engineer that can deliver high-quality, maintainable, and scalable data solutions. The Senior Data Engineer will work with Solution Architects and other data engineers to develop an enterprise analytics data platform through new and updated data pipelines leveraging shared components and aligning to standards and best practices. This role is focused on collecting data from internal and external sources and transforming it into usable information for the business including data scientists and data analysts. If you have strong experience in AWS services and leveraging data pipelines, this role has a lot to offer. This position is a 6-month contract to hire position that requires 3 days onsite in Milwaukee. Benefits including PTO and health insurance can be provided throughout the contract duration. How will you make an impact? How will you make an impact? Deliver high quality data assets to be used by the business to transform business processes and to enable leaders to make data-driven decisions Continuously improve data solutions to increase quality, speed of delivery and trust of data engineering team’s deliverables to enable business outcomes Reduce total cost of ownership of solutions by developing shared components and implementing best practices and coding standards Deliver high quality data assets to be used by the business to transform business processes and to enable leaders to make data-driven decisions Continuously improve data solutions to increase quality, speed of delivery and trust of data engineering team’s deliverables to enable business outcomes Reduce total cost of ownership of solutions by developing shared components and implementing best practices and coding standards What are we looking for? What are we looking for? Ability to translate data engineering designs into working code Data analysis and data engineering pipeline experience including design, development, and support Experience with AWS services including S3, Lambda, EMR, RDS, Glue, Data Pipeline, Redshift and/or other big data technologies. Experience with coding in Python, PySpark, and Terraform Experience with DevOps practices including Continuous Integration, Continuous Delivery, and Infrastructure as Code to deliver end-to-end automation of data delivery. Ability to train and mentor junior data engineers. Experience with Agile engineering practices including the scrum framework Ability to translate data engineering designs into working code Data analysis and data engineering pipeline experience including design, development, and support Experience with AWS services including S3, Lambda, EMR, RDS, Glue, Data Pipeline, Redshift and/or other big data technologies. Experience with coding in Python, PySpark, and Terraform Experience with DevOps practices including Continuous Integration, Continuous Delivery, and Infrastructure as Code to deliver end-to-end automation of data delivery. Ability to train and mentor junior data engineers. Experience with Agile engineering practices including the scrum framework You might be a good fit if you like to: You might be a good fit if you like to: Leverage technology to deliver data solutions to drive business outcomes Drive for continuous improvement of processes and solutions Bring energy and commitment to excellence to drive delivery of high-quality solutions Share knowledge and mentor team members on technologies and solutions Leverage technology to deliver data solutions to drive business outcomes Drive for continuous improvement of processes and solutions Bring energy and commitment to excellence to drive delivery of high-quality solutions Share knowledge and mentor team members on technologies and solutions What you can learn on the job: What you can learn on the job: Gain experience and expertise using AWS cloud technologies through hands-on solution execution alongside solution architects and data engineers Grow practical understanding of agile work practices through application of the scrum framework to deliver work products Partner with others to deliver solutions and build knowledge around other data functions including Information Governance, Information Architecture, and Data Science Gain experience and expertise using AWS cloud technologies through hands-on solution execution alongside solution architects and data engineers Grow practical understanding of agile work practices through application of the scrum framework to deliver work products Partner with others to deliver solutions and build knowledge around other data functions including Information Governance, Information Architecture, and Data Science What does success look like? What does success look like? First 30 days: Understand the existing analytics data platform and technologies Understand team’s work management processes First 6 months: Develop data solutions for the business including shared components and specific data sets Learn business processes and develop necessary contacts throughout the organization First year: Continue to develop data solutions for the business Become expert on the team for other data engineers to go to for mentoring in completing their work Partner with solution architects and lead data engineers to advance capabilities of the platform First 30 days: Understand the existing analytics data platform and technologies Understand team’s work management processes First 30 days: Understand the existing analytics data platform and technologies Understand team’s work management processes Understand the existing analytics data platform and technologies Understand team’s work management processes First 6 months: Develop data solutions for the business including shared components and specific data sets Learn business processes and develop necessary contacts throughout the organization First 6 months: Develop data solutions for the business including shared components and specific data sets Learn business processes and develop necessary contacts throughout the organization Develop data solutions for the business including shared components and specific data sets Learn business processes and develop necessary contacts throughout the organization First year: Continue to develop data solutions for the business Become expert on the team for other data engineers to go to for mentoring in completing their work Partner with solution architects and lead data engineers to advance capabilities of the platform First year: Continue to develop data solutions for the business Become expert on the team for other data engineers to go to for mentoring in completing their work Partner with solution architects and lead data engineers to advance capabilities of the platform Continue to develop data solutions for the business Become expert on the team for other data engineers to go to for mentoring in completing their work Partner with solution architects and lead data engineers to advance capabilities of the platform",4.2,51 to 200 Employees,1949,Company - Private,Computer Hardware Development,Information Technology,$25 to $100 million (USD)
"Phoenix, AZ",Big Data Engineer,$55.00 Per Hour (Employer est.),"Samson Software Solutions, INC","Hi Hope you are doing great! I am Sending this E-mail to you for the position o*f Sr. Big Data Engineer (Local to Phoenix AZ)* o*f Kindly let me know your thoughts along with your updated resume If you are not interested so, please let me know any reference, I would really appreciate your efforts. Job Title - Sr. Big Data Engineer (Local to Phoenix AZ) Job Title - Sr. Big Data Engineer (Local to Phoenix AZ) Location - Phoenix, AZ Location - Phoenix, AZ Job Description: Job Description: Exp.- Minimum 10+ Years’ Experience required. Exp.- Minimum 10+ Years’ Experience required. Job Description:· 10+ years of experience with Big Data technologies such as Hadoop and related eco system. - Job Description: Big Data technologies such as Hadoop and related eco system. - Must Have- Practical experience and in depth understanding of Map reduce.· Hands on experience with spark/hive/pig/flume/sqoop/kafka Good to have.· Should have a good programming background with expertise in Java OR Scala OR Python - Must Map reduce · Hands on experience with spark/hive/pig/flume/sqoop/kafka Good to have. Java OR Scala OR Python - Must have - Comfortable developing Object Oriented, multi-tier applications in a complex architectural landscape· Must have - Comfortable developing Object Oriented, multi-tier applications in a complex architectural landscape Knowledge of bit bucket/git, Maven, Jenkins and Shell scripting - Good to have.· Experience, interest and adaptability to working in an Agile delivery environment· bit bucket/git, Maven, Jenkins and Shell scripting · Experience, interest and adaptability to working in an Agile delivery environment· Ability to select the right tool for the job· Ability to select the right tool for the job· Experience with RESTful services, JSON and XML metadata, containers such as Tomcat/Jetty. - Good to have.· Experience with RESTful services, JSON and XML metadata, containers such as Tomcat/Jetty. - Good to have.· Experience in working on hadoop distribution (CDH/HDP/Map reduce) - Good to have.· Exposure to data engineering ( data exploration, modelling and schema designing)- Good have Experience in working on hadoop distribution (CDH/HDP/Map reduce) - Good to have. data exploration, modelling and schema designing Thanks & Regards, Thanks & Regards Tanya Saxena - US IT Recruiter Tanya Saxena - US IT Recruiter Samson Software Solutions Inc. Samson Software Solutions Inc Email: Tanya.saxena@samsonsoft.com Email: Contact No. – (1001687081) Contact No. – (1001687081) https://www.linkedin.com/in/tanya-saxenahr/ https://www.linkedin.com/in/tanya-saxenahr/ www.Samsonsoft.com Certified WBENC Certified Minority and Women Based Enterprise Certified Small Business Enterprise Top 100 Diversity Owned Businesses in New Jersey Job Type: Contract Pay: From $55.00 per hour Work Location: On the road",N/A,,,,,,
"Nashville, TN",Data Engineer,$79K - $105K (Glassdoor est.),HCA Healthcare,"Introduction Do you want to join an organization that invests in you as a Data Engineer? At HCA Healthcare, you come first. HCA Healthcare has committed up to $300 million in programs to support our incredible team members over the course of three years. Benefits HCA Healthcare, offers a total rewards package that supports the health, life, career and retirement of our colleagues. The available plans and programs include: Comprehensive medical coverage that covers many common services at no cost or for a low copay. Plans include prescription drug and behavioral health coverage as well as free telemedicine services and free AirMed medical transportation. Additional options for dental and vision benefits, life and disability coverage, flexible spending accounts, supplemental health protection plans (accident, critical illness, hospital indemnity), auto and home insurance, identity theft protection, legal counseling, long-term care coverage, moving assistance, pet insurance and more. Free counseling services and resources for emotional, physical and financial wellbeing 401(k) Plan with a 100% match on 3% to 9% of pay (based on years of service) Employee Stock Purchase Plan with 10% off HCA Healthcare stock Family support through fertility and family building benefits with Progyny and adoption assistance. Referral services for child, elder and pet care, home and auto repair, event planning and more Consumer discounts through Abenity and Consumer Discounts Retirement readiness, rollover assistance services and preferred banking partnerships Education assistance (tuition, student loan, certification support, dependent scholarships) Colleague recognition program Time Away From Work Program (paid time off, paid family leave, long- and short-term disability coverage and leaves of absence) Employee Health Assistance Fund that offers free employee-only coverage to full-time and part-time colleagues based on income. Learn more about Employee Benefits Note: Eligibility for benefits may vary by location.  You contribute to our success. Every role has an impact on our patients’ lives and you have the opportunity to make a difference. We are looking for a dedicated Data Engineer like you to be a part of our team. Job Summary and Qualifications Data Engineers within HCA’s Information and Analytics organization are responsible for defining and implementing data management practices across the enterprise. This position will focus primarily on enterprise data management and migrating of data to the cloud. This role requires working closely with the different data teams and requires ‘self-starters’ who are proficient in problem solving and capable of bringing clarity to complex situations. Data Engineers are expected to source and incorporate new data sources into the Enterprise Data Ecosystem. The responsibilities will include writing, testing, and reviewing ETL pipelines for defining and implementing data management practices across the enterprise This candidate will have a history of increasing responsibility in a small multi-role team. This position requires a candidate who can analyze business requirements, perform design tasks, construct, test, and implement solutions with minimal supervision. This candidate will have a record of accomplishment of participation in successful projects in a fast-paced, mixed team environment. Major Responsibilities: Responsible for building and supporting a Cloud based ecosystem designed for enterprise-wide analysis of structured, semi-structured, and unstructured data. Direct the transformation from HCA Healthcare’s current on premise Teradata platform to Google Cloud Platform to enable analytics and machine learning at scale. Implement enterprise data management practices, standards, and frameworks for Data Integration Develop, manage, and own full data lifecycle from raw data acquisition through transformation to end user consumption. Analyze requirements, design data pipelines and integrate those solutions for customer environments Translate business requirements into technical design specifications Closely collaborates with team members to successfully execute development initiatives using Agile practices and principles Maintains a holistic view of information assets by creating and maintaining artifacts that illustrate how information is stored, processed, and accessed Provide guidance on technology choices and design considerations for migrating data to the Cloud Experience with building consumable data lakes, analytics applications and tools Designing the cloud environment from a comprehensive perspective, ensuring that it satisfies all of the company’s needs. Performs activities such as deployment, maintenance, monitoring, and management inside the cloud framework that has been created Work closely with individuals across the technology organizations to help promote awareness of the data architecture and ensure that enterprise assets of competence are leveraged Education & Experience: Bachelor's degree required Master's degree preferred 7+ years of experience in Information Technology required 3+ years of experience in Cloud Technologies required Knowledge, Skills, Abilities, Behaviors: Teradata ETL experience using BTEQ and SQL scripts. Extensive experience with relational database management systems; Teradata, Oracle or SQL Server preferred. Ability to troubleshoot, maintain, reverse engineer, and optimize existing ETL pipelines Advanced SQL skills, including the ability to write, tune, and interpret SQL queries Experience developing and supporting data pipelines from various source types (on-prem RDBMS, AWS, GCS bucket, flat file) to Big Query utilizing Google Cloud Platform native technologies Scripting experience with Python/Unix/Linux. Experience with Git and GitHub version control. Experience with relational databases such as Teradata and public cloud technologies such as, GCP Big Query, GCP Data Catalog and Azure Data Bricks preferred Experience with Cloud Data Flow, Airflow, Cloud Composer, Streamsets or managing streaming data is strongly preferred Ability to troubleshoot, maintain, reverse engineer and optimize existing ETL pipelines. Experience with Cloud Data Flow, Airflow, Cloud Composer, Cloud Data Fusion, Data Catalog, gsutil, GCS, Pub/Sub, Kafka, DataProc, github Experience with handling streaming data is strongly preferred NoSQL, Hbase, Cassandra, MongoDB, In-memory, Columnar, other emerging technologies Ability to analyze and interpret complex data, and offer solutions to complex clinical problems. Ability to work independently on assigned tasks. Strong written and verbal communication skills including the ability to explain complex technical issues in a way that non-technical people may understand. Excellent problem-solving and critical thinking skills. Knowledge of IT governance and operations. HCA Healthcare has been recognized as one of the World’s Most Ethical Companies® by the Ethisphere Institute more than ten times. In recent years, HCA Healthcare spent an estimated $3.7 billion in cost for the delivery of charitable care, uninsured discounts, and other uncompensated expenses.  ""Good people beget good people.""- Dr. Thomas Frist, Sr. HCA Healthcare Co-Founder We are a family 270,000 dedicated professionals! Our Talent Acquisition team is reviewing applications for our Data Engineer opening. Qualified candidates will be contacted for interviews. Submit your resume today to join our community of caring! We are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status. Introduction Introduction Do you want to join an organization that invests in you as a Data Engineer? At HCA Healthcare, you come first. HCA Healthcare has committed up to $300 million in programs to support our incredible team members over the course of three years. Benefits Benefits HCA Healthcare, offers a total rewards package that supports the health, life, career and retirement of our colleagues. The available plans and programs include: Comprehensive medical coverage that covers many common services at no cost or for a low copay. Plans include prescription drug and behavioral health coverage as well as free telemedicine services and free AirMed medical transportation. Additional options for dental and vision benefits, life and disability coverage, flexible spending accounts, supplemental health protection plans (accident, critical illness, hospital indemnity), auto and home insurance, identity theft protection, legal counseling, long-term care coverage, moving assistance, pet insurance and more. Free counseling services and resources for emotional, physical and financial wellbeing 401(k) Plan with a 100% match on 3% to 9% of pay (based on years of service) Employee Stock Purchase Plan with 10% off HCA Healthcare stock Family support through fertility and family building benefits with Progyny and adoption assistance. Referral services for child, elder and pet care, home and auto repair, event planning and more Consumer discounts through Abenity and Consumer Discounts Retirement readiness, rollover assistance services and preferred banking partnerships Education assistance (tuition, student loan, certification support, dependent scholarships) Colleague recognition program Time Away From Work Program (paid time off, paid family leave, long- and short-term disability coverage and leaves of absence) Employee Health Assistance Fund that offers free employee-only coverage to full-time and part-time colleagues based on income. Comprehensive medical coverage that covers many common services at no cost or for a low copay. Plans include prescription drug and behavioral health coverage as well as free telemedicine services and free AirMed medical transportation. Additional options for dental and vision benefits, life and disability coverage, flexible spending accounts, supplemental health protection plans (accident, critical illness, hospital indemnity), auto and home insurance, identity theft protection, legal counseling, long-term care coverage, moving assistance, pet insurance and more. Free counseling services and resources for emotional, physical and financial wellbeing 401(k) Plan with a 100% match on 3% to 9% of pay (based on years of service) Employee Stock Purchase Plan with 10% off HCA Healthcare stock Family support through fertility and family building benefits with Progyny and adoption assistance. Referral services for child, elder and pet care, home and auto repair, event planning and more Consumer discounts through Abenity and Consumer Discounts Retirement readiness, rollover assistance services and preferred banking partnerships Education assistance (tuition, student loan, certification support, dependent scholarships) Colleague recognition program Time Away From Work Program (paid time off, paid family leave, long- and short-term disability coverage and leaves of absence) Employee Health Assistance Fund that offers free employee-only coverage to full-time and part-time colleagues based on income. Learn more about Employee Benefits Note: Eligibility for benefits may vary by location. Note: Eligibility for benefits may vary by location. Note: Eligibility for benefits may vary by location. You contribute to our success. Every role has an impact on our patients’ lives and you have the opportunity to make a difference. We are looking for a dedicated Data Engineer like you to be a part of our team. Job Summary and Qualifications Job Summary and Qualifications Data Engineers within HCA’s Information and Analytics organization are responsible for defining and implementing data management practices across the enterprise. This position will focus primarily on enterprise data management and migrating of data to the cloud. This role requires working closely with the different data teams and requires ‘self-starters’ who are proficient in problem solving and capable of bringing clarity to complex situations. Data Engineers are expected to source and incorporate new data sources into the Enterprise Data Ecosystem. The responsibilities will include writing, testing, and reviewing ETL pipelines for defining and implementing data management practices across the enterprise This candidate will have a history of increasing responsibility in a small multi-role team. This position requires a candidate who can analyze business requirements, perform design tasks, construct, test, and implement solutions with minimal supervision. This candidate will have a record of accomplishment of participation in successful projects in a fast-paced, mixed team environment. Major Responsibilities: Responsible for building and supporting a Cloud based ecosystem designed for enterprise-wide analysis of structured, semi-structured, and unstructured data. Direct the transformation from HCA Healthcare’s current on premise Teradata platform to Google Cloud Platform to enable analytics and machine learning at scale. Implement enterprise data management practices, standards, and frameworks for Data Integration Develop, manage, and own full data lifecycle from raw data acquisition through transformation to end user consumption. Analyze requirements, design data pipelines and integrate those solutions for customer environments Translate business requirements into technical design specifications Closely collaborates with team members to successfully execute development initiatives using Agile practices and principles Maintains a holistic view of information assets by creating and maintaining artifacts that illustrate how information is stored, processed, and accessed Provide guidance on technology choices and design considerations for migrating data to the Cloud Experience with building consumable data lakes, analytics applications and tools Designing the cloud environment from a comprehensive perspective, ensuring that it satisfies all of the company’s needs. Performs activities such as deployment, maintenance, monitoring, and management inside the cloud framework that has been created Work closely with individuals across the technology organizations to help promote awareness of the data architecture and ensure that enterprise assets of competence are leveraged Responsible for building and supporting a Cloud based ecosystem designed for enterprise-wide analysis of structured, semi-structured, and unstructured data. Direct the transformation from HCA Healthcare’s current on premise Teradata platform to Google Cloud Platform to enable analytics and machine learning at scale. Implement enterprise data management practices, standards, and frameworks for Data Integration Develop, manage, and own full data lifecycle from raw data acquisition through transformation to end user consumption. Analyze requirements, design data pipelines and integrate those solutions for customer environments Translate business requirements into technical design specifications Closely collaborates with team members to successfully execute development initiatives using Agile practices and principles Maintains a holistic view of information assets by creating and maintaining artifacts that illustrate how information is stored, processed, and accessed Provide guidance on technology choices and design considerations for migrating data to the Cloud Experience with building consumable data lakes, analytics applications and tools Designing the cloud environment from a comprehensive perspective, ensuring that it satisfies all of the company’s needs. Performs activities such as deployment, maintenance, monitoring, and management inside the cloud framework that has been created Work closely with individuals across the technology organizations to help promote awareness of the data architecture and ensure that enterprise assets of competence are leveraged Education & Experience: Bachelor's degree required Master's degree preferred 7+ years of experience in Information Technology required 3+ years of experience in Cloud Technologies required Bachelor's degree required Master's degree preferred 7+ years of experience in Information Technology required 3+ years of experience in Cloud Technologies required Knowledge, Skills, Abilities, Behaviors: Teradata ETL experience using BTEQ and SQL scripts. Extensive experience with relational database management systems; Teradata, Oracle or SQL Server preferred. Ability to troubleshoot, maintain, reverse engineer, and optimize existing ETL pipelines Advanced SQL skills, including the ability to write, tune, and interpret SQL queries Experience developing and supporting data pipelines from various source types (on-prem RDBMS, AWS, GCS bucket, flat file) to Big Query utilizing Google Cloud Platform native technologies Scripting experience with Python/Unix/Linux. Experience with Git and GitHub version control. Experience with relational databases such as Teradata and public cloud technologies such as, GCP Big Query, GCP Data Catalog and Azure Data Bricks preferred Experience with Cloud Data Flow, Airflow, Cloud Composer, Streamsets or managing streaming data is strongly preferred Ability to troubleshoot, maintain, reverse engineer and optimize existing ETL pipelines. Experience with Cloud Data Flow, Airflow, Cloud Composer, Cloud Data Fusion, Data Catalog, gsutil, GCS, Pub/Sub, Kafka, DataProc, github Experience with handling streaming data is strongly preferred NoSQL, Hbase, Cassandra, MongoDB, In-memory, Columnar, other emerging technologies Ability to analyze and interpret complex data, and offer solutions to complex clinical problems. Ability to work independently on assigned tasks. Strong written and verbal communication skills including the ability to explain complex technical issues in a way that non-technical people may understand. Excellent problem-solving and critical thinking skills. Teradata ETL experience using BTEQ and SQL scripts. Extensive experience with relational database management systems; Teradata, Oracle or SQL Server preferred. Ability to troubleshoot, maintain, reverse engineer, and optimize existing ETL pipelines Advanced SQL skills, including the ability to write, tune, and interpret SQL queries Experience developing and supporting data pipelines from various source types (on-prem RDBMS, AWS, GCS bucket, flat file) to Big Query utilizing Google Cloud Platform native technologies Scripting experience with Python/Unix/Linux. Experience with Git and GitHub version control. Experience with relational databases such as Teradata and public cloud technologies such as, GCP Big Query, GCP Data Catalog and Azure Data Bricks preferred Experience with Cloud Data Flow, Airflow, Cloud Composer, Streamsets or managing streaming data is strongly preferred Ability to troubleshoot, maintain, reverse engineer and optimize existing ETL pipelines. Experience with Cloud Data Flow, Airflow, Cloud Composer, Cloud Data Fusion, Data Catalog, gsutil, GCS, Pub/Sub, Kafka, DataProc, github Experience with handling streaming data is strongly preferred NoSQL, Hbase, Cassandra, MongoDB, In-memory, Columnar, other emerging technologies Ability to analyze and interpret complex data, and offer solutions to complex clinical problems. Ability to work independently on assigned tasks. Strong written and verbal communication skills including the ability to explain complex technical issues in a way that non-technical people may understand. Excellent problem-solving and critical thinking skills. Knowledge of IT governance and operations. Knowledge of IT governance and operations. HCA Healthcare has been recognized as one of the World’s Most Ethical Companies® by the Ethisphere Institute more than ten times. In recent years, HCA Healthcare spent an estimated $3.7 billion in cost for the delivery of charitable care, uninsured discounts, and other uncompensated expenses. ""Good people beget good people.""- Dr. Thomas Frist, Sr. HCA Healthcare Co-Founder We are a family 270,000 dedicated professionals! Our Talent Acquisition team is reviewing applications for our Data Engineer opening. Qualified candidates will be contacted for interviews. Submit your resume today to join our community of caring! Submit your resume today to join our community of caring! We are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status. We are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.",3.3,10000+ Employees,1968,Company - Public,Health Care Services & Hospitals,Healthcare,$10+ billion (USD)
"Maryland Heights, MO",Data Engineer,$75K - $104K (Glassdoor est.),Lexicon,"Lexicon Data Engineer The Lexicon Data Engineer will develop and optimize data pipelines for scalability and performance for datasets of all sizes. As the Data Engineer, you will work closely with the Database Administrator to build and maintain the Lexicon data warehouse; you will support data science by preparing data for data mining, modeling, and reporting; and you will support software development by assisting with database development and data migration efforts. About Us Lexicon is a legal services and technology provider with deep expertise in the legal industry. We provide a world-class practice management software suite, enabling attorneys to maximize productive use of their time when working cases. With expertise in marketing for law firms, revenue optimization, billing and collections, support services, and more, Lexicon is your trusted partner for all legal practice needs. This is a Permanent (Hybrid), Full-time opportunity. 1099/C2C employees will not be considered.  Qualifications Degree in Computer Science, IT or similar field from an accredited institution required, Master’s degree is a plus 5+ years’ experience as a Data Engineer or in a similar role Strong T-SQL and data management skills Experience with Data Model design and Data warehouse concepts Experience with Azure Data Warehouse, Azure Data Factory, SQL Server Integration Services (SSIS), SQL Server Analysis Services (SSAS) Azure Data Engineer certification is a plus Functional knowledge of programming languages (e.g., .NET framework, PowerShell, Python, R) Technical expertise with data mining and machine learning techniques is preferred Experience with PowerBI a plus Familiarity with NoSQL data structures and search engine optimization is a plus Strong communication, analytical, and numerical skills Responsibilities Develop algorithms to transform data into useful, actionable information. Identify and acquire new data sources and assemble complex datasets that align with business needs. Create and maintain data pipeline infrastructure for optimal extraction, transformation, and loading of data from various data sources using Azure and SQL technologies. Identify, design, and implement internal process improvements including re-designing infrastructure for greater scalability, optimizing data delivery, and automating manual processes. Implement processes and systems to monitor data quality, troubleshoot and resolve data related issues, and improve data reliability and quality. Work with stakeholders including the executive leadership, data science, and software development to evaluate business objectives, support data infrastructure needs, and assist with data-related technical issues. Collaborate with data science team to improve data models that feed business intelligence tools, increase data accessibility, and foster data-driven decision making across the organization. Ensure compliance with data governance and security policies Maintain documentation for database and data pipeline infrastructure Lexicon provides exceptional benefits and a great working environment including: Participate in our Wellness Program and earn 100% Employer paid health premiums Employer paid dental premiums Employer paid Life, LTD & STD premiums 401k & Profit Sharing Flexible spending plans & More! Lexicon Data Engineer Lexicon Data Engineer The Lexicon Data Engineer will develop and optimize data pipelines for scalability and performance for datasets of all sizes. As the Data Engineer, you will work closely with the Database Administrator to build and maintain the Lexicon data warehouse; you will support data science by preparing data for data mining, modeling, and reporting; and you will support software development by assisting with database development and data migration efforts. About Us About Us Lexicon is a legal services and technology provider with deep expertise in the legal industry. We provide a world-class practice management software suite, enabling attorneys to maximize productive use of their time when working cases. With expertise in marketing for law firms, revenue optimization, billing and collections, support services, and more, Lexicon is your trusted partner for all legal practice needs. This is a Permanent (Hybrid), Full-time opportunity. 1099/C2C employees will not be considered. This is a Permanent (Hybrid), Full-time opportunity. 1099/C2C employees will not be considered. This is a Permanent (Hybrid), Full-time opportunity. 1099/C2C employees will not be considered. This is a Permanent (Hybrid), Full-time opportunity. 1099/C2C employees will not be considered. Qualifications Qualifications Degree in Computer Science, IT or similar field from an accredited institution required, Master’s degree is a plus 5+ years’ experience as a Data Engineer or in a similar role Strong T-SQL and data management skills Experience with Data Model design and Data warehouse concepts Experience with Azure Data Warehouse, Azure Data Factory, SQL Server Integration Services (SSIS), SQL Server Analysis Services (SSAS) Azure Data Engineer certification is a plus Functional knowledge of programming languages (e.g., .NET framework, PowerShell, Python, R) Technical expertise with data mining and machine learning techniques is preferred Experience with PowerBI a plus Familiarity with NoSQL data structures and search engine optimization is a plus Strong communication, analytical, and numerical skills Degree in Computer Science, IT or similar field from an accredited institution required, Master’s degree is a plus 5+ years’ experience as a Data Engineer or in a similar role Strong T-SQL and data management skills Experience with Data Model design and Data warehouse concepts Experience with Azure Data Warehouse, Azure Data Factory, SQL Server Integration Services (SSIS), SQL Server Analysis Services (SSAS) Azure Data Engineer certification is a plus Functional knowledge of programming languages (e.g., .NET framework, PowerShell, Python, R) Technical expertise with data mining and machine learning techniques is preferred Experience with PowerBI a plus Familiarity with NoSQL data structures and search engine optimization is a plus Strong communication, analytical, and numerical skills Responsibilities Responsibilities Develop algorithms to transform data into useful, actionable information. Identify and acquire new data sources and assemble complex datasets that align with business needs. Create and maintain data pipeline infrastructure for optimal extraction, transformation, and loading of data from various data sources using Azure and SQL technologies. Identify, design, and implement internal process improvements including re-designing infrastructure for greater scalability, optimizing data delivery, and automating manual processes. Implement processes and systems to monitor data quality, troubleshoot and resolve data related issues, and improve data reliability and quality. Work with stakeholders including the executive leadership, data science, and software development to evaluate business objectives, support data infrastructure needs, and assist with data-related technical issues. Collaborate with data science team to improve data models that feed business intelligence tools, increase data accessibility, and foster data-driven decision making across the organization. Ensure compliance with data governance and security policies Maintain documentation for database and data pipeline infrastructure Develop algorithms to transform data into useful, actionable information. Identify and acquire new data sources and assemble complex datasets that align with business needs. Create and maintain data pipeline infrastructure for optimal extraction, transformation, and loading of data from various data sources using Azure and SQL technologies. Identify, design, and implement internal process improvements including re-designing infrastructure for greater scalability, optimizing data delivery, and automating manual processes. Implement processes and systems to monitor data quality, troubleshoot and resolve data related issues, and improve data reliability and quality. Work with stakeholders including the executive leadership, data science, and software development to evaluate business objectives, support data infrastructure needs, and assist with data-related technical issues. Collaborate with data science team to improve data models that feed business intelligence tools, increase data accessibility, and foster data-driven decision making across the organization. Ensure compliance with data governance and security policies Maintain documentation for database and data pipeline infrastructure Lexicon provides exceptional benefits and a great working environment including: Participate in our Wellness Program and earn 100% Employer paid health premiums Employer paid dental premiums Employer paid Life, LTD & STD premiums 401k & Profit Sharing Flexible spending plans & More! Lexicon provides exceptional benefits and a great working environment including: Lexicon provides exceptional benefits and a great working environment including: Participate in our Wellness Program and earn 100% Employer paid health premiums Employer paid dental premiums Employer paid Life, LTD & STD premiums 401k & Profit Sharing Flexible spending plans & More!",3.2,51 to 200 Employees,2008,Company - Private,Legal,Legal,Unknown / Non-Applicable
"Nashville, TN",Data Science Engineer,$85K - $115K (Glassdoor est.),HCA Healthcare,"Introduction Do you want to join an organization that invests in you as a(an) Data Science Engineer? At HCA Healthcare, you come first. HCA Healthcare has committed up to $300 million in programs to support our incredible team members over the course of three years. Benefits HCA Healthcare, offers a total rewards package that supports the health, life, career and retirement of our colleagues. The available plans and programs include: Comprehensive medical coverage that covers many common services at no cost or for a low copay. Plans include prescription drug and behavioral health coverage as well as free telemedicine services and free AirMed medical transportation. Additional options for dental and vision benefits, life and disability coverage, flexible spending accounts, supplemental health protection plans (accident, critical illness, hospital indemnity), auto and home insurance, identity theft protection, legal counseling, long-term care coverage, moving assistance, pet insurance and more. Free counseling services and resources for emotional, physical and financial wellbeing 401(k) Plan with a 100% match on 3% to 9% of pay (based on years of service) Employee Stock Purchase Plan with 10% off HCA Healthcare stock Family support through fertility and family building benefits with Progyny and adoption assistance. Referral services for child, elder and pet care, home and auto repair, event planning and more Consumer discounts through Abenity and Consumer Discounts Retirement readiness, rollover assistance services and preferred banking partnerships Education assistance (tuition, student loan, certification support, dependent scholarships) Colleague recognition program Time Away From Work Program (paid time off, paid family leave, long- and short-term disability coverage and leaves of absence) Employee Health Assistance Fund that offers free employee-only coverage to full-time and part-time colleagues based on income. Learn more about Employee Benefits Note: Eligibility for benefits may vary by location.  You contribute to our success. Every role has an impact on our patients’ lives and you have the opportunity to make a difference. We are looking for a dedicated Data Science Engineer like you to be a part of our team. Job Summary Position Summary This engineer delivers on development commitments from start to finish for assigned component of the Data Science organization within Information Technology Group (ITG). This technically focused position is responsible for designing, developing, testing and deploying components of our data products across the Enterprise. Major Responsibilities This engineer can quickly learn and maintain existing solutions as well as new development. They will provide key problem resolutions for production systems as needed. They have an in depth understanding of the services provided by Information Technology Group, Accelerated Technologies (AT) and can develop relationships throughout the organization to assist in accomplishing its goals for the company. This engineer strategically designs, constructs, and implements software in a software development environment. This includes selecting, gathering requirements for, designing, and implementing solutions for consumers throughout the enterprise. This engineer is a highly motivated self-starter and is committed to delivering high quality solutions within agreed upon timelines. This table illustrates the relative experience/expertise of the Data Science Engineer. Development Activities Build life-changing healthcare technology Act as technical developer within AT and project integrations, including requirements gathering, design, development, and testing Participate in requirements validation and feasibility analysis with respect to AT Participate in creation of design specification that will enable day to day build activities and troubleshooting Estimate work effort required in delivering features keeping DS capabilities in mind Produce modular, reusable code that incorporates coding best practices and serves as an example for less experienced developers Write code and assist in development of new products/features and enhance and/or maintain existing ones Help design and execute DevOps strategies and processes, driving the change management which accompanies these types of transformative solutions Help design and implement highly scalable applications that take advantage of distributed infrastructure on physical and cloud servers with technologies such as Docker, Kubernetes, OpenShift, and Rancher Help design, build and maintain automated deployment frameworks (Continuous Integration, Continuous Delivery), to reduce the software development cycle time Work collaboratively with infrastructure team to evolve data product configuration/customization for different environments Accurately report issues and status to project management Create and execute test cases (both automated and manual) Participate in various code review activities Help produce and review enterprise-level system design documentation, including: XML schemas, WSDL's, Use Cases, Software Architecture Documentation, Service Mapping (i.e., map service schema to backend source systems), Consumer Guide (i.e., end user documentation), and transition documentation to support the team. Create Service Level Agreements (states the agreed upon availability-uptime/downtime, maintenance windows, etc. for a Service) and Supplementary Specifications (i.e., non-functional specifications). Ensure implementations are up to current standards for coding, naming, security, and versioning. Possess knowledge and experience with different phases of testing (unit testing, integration testing, performance testing). Possess excellent communication skills to interface with various stakeholders from business consumer to technical staff. Be incorporated into development teams from design to deployment of enterprise services. Oversee and participate in the day-to-day support and maintenance activities. Understand assigned applications and system architecture Support troubleshooting activities Work on assignments involving the use of various technologies both old and new. Develop software with a focus on delivering reusable code. Complete assignments on time. Work as part of a team and work independently. Provide after hours/on-call support as needed Knowledge, Skills, Abilities, Behaviors: Technology Experience: 2+ years of experience in most of the following: We are looking for experts in these areas. If you don’t have experience in some of these, you are able to work collaboratively on a cross-functional team that builds Data Science signal delivery, data pipeline, and DevOps infrastructure. Full stack application development (e.g. JavaScript, Node, Vue.js, HTML/CSS and other web stack technologies) Extensive experience in ANSI SQL languages (e.g., MSSQL, Teradata, Postgres) required. Extensive expertise/experience in data acquisition, data cleansing and parsing required. Experience with container-based platforms such as Docker, Kubernetes, OpenShift, Mesosphere, Rancher, and CoreOS ETL experience required Experience using a distributed version control system (DVCS; e.g., GitHub, TFS) required. Extensive expertise/experience in data analysis, modeling and visualization required Experience with business intelligence platforms (e.g., Tableau, Qlik, MicroStrategy) required. Extensive experience with container monitoring applications such as Prometheus and Grafana Experience with Hadoop, Spark, Kafka, Cassandra (i.e. distributed compute and storage) SQL experience / database interrogation techniques Linux command line skill is required Mastery of one or more formal development languages (e.g., Python, R, JavaScript, Ruby, Scala, or Clojure) required Extensive expertise/experience in the areas of data structures, warehousing, and profiling Scrum, Agile, Lean Product Development, Domain Driven Design Excellent communication skills, both written and verbal Experience and knowledge with Service Oriented Architecture (SOA) Healthcare experience, preferable Exposure to the fundamentals of Enterprise Architecture (preferred) Adaptability Treats change and new situations as opportunities for learning or growth. Focuses on the beneficial aspects of change and speaks positively about the change to others. Seeks to understand changes in work tasks, situations, and environment as well as the logic or basis for change. Demonstrates the ability to help others adapt to change and to personally adapt to various work environments. Adaptability - Expected Level of Competency Willingness to network with corporate and field contacts. Occasional inter-departmental contact and presentations. Collaboratively works with customers on a 1:1 basis, as well as in teams. Working Standards Continues to refine development and analytical skills. Applies a consistent development approach to provide creative solutions to problems. Contribute to the design, construction and implementation of software and analytics solutions in the data science engineering environment. Works to build technical knowledge by taking advantage of internal team mentoring and attending professional development opportunities. Education & Experience Bachelor Degree Required Masters Degree Preferred 2+ Years Relevant Work Experience Required Travel Requirement The job may require up to 25% travel. HCA Healthcare has been recognized as one of the World’s Most Ethical Companies® by the Ethisphere Institute more than ten times. In recent years, HCA Healthcare spent an estimated $3.7 billion in cost for the delivery of charitable care, uninsured discounts, and other uncompensated expenses.  ""Good people beget good people.""- Dr. Thomas Frist, Sr. HCA Healthcare Co-Founder We are a family 270,000 dedicated professionals! Our Talent Acquisition team is reviewing applications for our Data Science Engineer opening. Qualified candidates will be contacted for interviews. Submit your resume today to join our community of caring! We are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status. Introduction Introduction Do you want to join an organization that invests in you as a(an) Data Science Engineer? At HCA Healthcare, you come first. HCA Healthcare has committed up to $300 million in programs to support our incredible team members over the course of three years. Benefits Benefits HCA Healthcare, offers a total rewards package that supports the health, life, career and retirement of our colleagues. The available plans and programs include: Comprehensive medical coverage that covers many common services at no cost or for a low copay. Plans include prescription drug and behavioral health coverage as well as free telemedicine services and free AirMed medical transportation. Additional options for dental and vision benefits, life and disability coverage, flexible spending accounts, supplemental health protection plans (accident, critical illness, hospital indemnity), auto and home insurance, identity theft protection, legal counseling, long-term care coverage, moving assistance, pet insurance and more. Free counseling services and resources for emotional, physical and financial wellbeing 401(k) Plan with a 100% match on 3% to 9% of pay (based on years of service) Employee Stock Purchase Plan with 10% off HCA Healthcare stock Family support through fertility and family building benefits with Progyny and adoption assistance. Referral services for child, elder and pet care, home and auto repair, event planning and more Consumer discounts through Abenity and Consumer Discounts Retirement readiness, rollover assistance services and preferred banking partnerships Education assistance (tuition, student loan, certification support, dependent scholarships) Colleague recognition program Time Away From Work Program (paid time off, paid family leave, long- and short-term disability coverage and leaves of absence) Employee Health Assistance Fund that offers free employee-only coverage to full-time and part-time colleagues based on income. Comprehensive medical coverage that covers many common services at no cost or for a low copay. Plans include prescription drug and behavioral health coverage as well as free telemedicine services and free AirMed medical transportation. Additional options for dental and vision benefits, life and disability coverage, flexible spending accounts, supplemental health protection plans (accident, critical illness, hospital indemnity), auto and home insurance, identity theft protection, legal counseling, long-term care coverage, moving assistance, pet insurance and more. Free counseling services and resources for emotional, physical and financial wellbeing 401(k) Plan with a 100% match on 3% to 9% of pay (based on years of service) Employee Stock Purchase Plan with 10% off HCA Healthcare stock Family support through fertility and family building benefits with Progyny and adoption assistance. Referral services for child, elder and pet care, home and auto repair, event planning and more Consumer discounts through Abenity and Consumer Discounts Retirement readiness, rollover assistance services and preferred banking partnerships Education assistance (tuition, student loan, certification support, dependent scholarships) Colleague recognition program Time Away From Work Program (paid time off, paid family leave, long- and short-term disability coverage and leaves of absence) Employee Health Assistance Fund that offers free employee-only coverage to full-time and part-time colleagues based on income. Learn more about Employee Benefits Note: Eligibility for benefits may vary by location. Note: Eligibility for benefits may vary by location. Note: Eligibility for benefits may vary by location. You contribute to our success. Every role has an impact on our patients’ lives and you have the opportunity to make a difference. We are looking for a dedicated Data Science Engineer like you to be a part of our team. Job Summary Position Summary This engineer delivers on development commitments from start to finish for assigned component of the Data Science organization within Information Technology Group (ITG). This technically focused position is responsible for designing, developing, testing and deploying components of our data products across the Enterprise. Major Responsibilities Major Responsibilities This engineer can quickly learn and maintain existing solutions as well as new development. They will provide key problem resolutions for production systems as needed. They have an in depth understanding of the services provided by Information Technology Group, Accelerated Technologies (AT) and can develop relationships throughout the organization to assist in accomplishing its goals for the company. This engineer strategically designs, constructs, and implements software in a software development environment. This includes selecting, gathering requirements for, designing, and implementing solutions for consumers throughout the enterprise. This engineer is a highly motivated self-starter and is committed to delivering high quality solutions within agreed upon timelines. This table illustrates the relative experience/expertise of the Data Science Engineer. Development Activities Development Activities Build life-changing healthcare technology Act as technical developer within AT and project integrations, including requirements gathering, design, development, and testing Participate in requirements validation and feasibility analysis with respect to AT Participate in creation of design specification that will enable day to day build activities and troubleshooting Estimate work effort required in delivering features keeping DS capabilities in mind Produce modular, reusable code that incorporates coding best practices and serves as an example for less experienced developers Write code and assist in development of new products/features and enhance and/or maintain existing ones Help design and execute DevOps strategies and processes, driving the change management which accompanies these types of transformative solutions Help design and implement highly scalable applications that take advantage of distributed infrastructure on physical and cloud servers with technologies such as Docker, Kubernetes, OpenShift, and Rancher Help design, build and maintain automated deployment frameworks (Continuous Integration, Continuous Delivery), to reduce the software development cycle time Work collaboratively with infrastructure team to evolve data product configuration/customization for different environments Accurately report issues and status to project management Create and execute test cases (both automated and manual) Participate in various code review activities Help produce and review enterprise-level system design documentation, including: XML schemas, WSDL's, Use Cases, Software Architecture Documentation, Service Mapping (i.e., map service schema to backend source systems), Consumer Guide (i.e., end user documentation), and transition documentation to support the team. Create Service Level Agreements (states the agreed upon availability-uptime/downtime, maintenance windows, etc. for a Service) and Supplementary Specifications (i.e., non-functional specifications). Ensure implementations are up to current standards for coding, naming, security, and versioning. Possess knowledge and experience with different phases of testing (unit testing, integration testing, performance testing). Possess excellent communication skills to interface with various stakeholders from business consumer to technical staff. Be incorporated into development teams from design to deployment of enterprise services. Build life-changing healthcare technology Act as technical developer within AT and project integrations, including requirements gathering, design, development, and testing Participate in requirements validation and feasibility analysis with respect to AT Participate in creation of design specification that will enable day to day build activities and troubleshooting Estimate work effort required in delivering features keeping DS capabilities in mind Produce modular, reusable code that incorporates coding best practices and serves as an example for less experienced developers Write code and assist in development of new products/features and enhance and/or maintain existing ones Help design and execute DevOps strategies and processes, driving the change management which accompanies these types of transformative solutions Help design and implement highly scalable applications that take advantage of distributed infrastructure on physical and cloud servers with technologies such as Docker, Kubernetes, OpenShift, and Rancher Help design, build and maintain automated deployment frameworks (Continuous Integration, Continuous Delivery), to reduce the software development cycle time Work collaboratively with infrastructure team to evolve data product configuration/customization for different environments Accurately report issues and status to project management Create and execute test cases (both automated and manual) Participate in various code review activities Help produce and review enterprise-level system design documentation, including: XML schemas, WSDL's, Use Cases, Software Architecture Documentation, Service Mapping (i.e., map service schema to backend source systems), Consumer Guide (i.e., end user documentation), and transition documentation to support the team. Create Service Level Agreements (states the agreed upon availability-uptime/downtime, maintenance windows, etc. for a Service) and Supplementary Specifications (i.e., non-functional specifications). Ensure implementations are up to current standards for coding, naming, security, and versioning. Possess knowledge and experience with different phases of testing (unit testing, integration testing, performance testing). Possess excellent communication skills to interface with various stakeholders from business consumer to technical staff. Be incorporated into development teams from design to deployment of enterprise services. Oversee and participate in the day-to-day support and maintenance activities. Oversee and participate in the day-to-day support and maintenance activities. Understand assigned applications and system architecture Support troubleshooting activities Work on assignments involving the use of various technologies both old and new. Develop software with a focus on delivering reusable code. Complete assignments on time. Work as part of a team and work independently. Provide after hours/on-call support as needed Understand assigned applications and system architecture Support troubleshooting activities Work on assignments involving the use of various technologies both old and new. Develop software with a focus on delivering reusable code. Complete assignments on time. Work as part of a team and work independently. Provide after hours/on-call support as needed Knowledge, Skills, Abilities, Behaviors: Knowledge, Skills, Abilities, Behaviors: Technology Experience: 2+ years of experience in most of the following: We are looking for experts in these areas. If you don’t have experience in some of these, you are able to work collaboratively on a cross-functional team that builds Data Science signal delivery, data pipeline, and DevOps infrastructure. Technology Experience: 2+ years of experience in most of the following: We are looking for experts in these areas. If you don’t have experience in some of these, you are able to work collaboratively on a cross-functional team that builds Data Science signal delivery, data pipeline, and DevOps infrastructure. Full stack application development (e.g. JavaScript, Node, Vue.js, HTML/CSS and other web stack technologies) Extensive experience in ANSI SQL languages (e.g., MSSQL, Teradata, Postgres) required. Extensive expertise/experience in data acquisition, data cleansing and parsing required. Experience with container-based platforms such as Docker, Kubernetes, OpenShift, Mesosphere, Rancher, and CoreOS ETL experience required Experience using a distributed version control system (DVCS; e.g., GitHub, TFS) required. Extensive expertise/experience in data analysis, modeling and visualization required Experience with business intelligence platforms (e.g., Tableau, Qlik, MicroStrategy) required. Extensive experience with container monitoring applications such as Prometheus and Grafana Experience with Hadoop, Spark, Kafka, Cassandra (i.e. distributed compute and storage) SQL experience / database interrogation techniques Linux command line skill is required Mastery of one or more formal development languages (e.g., Python, R, JavaScript, Ruby, Scala, or Clojure) required Extensive expertise/experience in the areas of data structures, warehousing, and profiling Scrum, Agile, Lean Product Development, Domain Driven Design Excellent communication skills, both written and verbal Experience and knowledge with Service Oriented Architecture (SOA) Healthcare experience, preferable Exposure to the fundamentals of Enterprise Architecture (preferred) Full stack application development (e.g. JavaScript, Node, Vue.js, HTML/CSS and other web stack technologies) Extensive experience in ANSI SQL languages (e.g., MSSQL, Teradata, Postgres) required. Extensive expertise/experience in data acquisition, data cleansing and parsing required. Experience with container-based platforms such as Docker, Kubernetes, OpenShift, Mesosphere, Rancher, and CoreOS ETL experience required Experience using a distributed version control system (DVCS; e.g., GitHub, TFS) required. Extensive expertise/experience in data analysis, modeling and visualization required Experience with business intelligence platforms (e.g., Tableau, Qlik, MicroStrategy) required. Extensive experience with container monitoring applications such as Prometheus and Grafana Experience with Hadoop, Spark, Kafka, Cassandra (i.e. distributed compute and storage) SQL experience / database interrogation techniques Linux command line skill is required Mastery of one or more formal development languages (e.g., Python, R, JavaScript, Ruby, Scala, or Clojure) required Extensive expertise/experience in the areas of data structures, warehousing, and profiling Scrum, Agile, Lean Product Development, Domain Driven Design Excellent communication skills, both written and verbal Experience and knowledge with Service Oriented Architecture (SOA) Healthcare experience, preferable Exposure to the fundamentals of Enterprise Architecture (preferred) Adaptability Adaptability Treats change and new situations as opportunities for learning or growth. Focuses on the beneficial aspects of change and speaks positively about the change to others. Seeks to understand changes in work tasks, situations, and environment as well as the logic or basis for change. Demonstrates the ability to help others adapt to change and to personally adapt to various work environments. Treats change and new situations as opportunities for learning or growth. Focuses on the beneficial aspects of change and speaks positively about the change to others. Seeks to understand changes in work tasks, situations, and environment as well as the logic or basis for change. Demonstrates the ability to help others adapt to change and to personally adapt to various work environments. Adaptability - Expected Level of Competency Adaptability - Expected Level of Competency Willingness to network with corporate and field contacts. Occasional inter-departmental contact and presentations. Collaboratively works with customers on a 1:1 basis, as well as in teams. Willingness to network with corporate and field contacts. Occasional inter-departmental contact and presentations. Collaboratively works with customers on a 1:1 basis, as well as in teams. Working Standards Working Standards Continues to refine development and analytical skills. Applies a consistent development approach to provide creative solutions to problems. Contribute to the design, construction and implementation of software and analytics solutions in the data science engineering environment. Works to build technical knowledge by taking advantage of internal team mentoring and attending professional development opportunities. Continues to refine development and analytical skills. Applies a consistent development approach to provide creative solutions to problems. Contribute to the design, construction and implementation of software and analytics solutions in the data science engineering environment. Works to build technical knowledge by taking advantage of internal team mentoring and attending professional development opportunities. Education & Experience Education & Experience Bachelor Degree Required Masters Degree Preferred 2+ Years Relevant Work Experience Required Bachelor Degree Required Masters Degree Preferred 2+ Years Relevant Work Experience Required Travel Requirement Travel Requirement The job may require up to 25% travel. The job may require up to 25% travel. HCA Healthcare has been recognized as one of the World’s Most Ethical Companies® by the Ethisphere Institute more than ten times. In recent years, HCA Healthcare spent an estimated $3.7 billion in cost for the delivery of charitable care, uninsured discounts, and other uncompensated expenses. ""Good people beget good people.""- Dr. Thomas Frist, Sr. HCA Healthcare Co-Founder We are a family 270,000 dedicated professionals! Our Talent Acquisition team is reviewing applications for our Data Science Engineer opening. Qualified candidates will be contacted for interviews. Submit your resume today to join our community of caring! Submit your resume today to join our community of caring! We are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status. We are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.",3.3,10000+ Employees,1968,Company - Public,Health Care Services & Hospitals,Healthcare,$10+ billion (USD)
"Austin, TX",Data Engineer (Contract),$77K - $111K (Glassdoor est.),RevOpsforce,"About RevOpsforce: At RevOpsforce, our mission is to drive sustainable revenue growth for our clients by leveraging data-driven insights, technology, and process optimization. We strive to deliver measurable results that exceed expectations, while building strong partnerships with our clients and continuously improving our own capabilities. Our Expert Network is composed of the highest skilled and certified professionals in revenue operations. We leverage this network to solve our clients' most complex operational challenges. Type: Contract Job Description: We are seeking a talented and experienced Data Engineer to join our growing team. As a Data Engineer at our company, you will be responsible for designing, building, and maintaining the data pipelines that power our data-driven applications. In this role, you will work closely with our data science and engineering teams to develop high-quality, efficient data pipelines that enable us to derive insights from vast amounts of data. You should be comfortable working with large volumes of structured and unstructured data, and have a strong understanding of SQL, Python, and other programming languages. Responsibilities: Design, build, and maintain data pipelines to support data-driven applications and analytics Analyze data to identify trends and patterns Collaborate with data scientists and engineers to develop data-driven solutions Write and maintain documentation for data pipelines Monitor and optimize data pipelines for performance and efficiency Qualifications: Bachelor's or Master's degree in a related field (e.g. Computer Science, Data Science, Engineering) 3+ years of experience in a data engineering role Strong skills in SQL and at least one programming language (e.g. Python, Java, C++, etc.) Experience with big data technologies (e.g. Hadoop, Spark, etc.) and cloud platforms (e.g. AWS, Azure, GCP) Strong problem-solving and analytical skills Excellent communication skills and ability to work in a team environment Join our dynamic and forward-thinking team to make a significant impact on our clients' revenue growth journey! We are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status. Website is coming soon and will be located at www.revopsforce.com About RevOpsforce: About RevOpsforce: At RevOpsforce, our mission is to drive sustainable revenue growth for our clients by leveraging data-driven insights, technology, and process optimization. We strive to deliver measurable results that exceed expectations, while building strong partnerships with our clients and continuously improving our own capabilities. Our Expert Network is composed of the highest skilled and certified professionals in revenue operations. We leverage this network to solve our clients' most complex operational challenges. Type: Contract Type: Job Description: Job Description: We are seeking a talented and experienced Data Engineer to join our growing team. As a Data Engineer at our company, you will be responsible for designing, building, and maintaining the data pipelines that power our data-driven applications. In this role, you will work closely with our data science and engineering teams to develop high-quality, efficient data pipelines that enable us to derive insights from vast amounts of data. You should be comfortable working with large volumes of structured and unstructured data, and have a strong understanding of SQL, Python, and other programming languages. Responsibilities: Responsibilities: Design, build, and maintain data pipelines to support data-driven applications and analytics Analyze data to identify trends and patterns Collaborate with data scientists and engineers to develop data-driven solutions Write and maintain documentation for data pipelines Monitor and optimize data pipelines for performance and efficiency Design, build, and maintain data pipelines to support data-driven applications and analytics Analyze data to identify trends and patterns Collaborate with data scientists and engineers to develop data-driven solutions Write and maintain documentation for data pipelines Monitor and optimize data pipelines for performance and efficiency Qualifications: Qualifications: Bachelor's or Master's degree in a related field (e.g. Computer Science, Data Science, Engineering) 3+ years of experience in a data engineering role Strong skills in SQL and at least one programming language (e.g. Python, Java, C++, etc.) Experience with big data technologies (e.g. Hadoop, Spark, etc.) and cloud platforms (e.g. AWS, Azure, GCP) Strong problem-solving and analytical skills Excellent communication skills and ability to work in a team environment Bachelor's or Master's degree in a related field (e.g. Computer Science, Data Science, Engineering) 3+ years of experience in a data engineering role Strong skills in SQL and at least one programming language (e.g. Python, Java, C++, etc.) Experience with big data technologies (e.g. Hadoop, Spark, etc.) and cloud platforms (e.g. AWS, Azure, GCP) Strong problem-solving and analytical skills Excellent communication skills and ability to work in a team environment Join our dynamic and forward-thinking team to make a significant impact on our clients' revenue growth journey! We are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status. We are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status. Website is coming soon and will be located at www.revopsforce.com Website is coming soon and will be located at www.revopsforce.com",N/A,Unknown,N/A,Company - Private,N/A,N/A,Unknown / Non-Applicable
Remote,Senior Data Engineer,$60.00 - $65.00 Per Hour (Employer est.),Anlage Infotech,"Qualifications/requirements: Bachelor’s degree in computer science, Engineering, or a related field is REQUIRED (no exception) 5 years or more experience as data engineer 5 years or more Experience with programming skills with Python 5 years or more Experience with data systems and complex query writing Experience with Azure is a plus Experience with NIFI is a plus Sense of ownership and pride in your performance and its impact on company’s success Critical thinker and problem-solving skills Team player Good time-management skills Great interpersonal and communication skills Bachelor’s degree in computer science, Engineering, or a related field is REQUIRED (no exception) 5 years or more experience as data engineer 5 years or more Experience with programming skills with Python 5 years or more Experience with data systems and complex query writing Experience with Azure is a plus Experience with NIFI is a plus Sense of ownership and pride in your performance and its impact on company’s success Critical thinker and problem-solving skills Team player Good time-management skills Great interpersonal and communication skills Job Type: Contract Salary: $60.00 - $65.00 per hour Benefits: Flexible schedule Flexible schedule Compensation package: Hourly pay Hourly pay Experience level: 8 years 8 years Schedule: 8 hour shift 8 hour shift Experience: ETL: 5 years (Preferred) Python: 5 years (Preferred) MySQL: 5 years (Preferred) ETL: 5 years (Preferred) Python: 5 years (Preferred) MySQL: 5 years (Preferred) Work Location: Remote",3.8,1001 to 5000 Employees,2012,Company - Private,Information Technology Support Services,Information Technology,$100 to $500 million (USD)
"Richardson, TX",Data Engineer,$79K - $108K (Glassdoor est.),Rehrig Pacific Company,"Role Description Position Title: Data Engineer Manager: Sr. Manager, Enterprise Solution Position Location: Rehrig Technology Center, TX Manager Once Removed: Director - IT & Security Level of Work: Individual Contributor Date Prepared: 08/09/2023 Brief Role Description The Data Engineer will be responsible for developing and managing the enterprise overall data architecture, which includes the data models, data warehouses, data pipelines, and other systems that will be used to store, manage, and analyze data. This role also requires a fair amount of knowledge on designing, implementing, and maintaining Microsoft SQL Servers, Data analytics using Power BI and azure data pipeline and implementing security and working with stakeholders to understand their data needs and deliver solutions that meet those needs. Accountabilities Database Management: Design, create, and maintain optimal database systems for various applications and services. Monitor and optimize database performance, security, and integrity. Handle database backups, recovery, and regular updates to ensure data accuracy and availability. Collaborate with IT teams to maintain server infrastructure and ensure database reliability. Configuring, solving problems, and supporting SQL Servers in production and non-production environments running in cloud environments and Windows operating systems Develop and maintain a disaster recovery plan to measure ensure that critical data systems are available in the event of outages. Develop and maintain a capacity plan to measure utilization and plan for growth in demand for database resources. Design scripts and utilities to automate day to day database administration. Assist with the monitoring performance analysis of existing applications and ensuring that the system is consistent with ongoing need Create technical documentation, operational procedures, Incident, and Problem communications and reporting. Quickly restore service during system outages, provide troubleshooting support to team members, perform proactive problem prevention, and advance technical details to technical lead and/or to vendor support Maintaining, detail and deliver Microsoft SQL 2012+ in accordance with certified specifications and security guidelines. Data Engineering: Design and construct scalable, maintainable, and efficient data pipelines. Implement and manage ETL processes, ensuring the accuracy, reliability, and timeliness of data processes. Design and maintain enterprise level data model and data flow Cloud and Integration: Design and implement solutions in Azure, with a particular focus on Azure data pipeline. Integrate Power BI solutions, ensuring that data sources are accessible and optimized for visualization. Stay updated with the latest Azure features and Power BI advancements to recommend and implement improvements. Collaboration & Training: Collaborate with cross-functional teams, assisting with data-related technical issues, and ensuring optimal data delivery architecture. Provide training and support to junior team members and stakeholders on database management and data engineering best practices. Qualifications BS in Information Technology or a related technical field plus 4-6 or more years related overall experience. 3-5 years of experience in data engineering Strong understanding of data engineering principles and practices Experience with Power BI and Azure Data Pipeline Experience with SQL, Python, and other programming languages Strong expertise in Azure data pipeline, including Azure Data Factory, Azure Data Lake, and other Azure data services. Proficiency in Power BI, including designing and deploying insightful dashboards and data visualizations. Strong knowledge of SQL and MS SQL databases, including optimization techniques. Excellent understanding of data pipeline and workflow management tools. Strong analytical skills with a problem-solving attitude. Good interpersonal and communication skills, with the ability to collaborate effectively across diverse teams. Certifications in Cloud and/or Microsoft SQL Server highly desired A minimum of 3 years of experience in on-premises and cloud-based Database Technologies - AWS RDS and/or Azure SQL PAAS Devise Strategy and Engage with IT leadership and business partners to define a Database & Analytics strategy for Rehrig Implement automation of database operations like upgrades and patching Demonstrated Experience in Defining and Implementing Production Proactive Health Monitoring of Database Technologies Strong Incident Root Cause Analysis on Database Execution and Application Interaction with Database Technology Experience with Data migration and SQL upgrades across Microsoft Server versions Ability to travel approximately 15%-20% of the time General Responsibilities Provide periodic reporting on workforce capacity and utilization. Provide periodic reporting on the availability and health of software development process. Other duties as assigned. Role Description Position Title: Data Engineer Manager: Sr. Manager, Enterprise Solution Position Location: Rehrig Technology Center, TX Manager Once Removed: Director - IT & Security Level of Work: Individual Contributor Date Prepared: 08/09/2023 Brief Role Description Brief Role Description The Data Engineer will be responsible for developing and managing the enterprise overall data architecture, which includes the data models, data warehouses, data pipelines, and other systems that will be used to store, manage, and analyze data. This role also requires a fair amount of knowledge on designing, implementing, and maintaining Microsoft SQL Servers, Data analytics using Power BI and azure data pipeline and implementing security and working with stakeholders to understand their data needs and deliver solutions that meet those needs. Accountabilities Accountabilities Database Management: Design, create, and maintain optimal database systems for various applications and services. Monitor and optimize database performance, security, and integrity. Handle database backups, recovery, and regular updates to ensure data accuracy and availability. Collaborate with IT teams to maintain server infrastructure and ensure database reliability. Configuring, solving problems, and supporting SQL Servers in production and non-production environments running in cloud environments and Windows operating systems Develop and maintain a disaster recovery plan to measure ensure that critical data systems are available in the event of outages. Develop and maintain a capacity plan to measure utilization and plan for growth in demand for database resources. Design scripts and utilities to automate day to day database administration. Assist with the monitoring performance analysis of existing applications and ensuring that the system is consistent with ongoing need Create technical documentation, operational procedures, Incident, and Problem communications and reporting. Quickly restore service during system outages, provide troubleshooting support to team members, perform proactive problem prevention, and advance technical details to technical lead and/or to vendor support Maintaining, detail and deliver Microsoft SQL 2012+ in accordance with certified specifications and security guidelines. Data Engineering: Design and construct scalable, maintainable, and efficient data pipelines. Implement and manage ETL processes, ensuring the accuracy, reliability, and timeliness of data processes. Design and maintain enterprise level data model and data flow Cloud and Integration: Design and implement solutions in Azure, with a particular focus on Azure data pipeline. Integrate Power BI solutions, ensuring that data sources are accessible and optimized for visualization. Stay updated with the latest Azure features and Power BI advancements to recommend and implement improvements. Collaboration & Training: Collaborate with cross-functional teams, assisting with data-related technical issues, and ensuring optimal data delivery architecture. Provide training and support to junior team members and stakeholders on database management and data engineering best practices. Database Management: Design, create, and maintain optimal database systems for various applications and services. Monitor and optimize database performance, security, and integrity. Handle database backups, recovery, and regular updates to ensure data accuracy and availability. Collaborate with IT teams to maintain server infrastructure and ensure database reliability. Configuring, solving problems, and supporting SQL Servers in production and non-production environments running in cloud environments and Windows operating systems Develop and maintain a disaster recovery plan to measure ensure that critical data systems are available in the event of outages. Develop and maintain a capacity plan to measure utilization and plan for growth in demand for database resources. Design scripts and utilities to automate day to day database administration. Assist with the monitoring performance analysis of existing applications and ensuring that the system is consistent with ongoing need Create technical documentation, operational procedures, Incident, and Problem communications and reporting. Quickly restore service during system outages, provide troubleshooting support to team members, perform proactive problem prevention, and advance technical details to technical lead and/or to vendor support Maintaining, detail and deliver Microsoft SQL 2012+ in accordance with certified specifications and security guidelines. Design, create, and maintain optimal database systems for various applications and services. Monitor and optimize database performance, security, and integrity. Handle database backups, recovery, and regular updates to ensure data accuracy and availability. Collaborate with IT teams to maintain server infrastructure and ensure database reliability. Configuring, solving problems, and supporting SQL Servers in production and non-production environments running in cloud environments and Windows operating systems Develop and maintain a disaster recovery plan to measure ensure that critical data systems are available in the event of outages. Develop and maintain a capacity plan to measure utilization and plan for growth in demand for database resources. Design scripts and utilities to automate day to day database administration. Assist with the monitoring performance analysis of existing applications and ensuring that the system is consistent with ongoing need Create technical documentation, operational procedures, Incident, and Problem communications and reporting. Quickly restore service during system outages, provide troubleshooting support to team members, perform proactive problem prevention, and advance technical details to technical lead and/or to vendor support Maintaining, detail and deliver Microsoft SQL 2012+ in accordance with certified specifications and security guidelines. Design, create, and maintain optimal database systems for various applications and services. Monitor and optimize database performance, security, and integrity. Handle database backups, recovery, and regular updates to ensure data accuracy and availability. Collaborate with IT teams to maintain server infrastructure and ensure database reliability. Configuring, solving problems, and supporting SQL Servers in production and non-production environments running in cloud environments and Windows operating systems Develop and maintain a disaster recovery plan to measure ensure that critical data systems are available in the event of outages. Develop and maintain a capacity plan to measure utilization and plan for growth in demand for database resources. Design scripts and utilities to automate day to day database administration. Assist with the monitoring performance analysis of existing applications and ensuring that the system is consistent with ongoing need Create technical documentation, operational procedures, Incident, and Problem communications and reporting. Quickly restore service during system outages, provide troubleshooting support to team members, perform proactive problem prevention, and advance technical details to technical lead and/or to vendor support Maintaining, detail and deliver Microsoft SQL 2012+ in accordance with certified specifications and security guidelines. Data Engineering: Design and construct scalable, maintainable, and efficient data pipelines. Implement and manage ETL processes, ensuring the accuracy, reliability, and timeliness of data processes. Design and maintain enterprise level data model and data flow Design and construct scalable, maintainable, and efficient data pipelines. Implement and manage ETL processes, ensuring the accuracy, reliability, and timeliness of data processes. Design and maintain enterprise level data model and data flow Design and construct scalable, maintainable, and efficient data pipelines. Implement and manage ETL processes, ensuring the accuracy, reliability, and timeliness of data processes. Design and maintain enterprise level data model and data flow Cloud and Integration: Design and implement solutions in Azure, with a particular focus on Azure data pipeline. Integrate Power BI solutions, ensuring that data sources are accessible and optimized for visualization. Stay updated with the latest Azure features and Power BI advancements to recommend and implement improvements. Design and implement solutions in Azure, with a particular focus on Azure data pipeline. Integrate Power BI solutions, ensuring that data sources are accessible and optimized for visualization. Stay updated with the latest Azure features and Power BI advancements to recommend and implement improvements. Design and implement solutions in Azure, with a particular focus on Azure data pipeline. Integrate Power BI solutions, ensuring that data sources are accessible and optimized for visualization. Stay updated with the latest Azure features and Power BI advancements to recommend and implement improvements. Collaboration & Training: Collaborate with cross-functional teams, assisting with data-related technical issues, and ensuring optimal data delivery architecture. Provide training and support to junior team members and stakeholders on database management and data engineering best practices. Collaborate with cross-functional teams, assisting with data-related technical issues, and ensuring optimal data delivery architecture. Provide training and support to junior team members and stakeholders on database management and data engineering best practices. Collaborate with cross-functional teams, assisting with data-related technical issues, and ensuring optimal data delivery architecture. Provide training and support to junior team members and stakeholders on database management and data engineering best practices. Qualifications Qualifications BS in Information Technology or a related technical field plus 4-6 or more years related overall experience. BS in Information Technology or a related technical field plus 4-6 or more years related overall experience. 3-5 years of experience in data engineering Strong understanding of data engineering principles and practices Experience with Power BI and Azure Data Pipeline Experience with SQL, Python, and other programming languages Strong expertise in Azure data pipeline, including Azure Data Factory, Azure Data Lake, and other Azure data services. Proficiency in Power BI, including designing and deploying insightful dashboards and data visualizations. Strong knowledge of SQL and MS SQL databases, including optimization techniques. Excellent understanding of data pipeline and workflow management tools. Strong analytical skills with a problem-solving attitude. Good interpersonal and communication skills, with the ability to collaborate effectively across diverse teams. 3-5 years of experience in data engineering Strong understanding of data engineering principles and practices Experience with Power BI and Azure Data Pipeline Experience with SQL, Python, and other programming languages Strong expertise in Azure data pipeline, including Azure Data Factory, Azure Data Lake, and other Azure data services. Proficiency in Power BI, including designing and deploying insightful dashboards and data visualizations. Strong knowledge of SQL and MS SQL databases, including optimization techniques. Excellent understanding of data pipeline and workflow management tools. Strong analytical skills with a problem-solving attitude. Good interpersonal and communication skills, with the ability to collaborate effectively across diverse teams. Certifications in Cloud and/or Microsoft SQL Server highly desired Certifications in Cloud and/or Microsoft SQL Server highly desired A minimum of 3 years of experience in on-premises and cloud-based Database Technologies - AWS RDS and/or Azure SQL PAAS A minimum of 3 years of experience in on-premises and cloud-based Database Technologies - AWS RDS and/or Azure SQL PAAS Devise Strategy and Engage with IT leadership and business partners to define a Database & Analytics strategy for Rehrig Devise Strategy and Engage with IT leadership and business partners to define a Database & Analytics strategy for Rehrig Implement automation of database operations like upgrades and patching Implement automation of database operations like upgrades and patching Demonstrated Experience in Defining and Implementing Production Proactive Health Monitoring of Database Technologies Demonstrated Experience in Defining and Implementing Production Proactive Health Monitoring of Database Technologies Strong Incident Root Cause Analysis on Database Execution and Application Interaction with Database Technology Strong Incident Root Cause Analysis on Database Execution and Application Interaction with Database Technology Experience with Data migration and SQL upgrades across Microsoft Server versions Experience with Data migration and SQL upgrades across Microsoft Server versions Ability to travel approximately 15%-20% of the time Ability to travel approximately 15%-20% of the time General Responsibilities General Responsibilities Provide periodic reporting on workforce capacity and utilization. Provide periodic reporting on workforce capacity and utilization. Provide periodic reporting on the availability and health of software development process. Provide periodic reporting on the availability and health of software development process. Other duties as assigned. Other duties as assigned.",3.8,501 to 1000 Employees,1913,Company - Private,Machinery Manufacturing,Manufacturing,$100 to $500 million (USD)
"Phoenix, AZ",Big Data Engineer,$50.00 - $60.00 Per Hour (Employer est.),Reuben Cooley Inc.,"Job Description: Job Description: Experience working with Hadoop stack (Hive, HDFS, Spark). Experience working with Hadoop stack (Hive, HDFS, Spark). Strong AWS/GCP/Azure experience. Strong AWS/GCP/Azure experience. Experience with Java or Scala or python. Experience with Java or Scala or python. Good knowledge of SQL. Good knowledge of SQL. Good Communication Skills. Good Communication Skills. Job Type: Contract Pay: $50.00 - $60.00 per hour Experience level: 5 years 5 years Schedule: 8 hour shift Day shift 8 hour shift Day shift Ability to commute/relocate: Phoenix, AZ 85027: Reliably commute or planning to relocate before starting work (Required) Phoenix, AZ 85027: Reliably commute or planning to relocate before starting work (Required) Experience: Hadoop: 4 years (Required) SQL: 5 years (Required) Scala: 2 years (Preferred) Hadoop: 4 years (Required) SQL: 5 years (Required) Scala: 2 years (Preferred) Work Location: In person",N/A,Unknown,N/A,Company - Public,N/A,N/A,Unknown / Non-Applicable
"Bloomington, MN",Big Data Engineer,$48.00 - $52.00 Per Hour (Employer est.),KONNECTINGTREE INC,"Greetings from KonnectingTree Inc., We are actively looking for a candidate with below mentioned experience, Role : Bigdata Engineer Location : Hybrid ( Bloomington, MN) Experience : 3-5 years Job Description: * Good experience in build and deployment of Big data applications using Pyspark * Good experience in Python. * Experience in Hadoop file structure. * Good Communication and team player Job Type: Contract Salary: $48.00 - $52.00 per hour Experience level: 3 years 4 years 3 years 4 years Schedule: 8 hour shift 8 hour shift Ability to commute/relocate: Bloomington, MN 55425: Reliably commute or planning to relocate before starting work (Required) Bloomington, MN 55425: Reliably commute or planning to relocate before starting work (Required) Experience: Informatica: 1 year (Preferred) SQL: 1 year (Preferred) Data warehouse: 1 year (Preferred) Informatica: 1 year (Preferred) SQL: 1 year (Preferred) Data warehouse: 1 year (Preferred) Work Location: In person",N/A,,,,,,
Minnesota,Data Engineer - Azure,$100K - $140K (Employer est.),Tail Wind Informatics,"About Us: Tail Wind is an IT Consulting Services company—Microsoft Solutions Partner—that delivers Data Architecture and Business Intelligence solutions. We offer cloud and on-premise Data Architecture, ETL Development, Data Migration, Reporting and Dashboard solutions. We’ve established an excellent reputation providing these services to awesome customers! We are building a talented crew of data savvy individuals for local and national projects in data. We offer excellent compensation (salary, bonus, benefits). Most importantly, our people have an incredible opportunity to build their skills in a team environment.  We are currently seeking candidates for our Data Engineer position to do the following: Responsibilities Using Data Science techniques to preform predictive modeling services. Building data sets in data warehouses using SQL, Azure and Python. Experience with Databricks or Snowflake Work across multiple different teams and projects. Requirements 3+ years experience using Azure Data Factory Must have strong backend development skills with SQL Server and writing complex SQL queries Strong understanding of predictive modeling Working in a fast paced, deadline heavy environment Benefits 401k + match Health Insurance Dental Insurance Vision Long-Term Disability Insurance Life Insurance The Tail Wind Team: A healthy work-life balance. We look for people that love what they do, want to learn, earn and enjoy life to the fullest. Equal Opportunity Employer - No Agencies Please About Us: About Us: Tail Wind is an IT Consulting Services company—Microsoft Solutions Partner—that delivers Data Architecture and Business Intelligence solutions. We offer cloud and on-premise Data Architecture, ETL Development, Data Migration, Reporting and Dashboard solutions. We’ve established an excellent reputation providing these services to awesome customers! We are building a talented crew of data savvy individuals for local and national projects in data. We offer excellent compensation (salary, bonus, benefits). Most importantly, our people have an incredible opportunity to build their skills in a team environment. services to awesome customers! data savvy We are currently seeking candidates for our Data Engineer position to do the following: Data Engineer Responsibilities Responsibilities Using Data Science techniques to preform predictive modeling services. Building data sets in data warehouses using SQL, Azure and Python. Experience with Databricks or Snowflake Work across multiple different teams and projects. Using Data Science techniques to preform predictive modeling services. Building data sets in data warehouses using SQL, Azure and Python. Experience with Databricks or Snowflake Work across multiple different teams and projects. Requirements Requirements 3+ years experience using Azure Data Factory Must have strong backend development skills with SQL Server and writing complex SQL queries Strong understanding of predictive modeling Working in a fast paced, deadline heavy environment 3+ years experience using Azure Data Factory Must have strong backend development skills with SQL Server and writing complex SQL queries Strong understanding of predictive modeling Working in a fast paced, deadline heavy environment Benefits Benefits 401k + match Health Insurance Dental Insurance Vision Long-Term Disability Insurance Life Insurance 401k + match Health Insurance Dental Insurance Vision Long-Term Disability Insurance Life Insurance The Tail Wind Team: A healthy work-life balance. We look for people that love what they do, want to learn, earn and enjoy life to the fullest. Equal Opportunity Employer - No Agencies Please No Agencies Please",N/A,1 to 50 Employees,N/A,Company - Public,N/A,N/A,Unknown / Non-Applicable
"Downers Grove, IL",Data Engineer,$79K - $116K (Glassdoor est.),3Cloud,"Are you looking for a role that motivates and challenges you? Are you ready for an opportunity for growth? Do you want to work on teams where people roll up their sleeves to take on tough problems together, and regularly blow the doors off our clients with their outstanding teamwork? If you answered yes to those questions, 3Cloud might just be for you! At 3Cloud, we hire people who aren't afraid to experiment or fail. We hire people who are willing to give direct and candid feedback to their managers, leaders, and team members. We hire people who jump at those opportunities because they care about our collective growth and success. We hire people who challenge and hold each other accountable for living 3Cloud's core values because they know that it will result in amazing experiences and solutions for our clients and each other. Sr. Consultant /Data Engineer – Data Architecture & Engineering Are you looking for a role that motivates and challenges you? Are you ready for an opportunity for growth? Do you want to work on teams where people roll up their sleeves to take on tough problems together, and regularly blow the doors off our clients with their outstanding teamwork? If you answered yes to those questions, 3Cloud might just be for you! At 3Cloud, we hire people who aren't afraid to experiment or fail. We hire people who are willing to give direct and candid feedback to their managers, leaders, and team members. We hire people who jump at those opportunities because they care about our collective growth and success. We hire people who challenge and hold each other accountable for living 3Cloud's core values because they know that it will result in amazing experiences and solutions for our clients and each other. A Senior Consultant in the Data and Analytics Practice is someone that looks forward to being challenged and continuously grow their technical and team collaboration skills. You'll provide technical guidance, as needed, to other team members for quality assurance, mentoring and growth opportunities. If this is a path that excites you, let's chat! Responsibilities Technical delivery in both Agile and Waterfall methodologies Implement solutions using the Microsoft Power BI tool suite and related technologies Leverage advanced database systems including data warehouses, data marts, and models needed for business, financial and operational analysis and/or reporting Develop high performing, reliable and scalable solutions Ability to clearly communicate technical details to business and management personnel Perform data analysis that will support and enhance Information Management systems Author, oversee and gain approval of design documents for projects assigned Develop reports, dashboards & scorecards, data exploration and visual analytics using existing and emerging tools and technologies. Work independently or as part of a team to design and develop solutions Assist business development team with pre-sales activities and RFPs Provide meaningful feedback and coaching of other team members to successfully overcome technological challenges Leading and demonstrating emerging technologies and concepts to teams Requirements Bachelor's Degree desired in Computer Science, Information Technology, or related field 2+ years of experience with data visualization Expert knowledge of Data Management, Business Intelligence and Analytics concepts including: Extract, Transform, Load (ETL) Data Warehousing, Data Lakes, Data Marts, Data Stores, and Cubes Data Quality and Profiling Data Governance, Master Data Management, and Metadata Management Data Visualization, Dashboard, Reporting, Self Service Business Intelligence (BI), Key Performance Indicators (KPI), and Scorecards Predictive, Prescription, and Descriptive Analytics Expert knowledge of Microsoft Business Intelligence stack including Power BI, Excel and SQL Server (SSAS, SSRS, SSIS) Minimum of 3-5 years of experience (of which 2-3 years in consulting) Experience with Azure Data Estate (Azure SQL DB, Cosmos DB, Azure SQL DW, Azure Data Lake) is a plus Azure ML Studio/Services experience a plus Certifications are a plus Advanced knowledge of SQL, including ability to write stored procedures, triggers, analytic functions, and tuning is a plus Knowledge of relational and dimensional database structures, theories, principles, and practice is a plus Experience with Data Science tools, technologies and techniques (R, Python, algorithms) is a plus Passionate about learning new technologies Ability to learn new concepts and software quickly Analytical approach to problem-solving; ability to use technology to solve business problems Familiarity with database-centric applications Ability to communicate effectively in both a technical and non-technical manner Ability to work in a fast-paced environment Ability to travel up to 30% Don't meet every single requirement? At 3Cloud we are dedicated to building a diverse, inclusive and authentic workplace, so if you're excited about this role but your past experience doesn't align perfectly with every qualification in the job description, we encourage you to apply anyways. At this time, we cannot sponsor applicants for work visas. Candidates should be based in the United States. About 3Cloud 3Cloud is a ""born in the cloud"" Gold-Certified Microsoft Azure technology consulting firm and Azure Expert Managed Services Provider that provides cloud strategy, design, implementation, acquisition and ongoing managed services to its clients across various industries. To meet the needs of its clients, 3Cloud offers comprehensive services across Cloud Infrastructure, DevOps & Automation, Cloud Application Development and Data, Analytics and AI. Founded by former Microsoft technology leaders, 3Cloud uniquely offers: (1) highly experienced and proven cloud architects and technologists, (2) strong business strategy, financial acumen and operational proficiency and (3) deep relationship and network into Microsoft engineering and field leadership. 3Cloud Total Rewards Highlights Include: Flexible work location with a virtual first approach to work! 401(K) with match up to 50% of your 6% contributions of eligible pay Generous PTO providing a minimum of 15 days in addition to 9 paid company holidays and 2 floating personal days Three medical plan options to allow you the choice to elect what works best for you! Option for vision and dental coverage 100% employer premium coverage for STD and LTD Paid leave for birth parents and non-birth parents Option for FSA, HSA, HRA and Dependent Care $67.00 monthly tech and home office allowance Utilization and/or discretionary bonus eligibility based on role Robust Employee Assistance Program to help with everyday challenges 3Cloud offers competitive compensation. In addition to base pay employees are eligible to receive an annual discretionary/utilization bonus. If you are hired at 3Cloud your final base salary is based on factors such as skills, education, experience and/or geographic location. Please keep in mind that the range mentioned above includes the full base salary range for the role. It is not typical for offers to be made at or near the top of the range. Base Salary Range $90,000—$182,000 USD Don't meet every single requirement? At 3Cloud we are dedicated to building a diverse, inclusive and authentic workplace, so if you're excited about this role but your past experience doesn't align perfectly with every qualification in the job description, we encourage you to apply anyway. At this time, we cannot sponsor applicants for work visas. Are you looking for a role that motivates and challenges you? Are you ready for an opportunity for growth? Do you want to work on teams where people roll up their sleeves to take on tough problems together, and regularly blow the doors off our clients with their outstanding teamwork? If you answered yes to those questions, 3Cloud might just be for you! At 3Cloud, we hire people who aren't afraid to experiment or fail. We hire people who are willing to give direct and candid feedback to their managers, leaders, and team members. We hire people who jump at those opportunities because they care about our collective growth and success. We hire people who challenge and hold each other accountable for living 3Cloud's core values because they know that it will result in amazing experiences and solutions for our clients and each other. Are you looking for a role that motivates and challenges you? Are you ready for an opportunity for growth? Do you want to work on teams where people roll up their sleeves to take on tough problems together, and regularly blow the doors off our clients with their outstanding teamwork? If you answered yes to those questions, 3Cloud might just be for you! At 3Cloud, we hire people who aren't afraid to experiment or fail. We hire people who are willing to give direct and candid feedback to their managers, leaders, and team members. We hire people who jump at those opportunities because they care about our collective growth and success. We hire people who challenge and hold each other accountable for living 3Cloud's core values because they know that it will result in amazing experiences and solutions for our clients and each other. Sr. Consultant /Data Engineer – Data Architecture & Engineering Sr. Consultant /Data Engineer – Data Architecture & Engineering Are you looking for a role that motivates and challenges you? Are you ready for an opportunity for growth? Do you want to work on teams where people roll up their sleeves to take on tough problems together, and regularly blow the doors off our clients with their outstanding teamwork? If you answered yes to those questions, 3Cloud might just be for you! At 3Cloud, we hire people who aren't afraid to experiment or fail. We hire people who are willing to give direct and candid feedback to their managers, leaders, and team members. We hire people who jump at those opportunities because they care about our collective growth and success. We hire people who challenge and hold each other accountable for living 3Cloud's core values because they know that it will result in amazing experiences and solutions for our clients and each other. A Senior Consultant in the Data and Analytics Practice is someone that looks forward to being challenged and continuously grow their technical and team collaboration skills. You'll provide technical guidance, as needed, to other team members for quality assurance, mentoring and growth opportunities. If this is a path that excites you, let's chat! Responsibilities Responsibilities Technical delivery in both Agile and Waterfall methodologies Implement solutions using the Microsoft Power BI tool suite and related technologies Leverage advanced database systems including data warehouses, data marts, and models needed for business, financial and operational analysis and/or reporting Develop high performing, reliable and scalable solutions Technical delivery in both Agile and Waterfall methodologies Implement solutions using the Microsoft Power BI tool suite and related technologies Leverage advanced database systems including data warehouses, data marts, and models needed for business, financial and operational analysis and/or reporting Develop high performing, reliable and scalable solutions Ability to clearly communicate technical details to business and management personnel Perform data analysis that will support and enhance Information Management systems Author, oversee and gain approval of design documents for projects assigned Develop reports, dashboards & scorecards, data exploration and visual analytics using existing and emerging tools and technologies. Work independently or as part of a team to design and develop solutions Assist business development team with pre-sales activities and RFPs Provide meaningful feedback and coaching of other team members to successfully overcome technological challenges Leading and demonstrating emerging technologies and concepts to teams Ability to clearly communicate technical details to business and management personnel Perform data analysis that will support and enhance Information Management systems Author, oversee and gain approval of design documents for projects assigned Develop reports, dashboards & scorecards, data exploration and visual analytics using existing and emerging tools and technologies. Work independently or as part of a team to design and develop solutions Assist business development team with pre-sales activities and RFPs Provide meaningful feedback and coaching of other team members to successfully overcome technological challenges Leading and demonstrating emerging technologies and concepts to teams Requirements Requirements Bachelor's Degree desired in Computer Science, Information Technology, or related field 2+ years of experience with data visualization Expert knowledge of Data Management, Business Intelligence and Analytics concepts including: Bachelor's Degree desired in Computer Science, Information Technology, or related field 2+ years of experience with data visualization Expert knowledge of Data Management, Business Intelligence and Analytics concepts including: Extract, Transform, Load (ETL) Data Warehousing, Data Lakes, Data Marts, Data Stores, and Cubes Extract, Transform, Load (ETL) Data Warehousing, Data Lakes, Data Marts, Data Stores, and Cubes Data Quality and Profiling Data Governance, Master Data Management, and Metadata Management Data Visualization, Dashboard, Reporting, Self Service Business Intelligence (BI), Key Performance Indicators (KPI), and Scorecards Predictive, Prescription, and Descriptive Analytics Data Quality and Profiling Data Governance, Master Data Management, and Metadata Management Data Visualization, Dashboard, Reporting, Self Service Business Intelligence (BI), Key Performance Indicators (KPI), and Scorecards Predictive, Prescription, and Descriptive Analytics Expert knowledge of Microsoft Business Intelligence stack including Power BI, Excel and SQL Server (SSAS, SSRS, SSIS) Expert knowledge of Microsoft Business Intelligence stack including Power BI, Excel and SQL Server (SSAS, SSRS, SSIS) Minimum of 3-5 years of experience (of which 2-3 years in consulting) Experience with Azure Data Estate (Azure SQL DB, Cosmos DB, Azure SQL DW, Azure Data Lake) is a plus Azure ML Studio/Services experience a plus Certifications are a plus Advanced knowledge of SQL, including ability to write stored procedures, triggers, analytic functions, and tuning is a plus Minimum of 3-5 years of experience (of which 2-3 years in consulting) Experience with Azure Data Estate (Azure SQL DB, Cosmos DB, Azure SQL DW, Azure Data Lake) is a plus Azure ML Studio/Services experience a plus Certifications are a plus Advanced knowledge of SQL, including ability to write stored procedures, triggers, analytic functions, and tuning is a plus Knowledge of relational and dimensional database structures, theories, principles, and practice is a plus Experience with Data Science tools, technologies and techniques (R, Python, algorithms) is a plus Passionate about learning new technologies Ability to learn new concepts and software quickly Analytical approach to problem-solving; ability to use technology to solve business problems Familiarity with database-centric applications Ability to communicate effectively in both a technical and non-technical manner Ability to work in a fast-paced environment Ability to travel up to 30% Knowledge of relational and dimensional database structures, theories, principles, and practice is a plus Experience with Data Science tools, technologies and techniques (R, Python, algorithms) is a plus Passionate about learning new technologies Ability to learn new concepts and software quickly Analytical approach to problem-solving; ability to use technology to solve business problems Familiarity with database-centric applications Ability to communicate effectively in both a technical and non-technical manner Ability to work in a fast-paced environment Ability to travel up to 30% Don't meet every single requirement? At 3Cloud we are dedicated to building a diverse, inclusive and authentic workplace, so if you're excited about this role but your past experience doesn't align perfectly with every qualification in the job description, we encourage you to apply anyways. At this time, we cannot sponsor applicants for work visas. Candidates should be based in the United States. At this time, we cannot sponsor applicants for work visas. Candidates should be based in the United States. About 3Cloud About 3Cloud 3Cloud is a ""born in the cloud"" Gold-Certified Microsoft Azure technology consulting firm and Azure Expert Managed Services Provider that provides cloud strategy, design, implementation, acquisition and ongoing managed services to its clients across various industries. To meet the needs of its clients, 3Cloud offers comprehensive services across Cloud Infrastructure, DevOps & Automation, Cloud Application Development and Data, Analytics and AI. Founded by former Microsoft technology leaders, 3Cloud uniquely offers: (1) highly experienced and proven cloud architects and technologists, (2) strong business strategy, financial acumen and operational proficiency and (3) deep relationship and network into Microsoft engineering and field leadership. 3Cloud Total Rewards Highlights Include: Flexible work location with a virtual first approach to work! 401(K) with match up to 50% of your 6% contributions of eligible pay Generous PTO providing a minimum of 15 days in addition to 9 paid company holidays and 2 floating personal days Three medical plan options to allow you the choice to elect what works best for you! Option for vision and dental coverage 100% employer premium coverage for STD and LTD Paid leave for birth parents and non-birth parents Option for FSA, HSA, HRA and Dependent Care $67.00 monthly tech and home office allowance Utilization and/or discretionary bonus eligibility based on role Robust Employee Assistance Program to help with everyday challenges 3Cloud offers competitive compensation. In addition to base pay employees are eligible to receive an annual discretionary/utilization bonus. If you are hired at 3Cloud your final base salary is based on factors such as skills, education, experience and/or geographic location. Please keep in mind that the range mentioned above includes the full base salary range for the role. It is not typical for offers to be made at or near the top of the range. Base Salary Range $90,000—$182,000 USD 3Cloud Total Rewards Highlights Include: Flexible work location with a virtual first approach to work! 401(K) with match up to 50% of your 6% contributions of eligible pay Generous PTO providing a minimum of 15 days in addition to 9 paid company holidays and 2 floating personal days Three medical plan options to allow you the choice to elect what works best for you! Option for vision and dental coverage 100% employer premium coverage for STD and LTD Paid leave for birth parents and non-birth parents Option for FSA, HSA, HRA and Dependent Care $67.00 monthly tech and home office allowance Utilization and/or discretionary bonus eligibility based on role Robust Employee Assistance Program to help with everyday challenges 3Cloud offers competitive compensation. In addition to base pay employees are eligible to receive an annual discretionary/utilization bonus. If you are hired at 3Cloud your final base salary is based on factors such as skills, education, experience and/or geographic location. Please keep in mind that the range mentioned above includes the full base salary range for the role. It is not typical for offers to be made at or near the top of the range. 3Cloud Total Rewards Highlights Include: 3Cloud Total Rewards Highlights Include: Flexible work location with a virtual first approach to work! 401(K) with match up to 50% of your 6% contributions of eligible pay Generous PTO providing a minimum of 15 days in addition to 9 paid company holidays and 2 floating personal days Three medical plan options to allow you the choice to elect what works best for you! Option for vision and dental coverage 100% employer premium coverage for STD and LTD Paid leave for birth parents and non-birth parents Option for FSA, HSA, HRA and Dependent Care $67.00 monthly tech and home office allowance Utilization and/or discretionary bonus eligibility based on role Robust Employee Assistance Program to help with everyday challenges Flexible work location with a virtual first approach to work! 401(K) with match up to 50% of your 6% contributions of eligible pay Generous PTO providing a minimum of 15 days in addition to 9 paid company holidays and 2 floating personal days Three medical plan options to allow you the choice to elect what works best for you! Option for vision and dental coverage 100% employer premium coverage for STD and LTD Paid leave for birth parents and non-birth parents Option for FSA, HSA, HRA and Dependent Care $67.00 monthly tech and home office allowance Utilization and/or discretionary bonus eligibility based on role Robust Employee Assistance Program to help with everyday challenges Base Salary Range Base Salary Range $90,000—$182,000 USD Don't meet every single requirement? At 3Cloud we are dedicated to building a diverse, inclusive and authentic workplace, so if you're excited about this role but your past experience doesn't align perfectly with every qualification in the job description, we encourage you to apply anyway. At this time, we cannot sponsor applicants for work visas. Don't meet every single requirement? At 3Cloud we are dedicated to building a diverse, inclusive and authentic workplace, so if you're excited about this role but your past experience doesn't align perfectly with every qualification in the job description, we encourage you to apply anyway. At this time, we cannot sponsor applicants for work visas. At this time, we cannot sponsor applicants for work visas.",4.3,501 to 1000 Employees,2016,Company - Private,Information Technology Support Services,Information Technology,Unknown / Non-Applicable
"El Segundo, CA",AWS Data Engineer,$68.00 - $72.00 Per Hour (Employer est.),Apolis,"Skills: Python, Snowflake, AWS, Data Bricks Skills: Python, Snowflake, AWS, Data Bricks Must Haves: Must Haves: 5+ years of experience within data engineering 5+ years of experience within data engineering Python experience Python experience Snowflake experience (developing ETL pipelines) Snowflake experience (developing ETL pipelines) Data Bricks experience Data Bricks experience Writing Workflows and Notebooks in python/scala/sql Writing Workflows and Notebooks in python/scala/sql Knowledge of AWS Knowledge of AWS Fluent in English (written and verbal) Kafka experience (loading batch and streaming data) Job Type: Contract Salary: $68.00 - $72.00 per hour Ability to commute/relocate: El Segundo, CA 90245: Reliably commute or planning to relocate before starting work (Required) El Segundo, CA 90245: Reliably commute or planning to relocate before starting work (Required) Experience: AWS Data bricks: 3 years (Required) Kafka: 3 years (Preferred) Python: 3 years (Preferred) Scala/Java: 3 years (Required) AWS Data bricks: 3 years (Required) Kafka: 3 years (Preferred) Python: 3 years (Preferred) Scala/Java: 3 years (Required) Work Location: In person",3.8,501 to 1000 Employees,1996,Company - Private,Information Technology Support Services,Information Technology,$25 to $100 million (USD)
"Dallas, TX",Data Engineer,$77K - $102K (Glassdoor est.),Anblicks,"Dallas, Texas | Full Time Dallas, Texas | Full Time Data Engineer Design and develop data pipeline models to extract, transform and load data from heterogenous source systems onto common data repository/data lake for provisioning data to various downstream teams to help them build their reports and dashboards for various analytics purposes and derive key business insights to enhance the user/customer satisfaction. Use Python, PySpark, Hive QL, Spark, Snowflake, BitBucket, Autosys, Jenkins CI/CD, Tableau, JuPyter, and ShellScripting to design, develop, extract, load, build, and code data models and frameworks. Gather requirements from stakeholders and identify the source data systems that provide transactional data. Build reusable framework and code scripts using python, spark, shell and yaml to automate data pipelines on top of distributed Hadoop cluster. Perform unit and regression testing to make sure developed data pipeline framework meets data quality and integrity standards. Generate key performance indicators metrics from data stored on reporting layer which is built by creating views on top of data objects in data lake and communicate with business teams in regular intervals when any deviations observed in data. May require travel/relocation to unanticipated work locations within the USA. Require Master of Science in Computer Science/Engineering, computer/management Information Systems/Technology, or related field and 1 year of experience in the job offered, software engineer/developer, data architect/engineer, Hadoop developer/associate/consultant, or related field. Please mail your resume to Maruthi Technologies, Inc, 14911 Quorum Drive, Suite 390, Dallas, TX 75254 Data Engineer Design and develop data pipeline models to extract, transform and load data from heterogenous source systems onto common data repository/data lake for provisioning data to various downstream teams to help them build their reports and dashboards for various analytics purposes and derive key business insights to enhance the user/customer satisfaction. Use Python, PySpark, Hive QL, Spark, Snowflake, BitBucket, Autosys, Jenkins CI/CD, Tableau, JuPyter, and ShellScripting to design, develop, extract, load, build, and code data models and frameworks. Gather requirements from stakeholders and identify the source data systems that provide transactional data. Build reusable framework and code scripts using python, spark, shell and yaml to automate data pipelines on top of distributed Hadoop cluster. Perform unit and regression testing to make sure developed data pipeline framework meets data quality and integrity standards. Generate key performance indicators metrics from data stored on reporting layer which is built by creating views on top of data objects in data lake and communicate with business teams in regular intervals when any deviations observed in data. May require travel/relocation to unanticipated work locations within the USA. Require Master of Science in Computer Science/Engineering, computer/management Information Systems/Technology, or related field and 1 year of experience in the job offered, software engineer/developer, data architect/engineer, Hadoop developer/associate/consultant, or related field. Please mail your resume to Maruthi Technologies, Inc, 14911 Quorum Drive, Suite 390, Dallas, TX 75254 Data Engineer Design and develop data pipeline models to extract, transform and load data from heterogenous source systems onto common data repository/data lake for provisioning data to various downstream teams to help them build their reports and dashboards for various analytics purposes and derive key business insights to enhance the user/customer satisfaction. Use Python, PySpark, Hive QL, Spark, Snowflake, BitBucket, Autosys, Jenkins CI/CD, Tableau, JuPyter, and ShellScripting to design, develop, extract, load, build, and code data models and frameworks. Gather requirements from stakeholders and identify the source data systems that provide transactional data. Build reusable framework and code scripts using python, spark, shell and yaml to automate data pipelines on top of distributed Hadoop cluster. Perform unit and regression testing to make sure developed data pipeline framework meets data quality and integrity standards. Generate key performance indicators metrics from data stored on reporting layer which is built by creating views on top of data objects in data lake and communicate with business teams in regular intervals when any deviations observed in data. May require travel/relocation to unanticipated work locations within the USA. Require Master of Science in Computer Science/Engineering, computer/management Information Systems/Technology, or related field and 1 year of experience in the job offered, software engineer/developer, data architect/engineer, Hadoop developer/associate/consultant, or related field. Please mail your resume to Maruthi Technologies, Inc, 14911 Quorum Drive, Suite 390, Dallas, TX 75254",4.1,501 to 1000 Employees,2004,Company - Private,Information Technology Support Services,Information Technology,$25 to $100 million (USD)
"Scottsdale, AZ",Data Engineer,$100K - $120K (Employer est.),Mind Mint,"JOIN AN AMAZING COMPANY AND GLOBALLY RECOGNIZED BRAND DIVE INTO INTRIGUING PROJECTS AND LEVEL UP YOUR SKILLS WORK WITH A FUN TEAM ONSITE IN SCOTTSDALE ARIZONA  Are you ready to embark on an exciting challenge in the field of data engineering?  Do you possess a genuine passion for unraveling patterns and crafting insightful solutions through data engineering?  Can you envision yourself delivering impactful outcomes that align with our company's objectives through data-driven solutions?  If so, we have an opportunity that will ignite your enthusiasm for data engineering and propel your career to new heights.  Mastermind.com, created in partnership by Dean Graziosi and Tony Robbins, is the #1 online platform for people who are looking to market and monetize their knowledge base. Mastermind has a worldwide following and touches lives all over the world.  We are seeking a world-class Data Engineer to join our IT team. Here is your chance to help take our company to new heights.  THE ROLE  ️ Build tools and solutions that automate complex workflows  ️ Create data pipelines and maintain the infrastructure and architecture for data generation, storage, and processing  Build systems that collect, manage, and convert raw data into usable information  This is a role that involves solving complex technical issues and working collaboratively with our team. You will report directly to the Chief Technology Officer.  Our interview process will include a technical assessment and peer code review to assess your technical proficiency.  This Data Engineer position is based in Phoenix, Arizona, and will require you to work from the Mastermind headquarters in Scottsdale, AZ.  We provide an excellent compensation model based on experience ranging from $100k-120k.  This opportunity is only available for USA residents with valid work authorization. We DO NOT offer sponsorship or relocation.  REQUIREMENTS You must have 5 years experience in designing and maintaining MySQL You must have experience with cloud data warehouses (preferably BigQuery), dbt, or Python experience You must have SQL development experience You must have experience with spreadsheet manipulation in SQL You must be able to write custom queries to answer any question about the data without leaving a SQL environment. You must have experience querying and manipulating large datasets Bachelor’s degree in a technical field or equivalent related work experience Ability to utilize Fivetran and Stitch for extracting and loading data into BigQuery Strong understanding of database design Experience with Web APIs and pulling data from them, preferably in Python, is a plus Excellent problem-solving and communication skills Experience querying and manipulating large datasets Experience with data parsing, scripting, and automation  RESPONSIBILITIES  Design and create database objects, such as tables, stored procedures, and views. Conduct technical research and data profiling on various data sources, utilizing technologies like Python, APIs, and SQL. Innovatively propose technical data solutions to address business challenges. Perform dimensional data modeling and optimize database objects for accessibility, performance, and consistency. Collaborate with business stakeholders to gather and understand data requirements. Communicate data concepts, reports, KPIs, and other technical subjects in a business-friendly language. Develop ETL applications using SQL, Python, etc., for data extraction, transformation, and loading. Document development standards, KPI calculations, business terms, table diagrams, and other relevant information related to data and reports. Keep up-to-date with emerging technologies and trends in data engineering Perform other duties as assigned.  PERKS & BENEFITS  Competitive salary and compensation Excellent Medical benefits EOY Profit Sharing 401(k) administration and matching program Incredible opportunities for growth and development Amazing in-office culture Become part of a mission-team making a difference!  HOW TO APPLY  Ready to dive into the fascinating realm of data engineering and take your skills to new heights as a Data Engineer?  If so, we invite you to join our exceptional team, where you'll have the opportunity to unleash the power of data, shape the future of data-driven decision-making, and embark on a fulfilling career journey.  Click the ""Apply Now"" button below to take the first step toward an exciting future. We can't wait to review your application and explore the endless possibilities of working together.  About Mastermind.com Mastermind.com, created in partnership by Dean Graziosi and Tony Robbins, is the #1 online platform for people who are looking to market and monetize their knowledge base. We are redefining what ""Self-Education"" means to the world.  Mastermind is not just ""another software"" but an all-in-one platform for Education, Entertainment, Implementation & Community. Mastermind serves people worldwide who seek transformation, fulfillment, and success outside the traditional education path.  The Mastermind software empowers and enables you to implement what you learn & actually get paid, in addition providing a community where you are surrounded by like-minded individuals cheering you on to YOUR NEXT LEVEL. JOIN AN AMAZING COMPANY AND GLOBALLY RECOGNIZED BRAND JOIN AN AMAZING COMPANY AND GLOBALLY RECOGNIZED BRAND DIVE INTO INTRIGUING PROJECTS AND LEVEL UP YOUR SKILLS DIVE INTO INTRIGUING PROJECTS AND LEVEL UP YOUR SKILLS WORK WITH A FUN TEAM ONSITE IN SCOTTSDALE ARIZONA WORK WITH A FUN TEAM ONSITE IN SCOTTSDALE ARIZONA Are you ready to embark on an exciting challenge in the field of data engineering? exciting challenge in the field of data engineering Do you possess a genuine passion for unraveling patterns and crafting insightful solutions through data engineering? crafting insightful solutions Can you envision yourself delivering impactful outcomes that align with our company's objectives through data-driven solutions? delivering impactful outcomes If so, we have an opportunity that will ignite your enthusiasm for data engineering and propel your career to new heights. Mastermind.com, created in partnership by Dean Graziosi and Tony Robbins, is the #1 online platform for people who are looking to market and monetize their knowledge base. Mastermind has a worldwide following and touches lives all over the world. We are seeking a world-class Data Engineer to join our IT team. Here is your chance to help take our company to new heights. Data Engineer THE ROLE THE ROLE ️ Build tools and solutions that automate complex workflows ️ Create data pipelines and maintain the infrastructure and architecture for data generation, storage, and processing Build systems that collect, manage, and convert raw data into usable information This is a role that involves solving complex technical issues and working collaboratively with our team. You will report directly to the Chief Technology Officer. Chief Technology Officer Our interview process will include a technical assessment and peer code review to assess your technical proficiency. technical assessment This Data Engineer position is based in Phoenix, Arizona, and will require you to work from the Mastermind headquarters in Scottsdale, AZ. This Data Engineer position is based in Phoenix, Arizona, and will require you to work from the Mastermind headquarters in Scottsdale, AZ. We provide an excellent compensation model based on experience ranging from $100k-120k. $100k-120k. This opportunity is only available for USA residents with valid work authorization. We DO NOT offer sponsorship or relocation. only available for USA residents We DO NOT offer sponsorship or relocation. REQUIREMENTS REQUIREMENTS You must have 5 years experience in designing and maintaining MySQL You must have experience with cloud data warehouses (preferably BigQuery), dbt, or Python experience You must have SQL development experience You must have experience with spreadsheet manipulation in SQL You must be able to write custom queries to answer any question about the data without leaving a SQL environment. You must have experience querying and manipulating large datasets Bachelor’s degree in a technical field or equivalent related work experience Ability to utilize Fivetran and Stitch for extracting and loading data into BigQuery Strong understanding of database design Experience with Web APIs and pulling data from them, preferably in Python, is a plus Excellent problem-solving and communication skills Experience querying and manipulating large datasets Experience with data parsing, scripting, and automation You must have 5 years experience in designing and maintaining MySQL You must have experience with cloud data warehouses (preferably BigQuery), dbt, or Python experience You must have SQL development experience You must have experience with spreadsheet manipulation in SQL You must be able to write custom queries to answer any question about the data without leaving a SQL environment. You must have experience querying and manipulating large datasets Bachelor’s degree in a technical field or equivalent related work experience Ability to utilize Fivetran and Stitch for extracting and loading data into BigQuery Strong understanding of database design Experience with Web APIs and pulling data from them, preferably in Python, is a plus Excellent problem-solving and communication skills Experience querying and manipulating large datasets Experience with data parsing, scripting, and automation RESPONSIBILITIES RESPONSIBILITIES Design and create database objects, such as tables, stored procedures, and views. Conduct technical research and data profiling on various data sources, utilizing technologies like Python, APIs, and SQL. Innovatively propose technical data solutions to address business challenges. Perform dimensional data modeling and optimize database objects for accessibility, performance, and consistency. Collaborate with business stakeholders to gather and understand data requirements. Communicate data concepts, reports, KPIs, and other technical subjects in a business-friendly language. Develop ETL applications using SQL, Python, etc., for data extraction, transformation, and loading. Document development standards, KPI calculations, business terms, table diagrams, and other relevant information related to data and reports. Keep up-to-date with emerging technologies and trends in data engineering Perform other duties as assigned. Design and create database objects, such as tables, stored procedures, and views. Conduct technical research and data profiling on various data sources, utilizing technologies like Python, APIs, and SQL. Innovatively propose technical data solutions to address business challenges. Perform dimensional data modeling and optimize database objects for accessibility, performance, and consistency. Collaborate with business stakeholders to gather and understand data requirements. Communicate data concepts, reports, KPIs, and other technical subjects in a business-friendly language. Develop ETL applications using SQL, Python, etc., for data extraction, transformation, and loading. Document development standards, KPI calculations, business terms, table diagrams, and other relevant information related to data and reports. Keep up-to-date with emerging technologies and trends in data engineering Perform other duties as assigned. PERKS & BENEFITS PERKS & BENEFITS Competitive salary and compensation Excellent Medical benefits EOY Profit Sharing 401(k) administration and matching program Incredible opportunities for growth and development Amazing in-office culture Become part of a mission-team making a difference! HOW TO APPLY HOW TO APPLY Ready to dive into the fascinating realm of data engineering and take your skills to new heights as a Data Engineer? Data Engineer If so, we invite you to join our exceptional team, where you'll have the opportunity to unleash the power of data, shape the future of data-driven decision-making, and embark on a fulfilling career journey. Click the ""Apply Now"" button below to take the first step toward an exciting future. ""Apply Now We can't wait to review your application and explore the endless possibilities of working together. About Mastermind.com About Mastermind.com Mastermind.com, created in partnership by Dean Graziosi and Tony Robbins, is the #1 online platform for people who are looking to market and monetize their knowledge base. We are redefining what ""Self-Education"" means to the world. Mastermind is not just ""another software"" but an all-in-one platform for Education, Entertainment, Implementation & Community. Mastermind serves people worldwide who seek transformation, fulfillment, and success outside the traditional education path. The Mastermind software empowers and enables you to implement what you learn & actually get paid, in addition providing a community where you are surrounded by like-minded individuals cheering you on to YOUR NEXT LEVEL.",N/A,Unknown,N/A,Company - Public,N/A,N/A,Unknown / Non-Applicable
Remote,Data Analytics Engineer,$60.00 Per Hour (Employer est.),Proits Hub LLC,"JD and Details Current location: Santa Clara Valley (Cupertino) Position: Digital Marketing Data Analyst Position Type: Hybrid - Long Term Contract JD: KEY QUALIFICATIONS Business Skills JD: KEY QUALIFICATIONS Business Skills Experienced data scientist with 5+ years of experience developing data reporting solutions and strategy across digital marketing platforms and social ecosystems Experience analyzing the launch of new products, services, or websites. In-depth knowledge of digital analytics data, measurement, methodologies and industry standards. Excellent communicator with good design sense and natural presentations skills that can weave results into a narrative for business and creative teams to comprehend easily. Curiosity to dive below the surface and identify important strategic implications in the data. Can take a project from start to finish with minimal supervision. Great collaborator and coordinator with the proven ability to work across teams to deliver quality deliverables on time. Able to balance multiple projects and assignments with a focus on efficiency. Detail-oriented, organized, and patient. Experienced data scientist with 5+ years of experience developing data reporting solutions and strategy across digital marketing platforms and social ecosystems Experience analyzing the launch of new products, services, or websites. In-depth knowledge of digital analytics data, measurement, methodologies and industry standards. Excellent communicator with good design sense and natural presentations skills that can weave results into a narrative for business and creative teams to comprehend easily. Curiosity to dive below the surface and identify important strategic implications in the data. Can take a project from start to finish with minimal supervision. Great collaborator and coordinator with the proven ability to work across teams to deliver quality deliverables on time. Able to balance multiple projects and assignments with a focus on efficiency. Detail-oriented, organized, and patient. Technical Skills Technical Skills Strong working knowledge of Adobe Analytics, Google Analytics or other web measurement platforms (Adobe Preferred). Proficiency with Tableau or other Data Visualization tools Strong working knowledge of SQL, Snowflake experience preferred Strong working knowledge of Adobe Analytics, Google Analytics or other web measurement platforms (Adobe Preferred). Proficiency with Tableau or other Data Visualization tools Strong working knowledge of SQL, Snowflake experience preferred Job Type: Contract Salary: Up to $60.00 per hour Schedule: Monday to Friday Monday to Friday Work Location: Remote",N/A,1 to 50 Employees,N/A,Company - Public,N/A,N/A,Unknown / Non-Applicable
"Naperville, IL",Staff Data Engineer,$116K - $140K (Employer est.),Egen Solutions Inc,"Egen Data Engineering teams build scalable data pipelines using Python, Spark, and cloud services (GCP and AWS). The pipelines we build typically integrate with technologies such as Kafka, Storm, and Elasticsearch. We are working on a continuous deployment pipeline that leverages rapid on-demand releases. Our developers work in an agile process to efficiently deliver high value applications and product packages. As a Staff Data Engineer at Egen, you will leverage Spark and GCP (preferred) to architect and implement cloud-native data pipelines and infrastructure to enable analytics and machine learning on rich datasets. Required Experience: Required Experience: Built and run resilient data pipelines in production and have implemented ETL/ELT to load a multi-terabyte enterprise data warehouse. Implemented analytics applications using multiple database technologies, such as relational, multidimensional (OLAP), key-value, document, or graph. Defined data contracts, and wrote specifications including REST APIs. Transformed data between data models and formats with the most modern PySpark practices. Have built cloud-native applications and supporting technologies / patterns / practices including: Cloud Services, Docker, CI/CD, DevOps, and microservices. Planned and designed artifacts that describe software architectures involving multiple systems and technologies You've worked in agile environments and are comfortable iterating quickly. Built and run resilient data pipelines in production and have implemented ETL/ELT to load a multi-terabyte enterprise data warehouse. Implemented analytics applications using multiple database technologies, such as relational, multidimensional (OLAP), key-value, document, or graph. Defined data contracts, and wrote specifications including REST APIs. Transformed data between data models and formats with the most modern PySpark practices. Have built cloud-native applications and supporting technologies / patterns / practices including: Cloud Services, Docker, CI/CD, DevOps, and microservices. Planned and designed artifacts that describe software architectures involving multiple systems and technologies You've worked in agile environments and are comfortable iterating quickly. Nice to have's (but not required): Nice to have's (but not required): GCP expertise is preferred but will consider AWS Experience moving trained machine learning models into production data pipelines. Experience in biotech, genomics, clinical research or precision medicine. Expert knowledge of relational database modeling concepts, SQL skills, proficiency in query performance tuning, and desire to share knowledge with others. GCP expertise is preferred but will consider AWS Experience moving trained machine learning models into production data pipelines. Experience in biotech, genomics, clinical research or precision medicine. Expert knowledge of relational database modeling concepts, SQL skills, proficiency in query performance tuning, and desire to share knowledge with others. This is full-time, On-site, and Remote. We are based out in Naperville, IL. On-site, and Remote. Naperville, IL Looking forward to striking up a conversation with you on the opportunity!, Kindly visit our website egen.solutions Looking forward to striking up a conversation with you on the opportunity! Job Type: Full-time Pay: $116,284.90 - $140,042.03 per year Benefits: 401(k) Dental insurance Health insurance 401(k) Dental insurance Health insurance Experience level: 5 years 5 years Schedule: Monday to Friday Monday to Friday Experience: Data Engineer: 5 years (Preferred) Data Engineer: 5 years (Preferred) Work Location: Hybrid remote in Naperville, IL 60540",N/A,,,,,,
Remote,Cloud Data Engineer,$140K - $160K (Employer est.),Alludo,"Cloud Data Engineer Push the boundaries of tech. In your sweatpants. Cloud Data Engineer We’re looking for an experienced Cloud Data Engineer to help us change how the world works. Here, you’ll be part of our Data Engineering & Analytics team, supporting cross-functional groups around the world. The right candidate will develop, review, and maintain data infrastructure and various data. You will also develop means to ensure continuous data validation and telemetry for all data processes. The top creative and technical minds could work anywhere. So why are so many of them choosing Alludo? Here are three reasons: This is the moment. It’s an exciting time at Alludo, with new leadership, a refreshed brand (you probably know us as Corel!), and a whole new approach to changing the way the world works. We’re at the forefront of a movement, and we want you to ride this wave with us. We want you* to be *you. Too often, companies tell you about their culture and then expect you to fit it. Our culture is built from the people who work here. We want you to feel safe to be who you are, take risks, and show us what you’ve got. It’s your world. We know you have a life. We want to be part of it, but not all of it. At Alludo, we’re serious about empowering people to work when, how, and where they want. Couch? Sweatpants? Cool with us. We believe that happy employees mean happy customers. That’s why we hire amazing people and get out of their way. This is the moment. It’s an exciting time at Alludo, with new leadership, a refreshed brand (you probably know us as Corel!), and a whole new approach to changing the way the world works. We’re at the forefront of a movement, and we want you to ride this wave with us. This is the moment. We want you* to be *you. Too often, companies tell you about their culture and then expect you to fit it. Our culture is built from the people who work here. We want you to feel safe to be who you are, take risks, and show us what you’ve got. We want you *you . It’s your world. We know you have a life. We want to be part of it, but not all of it. At Alludo, we’re serious about empowering people to work when, how, and where they want. Couch? Sweatpants? Cool with us. We believe that happy employees mean happy customers. That’s why we hire amazing people and get out of their way. It’s your world. Sound good so far? Awesome. Let’s talk more about the role and see if we’re destined to be together. THE ROLE: Design, develop and implement large scale, high-volume, high-performance data infrastructure and pipelines for Data Lake and Data Warehouse THE ROLE: Build and implement ETL frameworks to improve code quality and reliability Build and enforce common design patterns to increase code maintainability Ensure accuracy and consistency of data processing, results, and reporting Design cloud-native data pipelines, automation routines, and database schemas that can be leveraged to do predictive and prescriptive machine learning. Communicate ideas clearly, both verbally and through concise documentation, to various business sponsors, business analysts and technical resources Build and implement ETL frameworks to improve code quality and reliability Build and enforce common design patterns to increase code maintainability Ensure accuracy and consistency of data processing, results, and reporting Design cloud-native data pipelines, automation routines, and database schemas that can be leveraged to do predictive and prescriptive machine learning. Communicate ideas clearly, both verbally and through concise documentation, to various business sponsors, business analysts and technical resources YOU: YOU: 5+ years of professional experience 3+ years of experience working in data engineering, business intelligence, or a similar role 2+ years of experience in ETL orchestration and workflow management tools like Airflow, flink, etc. using AWS/GCP (i.e Airflow, Luigi, Prefect, Dagster, digdag.io, Google Cloud Composer, AWS Step Functions, Azure Data Factory, UC4, Control-M) 1+ years of experience with the Distributed data/similar ecosystem (Spark, Hive, Druid, Presto) and streaming technologies such as Kafka/Flink Expert knowledge of at least one programming language such as Python/Java, Python preferred Expert knowledge of SQL Familiarity with DevOps SnowFlake, Netezza, Teradata, AWS Redshift, Google BigQuery, Azure Data Warehouse, or similar Experience with cloud service providers: Microsoft Azure, Amazon AWS Expertise with containerization orchestration engines (Kubernetes) BS in Computer Science, Software Engineering, or relevant field desired 5+ years of professional experience 3+ years of experience working in data engineering, business intelligence, or a similar role 2+ years of experience in ETL orchestration and workflow management tools like Airflow, flink, etc. using AWS/GCP (i.e Airflow, Luigi, Prefect, Dagster, digdag.io, Google Cloud Composer, AWS Step Functions, Azure Data Factory, UC4, Control-M) 1+ years of experience with the Distributed data/similar ecosystem (Spark, Hive, Druid, Presto) and streaming technologies such as Kafka/Flink Expert knowledge of at least one programming language such as Python/Java, Python preferred Expert knowledge of SQL Familiarity with DevOps SnowFlake, Netezza, Teradata, AWS Redshift, Google BigQuery, Azure Data Warehouse, or similar Experience with cloud service providers: Microsoft Azure, Amazon AWS Expertise with containerization orchestration engines (Kubernetes) BS in Computer Science, Software Engineering, or relevant field desired About Alludo: Alludo is a beloved and trusted industry titan fueled by make-everything-easier flexibility. With a 30+ year legacy of innovation, we understand where you've been and we're uniquely equipped to get you where you want to be. Our comprehensive collection of creative, collaborative, and productivity solutions propel your teams on their journey. From meeting your deadlines to realizing your dreams, Alludo empowers all you do. Our products enable millions of connected knowledge workers around the world to do great work faster. Our success is driven by an unwavering commitment to deliver a broad portfolio of innovative applications – including CorelDRAW®, MindManager®, Parallels®, and WinZip® – to inspire users and help them achieve their goals. It is our policy and practice to offer equal employment opportunities to all qualified applicants and employees without regard to race, color, age, religion, national origin, sex, political affiliation, sexual orientation, marital status, disability, veteran status, genetics, or any other protected characteristic. Alludo is committed to an inclusive, barrier-free recruitment and selection process and work environment. If you are contacted for a job opportunity, please advise us of any accommodations that are required. Appropriate accommodations will be provided upon request as required by Federal and Provincial regulations and Company Policy. Any information received relating to accommodations will be treated as confidential. About Alludo: Job Type: Full-time Pay: $140,000.00 - $160,000.00 per year Schedule: Monday to Friday Monday to Friday Work Location: Remote",3.5,501 to 1000 Employees,1985,Company - Private,Computer Hardware Development,Information Technology,$100 to $500 million (USD)
"Addison, TX",Data Engineer,N/A,N J Malin & Associates,"Data Engineer Duties and Responsibilities Assemble large, complex sets of data that meet non-functional and functional business requirements Identifying, designing and implementing internal process improvements including re-designing infrastructure for greater scalability, optimizing data delivery, and automating manual processes Building required infrastructure for optimal extraction, transformation and loading of data from various data sources using AWS and SQL technologies Building analytical tools to utilize the data pipeline, providing actionable insight into key business performance metrics including operational efficiency and customer acquisition Working with stakeholders including data, design, product and executive teams and assisting them with data-related technical issues Working with stakeholders including the Executive, Product, Data and Design teams to support their data infrastructure needs while assisting with data-related technical issues Skills and Qualifications Ability to build and optimize data sets, ‘big data' data pipelines and architectures Ability to understand and build complex data models for Power BI reporting. Ability to perform root cause analysis on external and internal processes and data to identify opportunities for improvement and answer questions Excellent analytic skills associated with working on unstructured datasets Ability to build processes that support data transformation, workload management, data structures, dependency and metadata Must Have experience in (in order of importance) 10 + years experience as a Data Engineer or similar role. SQL Server T-SQL code Administrator functions Data Factory Python Power BI DAX code M code Azure Malin is an Equal Opportunity Employer - M/F/Veteran/Disability/Sexual Orientation/Gender Identity Data Engineer Duties and Responsibilities Data Engineer Duties and Responsibilities Assemble large, complex sets of data that meet non-functional and functional business requirements Identifying, designing and implementing internal process improvements including re-designing infrastructure for greater scalability, optimizing data delivery, and automating manual processes Building required infrastructure for optimal extraction, transformation and loading of data from various data sources using AWS and SQL technologies Building analytical tools to utilize the data pipeline, providing actionable insight into key business performance metrics including operational efficiency and customer acquisition Working with stakeholders including data, design, product and executive teams and assisting them with data-related technical issues Working with stakeholders including the Executive, Product, Data and Design teams to support their data infrastructure needs while assisting with data-related technical issues Assemble large, complex sets of data that meet non-functional and functional business requirements Identifying, designing and implementing internal process improvements including re-designing infrastructure for greater scalability, optimizing data delivery, and automating manual processes Building required infrastructure for optimal extraction, transformation and loading of data from various data sources using AWS and SQL technologies Building analytical tools to utilize the data pipeline, providing actionable insight into key business performance metrics including operational efficiency and customer acquisition Working with stakeholders including data, design, product and executive teams and assisting them with data-related technical issues Working with stakeholders including the Executive, Product, Data and Design teams to support their data infrastructure needs while assisting with data-related technical issues Skills and Qualifications Skills and Qualifications Ability to build and optimize data sets, ‘big data' data pipelines and architectures Ability to understand and build complex data models for Power BI reporting. Ability to perform root cause analysis on external and internal processes and data to identify opportunities for improvement and answer questions Excellent analytic skills associated with working on unstructured datasets Ability to build processes that support data transformation, workload management, data structures, dependency and metadata Ability to build and optimize data sets, ‘big data' data pipelines and architectures Ability to understand and build complex data models for Power BI reporting. Ability to perform root cause analysis on external and internal processes and data to identify opportunities for improvement and answer questions Excellent analytic skills associated with working on unstructured datasets Ability to build processes that support data transformation, workload management, data structures, dependency and metadata Must Have experience in (in order of importance) Must Have experience in (in order of importance) 10 + years experience as a Data Engineer or similar role. SQL Server T-SQL code Administrator functions Data Factory Python Power BI DAX code M code Azure 10 + years experience as a Data Engineer or similar role. SQL Server T-SQL code Administrator functions T-SQL code Administrator functions T-SQL code Administrator functions Data Factory Python Python Python Power BI DAX code M code DAX code M code DAX code M code Azure Malin is an Equal Opportunity Employer - M/F/Veteran/Disability/Sexual Orientation/Gender Identity Malin is an Equal Opportunity Employer - M/F/Veteran/Disability/Sexual Orientation/Gender Identity",3.7,501 to 1000 Employees,1971,Company - Private,Shipping & Trucking,Transportation & Logistics,$100 to $500 million (USD)
"Charlottesville, VA",Data Engineer,$101K - $120K (Employer est.),Anytime Infotech LLC.,"Role : Data Engineer (US Citizen Only) Role : Data Engineer (US Citizen Only) Location : Chantilly, VA (Onsite Role) Location : Chantilly, VA (Onsite Role) Experience : 5+ Years Experience : 5+ Years Skills: Python, Java, Scala, UNIX, Linux, Solaris, ssh, git., Data manipulation, SQL, relational databases, and/or NoSWL databases., AWS, stack configuration and management Skills: Job Description: Job Description: As a Data Engineer, you will support robust and repeatable data manipulation, large scale infrastructure for data ingestion, and stunning data visualization for custom client applications. Essential Functions: Essential Functions: · Work collaboratively with data scientists, business consultants, and software engineers to create and deploy dynamic data applications that help our customers make meaningful business decisions. · Develop and deploy robust data pipelines and end-to-end systems · Participate in every stage of the engineering lifecycle, from ideation and requirements gathering through implementation, testing, deployment, and maintenance · Provide leadership and coordination for certain stages of the engineering lifecycle as needed · Perform other technical tasks as needed, including writing project reports, managing, implementing, and/or maintaining technical infrastructure, etc. · Ability and the willingness to tailor applications to a client’s business goals using an iterative methodology. · Ability to consider both long-term stability and scalability while taking a user-focused approach to development and deployment. · Communicate clearly both verbally and in writing to teammates and clients · Ability to work independently in a collaborative, dynamic, cross-functional environment · Travel to and work on-site at clients both local and non-local. Number of days at client site vary depending on project requirements Required Skills: Required Skills: · Bachelors or Master’s degree in Computer Science or related field, and 5 years of experience · TS/SCI with Polygraph · Ability to work with high-level mathematical concepts and associated code-form representations · Experience with Python, Java, Scala, Familiarity with R O/S: UNIX, Linux, Solaris, ssh, git. · Experience with Data manipulation, SQL, relational databases, and/or NoSWL databases. · Cloud: AWS, stack configuration and management · Focus areas: data manipulation, big data architecture, data structures, database administration, cloud platforms and SaaS, development operations (devops), data visualization and user experience. Job Type: Full-time Salary: $101,000.00 - $120,000.00 per year Compensation package: Yearly pay Yearly pay Experience level: 5 years 5 years Schedule: Monday to Friday Monday to Friday Ability to commute/relocate: Charlottesville, VA 22903: Reliably commute or planning to relocate before starting work (Required) Charlottesville, VA 22903: Reliably commute or planning to relocate before starting work (Required) Experience: Informatica: 1 year (Preferred) SQL: 1 year (Preferred) Data warehouse: 1 year (Preferred) Informatica: 1 year (Preferred) SQL: 1 year (Preferred) Data warehouse: 1 year (Preferred) Work Location: In person",N/A,,,,,,
"Dallas, TX",Data Engineer,$73K - $101K (Glassdoor est.),The Cervantes Group,"Data Engineer Data Engineer Location: Dallas, TX Location: Dallas, TX The Cervantes Group is a fast-growing startup that provides software and hardware solutions to help people with disabilities learn and use technology. We are looking for a Data Engineer to join our team and help us scale our data collection, analysis, and reporting efforts. The ideal candidate will be a data scientist with strong experience in machine learning, data science, and big data. You should be comfortable working in a fast-paced, startup environment, and have an entrepreneurial mindset. Responsibilities: Responsibilities: Build, test, and deploy machine learning models using Python, R, Scala, or Java. Work with the engineering team to analyze data from multiple sources (e.g., webhooks, API calls, etc.) Work with the sales team to create new features for our customers’ products based on data analysis. Work with the marketing team to write articles about our company’s technology and products. Collaborate with other teams (product marketing, sales, engineering) to build out new features of our products. Help us improve our product by providing feedback on issues related to data collection and analysis. Help us build out new features of our products based on data analysis. Build, test, and deploy machine learning models using Python, R, Scala, or Java. Work with the engineering team to analyze data from multiple sources (e.g., webhooks, API calls, etc.) Work with the sales team to create new features for our customers’ products based on data analysis. Work with the marketing team to write articles about our company’s technology and products. Collaborate with other teams (product marketing, sales, engineering) to build out new features of our products. Help us improve our product by providing feedback on issues related to data collection and analysis. Help us build out new features of our products based on data analysis. Qualifications: Qualifications: Bachelor’s degree in Computer Science or related field (or equivalent work experience). 3+ years of experience building machine learning models in Python or R. 3+ years of experience building pipelines using Scala or Java. Strong knowledge of Python 3 and R 3.2+ experience is preferred. We are interested in adding new languages (R) to the team as well if they have experience in that language but not required. We are interested in hiring candidates who can also work in an Agile environment as we are always looking for ways to improve our processes and culture of collaboration is key for us to succeed as a company. We are interested in hiring candidates who can work independently as well as part-time if possible but not required. Bachelor’s degree in Computer Science or related field (or equivalent work experience). 3+ years of experience building machine learning models in Python or R. 3+ years of experience building pipelines using Scala or Java. Strong knowledge of Python 3 and R 3.2+ experience is preferred. We are interested in adding new languages (R) to the team as well if they have experience in that language but not required. We are interested in hiring candidates who can also work in an Agile environment as we are always looking for ways to improve our processes and culture of collaboration is key for us to succeed as a company. We are interested in hiring candidates who can work independently as well as part-time if possible but not required. Job Type: Full-time Pay: From $1.00 per year Experience level: 2 years 2 years Schedule: 8 hour shift Monday to Friday 8 hour shift Monday to Friday Ability to commute/relocate: Dallas, TX: Reliably commute or planning to relocate before starting work (Required) Dallas, TX: Reliably commute or planning to relocate before starting work (Required) Application Question(s): Do you have experience working with Snowflake? Do you have experience working with IDMC (intelligent data management cloud)? Do you have experience working with Snowflake? Do you have experience working with IDMC (intelligent data management cloud)? Education: Bachelor's (Required) Bachelor's (Required) Experience: ETL: 1 year (Preferred) AWS: 2 years (Preferred) Data warehouse: 1 year (Preferred) ETL: 1 year (Preferred) AWS: 2 years (Preferred) Data warehouse: 1 year (Preferred) Work Location: In person",3.8,51 to 200 Employees,2004,Company - Private,Information Technology Support Services,Information Technology,$5 to $25 million (USD)
Remote,Sr. Data Engineer,$90K - $120K (Employer est.),SEAM Group,"About SEAM Group: SEAM Group is an innovative global leader in safety, reliability, and maintenance services. Our focus and dedication to creating a safer, more reliable world has earned us top recognition as award winners for both Inc 5000 and NorthCoast 99. By emphasizing values such as teamwork, trust, respect, and support, we have built an unparalleled people-first culture. Through leveraging innovative technologies and a commitment to excellence, we combine our employees’ unique talents and individualities to build the best solutions both within our organization and for our customers. About SEAM Group: SEAM Group safety safety reliability reliability maintenance services maintenance services creating a safer, more reliable world creating a safer, more reliable world teamwork teamwork trust trust respect respect support support people-first culture people-first culture Position Summary: At SEAM Group, we are on a mission to build a safer, more reliable world. Our software products coupled with comprehensive data analytics are key to our mission. We are looking to add a Senior Data Engineer to join our pursuit of excellence, providing ownership of SEAM Group’s data analytics and reporting. This includes the creation of detailed analytical models by providing expert opinion related to the integration and processing of data used to drive value and guide business decisions. Position Summary: Essential Functions/ Responsibilities: The demands described here are representative of those that must be met by an employee to successfully perform the essential functions of this job. Reasonable accommodations may be made to enable individuals with disabilities to perform essential functions. Essential Functions/ Responsibilities Lead technical design, implementation, and problem resolution Partner cross-functionally to define and complete well-structured user stories Maintain, evolve and adhere to a maturing data governance policy Maintain and evolve a rapidly growing and robust data model containing vital customer information Design and implement reliable ETL solutions from a variety of data sources (APIs, Proprietary DB, No-SQL ) Provide best in class business analytics in the form of web-based reports and dashboards Define Data Engineering project feature delivery timelines and risks Provide mitigation options to solve for blockers and technical risks as they arise Identify technical debt, and communicate plans to manage it responsibly Provide mentoring as needed to team members Lead technical design, implementation, and problem resolution Partner cross-functionally to define and complete well-structured user stories Maintain, evolve and adhere to a maturing data governance policy Maintain and evolve a rapidly growing and robust data model containing vital customer information Design and implement reliable ETL solutions from a variety of data sources (APIs, Proprietary DB, No-SQL ) Provide best in class business analytics in the form of web-based reports and dashboards Define Data Engineering project feature delivery timelines and risks Provide mitigation options to solve for blockers and technical risks as they arise Identify technical debt, and communicate plans to manage it responsibly Provide mentoring as needed to team members Required Skills/Abilities: The requirements listed below are representative of the knowledge, skills, and/or abilities required to perform each essential duty satisfactorily. Reasonable accommodations may be made to enable individuals with disabilities to perform essential functions. Required Skills/Abilities: Desire and passion to learn from like-minded, experienced, data-obsessed team members A strong sense of ownership, pride and commitment to quality Mastery of Microsoft Power BI Mastery of Business Intelligence data visualization tools Mastery of RDBMS (MSSQL) and/or NoSQL database technologies (MongoDB, Cosmos DB) Experience with cloud-based and/or on premises data warehousing (Azure preferred) Experience with Power BI Embedded Analytics Experience with Agile software development principles A track record of teamwork through high quality and timely Data Engineering projects Desire and passion to learn from like-minded, experienced, data-obsessed team members A strong sense of ownership, pride and commitment to quality Mastery of Microsoft Power BI Mastery of Business Intelligence data visualization tools Mastery of RDBMS (MSSQL) and/or NoSQL database technologies (MongoDB, Cosmos DB) Experience with cloud-based and/or on premises data warehousing (Azure preferred) Experience with Power BI Embedded Analytics Experience with Agile software development principles A track record of teamwork through high quality and timely Data Engineering projects Benefits you will receive: Benefits you will receive: Opportunity to work on an exciting and highly visible project at SEAM Group Participation in a fast-growing company Opportunity to learn new technologies and expand your skillset Flexible time off Flexible work hours Collaborative and embracing culture Comprehensive healthcare plan that includes access to health, dental, vision, disability, and life insurance at group rates 8 Paid Holidays Company 401k matching program Educational Reimbursement up to $1,000/year Opportunity to work on an exciting and highly visible project at SEAM Group Participation in a fast-growing company Opportunity to learn new technologies and expand your skillset Flexible time off Flexible work hours Collaborative and embracing culture Comprehensive healthcare plan that includes access to health, dental, vision, disability, and life insurance at group rates 8 Paid Holidays Company 401k matching program Educational Reimbursement up to $1,000/year Education and Experience: Education and Experience: High school diploma or GED required; bachelor’s degree preferred 5+ years experience in Data Engineering required High school diploma or GED required; bachelor’s degree preferred 5+ years experience in Data Engineering required SEAM Group is an equal-opportunity employer and is dedicated to diversity and inclusion in our workforce. All employment decisions are made based on qualifications, merit, and business need. EEO/M/F/D/V. SEAM Group is an e-verify employer. At this time, SEAM Group does not sponsor work visas. Job Type: Full-time Pay: $90,000.00 - $120,000.00 per year Benefits: 401(k) Dental insurance Flexible schedule Health insurance Paid time off Tuition reimbursement Vision insurance 401(k) Dental insurance Flexible schedule Health insurance Paid time off Tuition reimbursement Vision insurance Compensation package: Bonus pay Performance bonus Yearly pay Bonus pay Performance bonus Yearly pay Experience level: 5 years 5 years Schedule: 8 hour shift 8 hour shift Application Question(s): Do you now or will you ever require sponsorship to work in the United States? Do you now or will you ever require sponsorship to work in the United States? Work Location: Remote",3.9,201 to 500 Employees,2018,Company - Private,Business Consulting,Management & Consulting,$25 to $100 million (USD)
"Berkeley Heights, NJ",Big Data Engineer,$60.00 Per Hour (Employer est.),Comprise IT Solutions,"""Architect, build, and launch new data models that provide intuitive analytics to the team. Lead large-scale data engineering, integration and warehousing projects, build ""Architect, build, and launch new data models that provide intuitive analytics to the team. Lead large-scale data engineering, integration and warehousing projects, build custom integrations between cloud-based systems using APIs and write complex and efficient queries to transform raw data sources into easily accessible models by using the Data integration tool with coding across several languages such as Java, Python, and SQL."" Banking/Finance domain experience, relational dimensional and/or unstructured data modeling experience Big Data applications development experience, NoSql experience, Azure (preferred), Java, Kafka/Spark Job Type: Contract Salary: Up to $60.00 per hour Experience level: 10 years 10 years Schedule: 8 hour shift 8 hour shift Ability to commute/relocate: Berkeley Heights, NJ 07922: Reliably commute or planning to relocate before starting work (Required) Berkeley Heights, NJ 07922: Reliably commute or planning to relocate before starting work (Required) Experience: Azure: 6 years (Preferred) NoSQL: 5 years (Preferred) Java: 5 years (Preferred) Azure: 6 years (Preferred) NoSQL: 5 years (Preferred) Java: 5 years (Preferred) Work Location: In person",N/A,1 to 50 Employees,N/A,Self-employed,N/A,N/A,Unknown / Non-Applicable
Remote,Informatica Data Engineer,$50.00 Per Hour (Employer est.),Abotts consulting,"Description Description (Candidates MUST HAVE: Informatica IICS, Azure, Azure Data Factory, Snowflake Informatica Powercenter, Informatica Powerexchange) (Candidates MUST HAVE: Informatica IICS, Azure, Azure Data Factory, Snowflake Informatica Powercenter, Informatica Powerexchange) Support data pipelines ensuring that ETL jobs run as scheduled, complete successfully and that there are no data quality issues resulting from the jobs runs. Ensure that supported data pipelines meet any predefined SLAs. Address errors/incidents with data pipeline runs to ensure business data delivery is not interrupted. Work with Enterprise Scheduling team to resubmit failed jobs and make schedule adjustments due to routine maintenance as needed. Work with Data Governance and Business Consumers to address any data quality issues resulting from errant runs. Address defects related to support pipelines as needed and assigned. Collaborate with Data Development team as needed for any changes introduced to existing pipelines due to new deployments. Track job performance to identify increasing processing times and implement optimizations as needed. Create RCA’s for large problems and document fixes or updates to SOPs to prevent future occurrences. Document any changes to supported jobs for incident resolution and/or defect fixes. Provides resolution to a diverse range of recognizable complex problems. Analysis is required to identify root cause. Uses judgment within defined boundaries to develop and iterate solutions, both long and short term. Support data pipelines ensuring that ETL jobs run as scheduled, complete successfully and that there are no data quality issues resulting from the jobs runs. Ensure that supported data pipelines meet any predefined SLAs. Address errors/incidents with data pipeline runs to ensure business data delivery is not interrupted. Work with Enterprise Scheduling team to resubmit failed jobs and make schedule adjustments due to routine maintenance as needed. Work with Data Governance and Business Consumers to address any data quality issues resulting from errant runs. Address defects related to support pipelines as needed and assigned. Collaborate with Data Development team as needed for any changes introduced to existing pipelines due to new deployments. Track job performance to identify increasing processing times and implement optimizations as needed. Create RCA’s for large problems and document fixes or updates to SOPs to prevent future occurrences. Document any changes to supported jobs for incident resolution and/or defect fixes. Provides resolution to a diverse range of recognizable complex problems. Analysis is required to identify root cause. Uses judgment within defined boundaries to develop and iterate solutions, both long and short term. Top Skills Details Top Skills Details 1. 6+ Years of experience as a data engineer with the ability to work with large data set, multiple databases and Data Warehouses 2. 6+ years of experience in Informatica Development with extensive experience with Informatica IICS (Intelligent Cloud Integration Services) 3. Strong experience with Informatica PowerCenter and Informatica PowerExchange 4. Strong Experience working in Azure Cloud Environments (including Azure Data Factory) 5. Strong experience working with Snowflake as the Enterprise Data Warehouse 6. Strong experience working with Oracle/SQL Server databases 7. Strong SQL Querying skills with the ability to do simple and complex queries and inner/outer joins Job Type: Contract Pay: From $50.00 per hour Experience level: 1 year 1 year Schedule: 8 hour shift 8 hour shift Experience: Informatica: 1 year (Preferred) SQL: 1 year (Preferred) Data warehouse: 1 year (Preferred) Informatica: 1 year (Preferred) SQL: 1 year (Preferred) Data warehouse: 1 year (Preferred) Work Location: Remote",4.1,51 to 200 Employees,2014,Company - Private,Business Consulting,Management & Consulting,$1 to $5 million (USD)
Remote,Azure Data Engineer,$50.00 Per Hour (Employer est.),Visvak Solutions,"7+ years of relevant experience At least two years of experience building and leading highly complex, technical engineering teams. Strong hands-on experience in Databricks Experience managing distributed teams preferred. Strong technical experience in large distributed systems, Data Warehousing, Data Lake at scale Project management skills: financial/budget management, scheduling and resource management experience with medium and large-scale projects Comfortable working with ambiguity and multiple stakeholders. Comfortable working cross functionality with product management and directly with customers; ability to deeply understand product and customer personas. Architecture Design Experience for Cloud and Non-cloud platforms Expertise on Azure Cloud platform Knowledge on orchestrating workloads on cloud Ability to set and lead the technical vision while balancing business drivers Strong experience with PySpark, Python programming Proficiency with APIs, containerization and orchestration is a plus. 7+ years of relevant experience At least two years of experience building and leading highly complex, technical engineering teams. Strong hands-on experience in Databricks Experience managing distributed teams preferred. Strong technical experience in large distributed systems, Data Warehousing, Data Lake at scale Project management skills: financial/budget management, scheduling and resource management experience with medium and large-scale projects Comfortable working with ambiguity and multiple stakeholders. Comfortable working cross functionality with product management and directly with customers; ability to deeply understand product and customer personas. Architecture Design Experience for Cloud and Non-cloud platforms Expertise on Azure Cloud platform Knowledge on orchestrating workloads on cloud Ability to set and lead the technical vision while balancing business drivers Strong experience with PySpark, Python programming Proficiency with APIs, containerization and orchestration is a plus. Job Type: Contract Salary: $50.00 per hour Experience level: 7 years 7 years Schedule: Monday to Friday Monday to Friday Work Location: Remote",3.8,51 to 200 Employees,N/A,Company - Private,Computer Hardware Development,Information Technology,Less than $1 million (USD)
Remote,Data Engineer III,$117K - $194K (Employer est.),"ShipBob, Inc.","As a member of the ShipBob Team, you will benefit from an environment where everything is achievable. We aim to be a place where you can: Write Your Career Story. Because we are solving some of the most difficult problems in global commerce, you have the opportunity to write the story that will make your career. Experience Global Impact and Global Connection. At ShipBob we benefit from diverse cultures and perspectives in service of the global community. Grow With An Ownership Mindset. We believe that great innovation comes from great transparency. We are more resilient and more creative when we have an inclusive and transparent culture where everyone knows our strengths and opportunities. Title: Data Engineer III Location: Remote in these states: AZ, CA, CO, FL, GA, KS, KY, IA, ID, IL, IN, MA, ME, MI, MN, MO, NC, NH, NJ, NV, NY, OH, OR, PA, RI, SC, SD, TN, TX, VA, VT, WA, WI Role Description: As a Data Engineer III at ShipBob, you will be working within a team handling all facets of the database environment, from assisting engineering teams in table design, to building databases, to working with our data warehouse and ETL. We are looking for someone who enjoys working in a fast paced environment, who can think on his/her feet, and produce results. What you'll do: Focus on growing as an data engineer in an Azure SQL environment Performing SQL programming and performance tuning Capable of taking well-defined sub-tasks and completing these tasks. Creating Power BI visualizations and paginated reports Modeling and implementing databases and warehouses Demonstrates they are a high energy individual. Effective in communicating status to the team. Exhibits ShipBobs's core values, focuses on understanding and living these values. Accepts feedback graciously and learns from everything they do Other duties/responsibilities as necessary. What you'll bring to the table: 6-9 Years of Experience Excellent problem solving skills Excellent SQL skills Excellent communication skills Performance oriented mindset Ability to work quickly and collaboratively in a fast-paced, entrepreneurial environment Experience in the following: SQL Performance Tuning and Optimization Data Modeling Azure SQL Administration, including Security Powershell Azure Data Factory Nice to have: Power BI Ability to own small well scoped projects and implement them Experience in Azure Synapse and Cosmos A passion for databases and an understanding that solutions you implement will affect our entire suite of applications Experience with big data Experience with Azure DevOps Experience with Git Experience with Agile Experience with Azure DevOps Experience with Azure Functions Mongo Atlas experience Spark Classification: Exempt Reports to: Manager, Database Administration Perks & Benefits: Medical, Dental, Vision & Basic Life Insurance Paid Maternity/Parental Leave Program Flexible Time Off Program Paid Sick Leave and Paid Emergency Leave Floating Holidays (2 days/year) Wellness Days (1 day/quarter) 401K Match Competitive Salary, Performance Bonus & Equity Variety of voluntary benefits, such as, short term disability Referral Bonus Program Fun Culture >>> Check us out on Instagram (@lifeatshipbob) ShipBob believes in transparency while providing a competitive total compensation package with a pay for performance approach. The expected base pay range for this position is $116,645 - $194,408. We recognize that people come with a wealth of experience and talent beyond just the technical requirements of a job. If your experience is close to what you see listed here, please still consider applying. Diversity of experience and skills combined with passion is a key to innovation and excellence; therefore, we encourage people from all backgrounds to apply to our positions. About You: At ShipBob, we're looking to bring on board people who embody our core values: Be Mission-Driven. We want team members that are passionate about helping entrepreneurs improve their business, and bring that passion every day. Be Humble. We have ambitious goals, and our team members understand that success or failure depends on us working together and leaving egos at the door. Be Resilient. Logistics is a complicated business. So is software. We value team members that never give up and keep iterating until a problem is solved. Be a Creative Problem Solver. As a startup, we value smart, innovative solutions to complex problems. We fall in love with the problem, not our ""favorite"" solution. Be Safety Minded. It's not just talk; it's the way you work. About Us: ShipBob is a cloud-based logistics platform that partners with over 7,000+ e-commerce businesses to help make their entrepreneurial dreams a reality. We offer a full suite of fulfillment solutions for our merchants, including the ability to improve their transit times, shipping costs and deliver best in class experience to their customers. With an almost 100% accuracy rate in fulfilling orders and orders shipped on time, our merchants can count on us to deliver excellent service. As one of the fastest growing tech companies in Chicago with over $300M+ raised from blue-chip investors like Menlo Ventures, Bain Capital Ventures, Hyde Park Venture Partners and SoftBank Vision Fund 2, our goal is to continue to be the #1 best fulfillment technology in the industry. ShipBob provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws. This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation and training. As a member of the ShipBob Team, you will benefit from an environment where everything is achievable. We aim to be a place where you can: Write Your Career Story. Because we are solving some of the most difficult problems in global commerce, you have the opportunity to write the story that will make your career. Experience Global Impact and Global Connection. At ShipBob we benefit from diverse cultures and perspectives in service of the global community. Grow With An Ownership Mindset. We believe that great innovation comes from great transparency. We are more resilient and more creative when we have an inclusive and transparent culture where everyone knows our strengths and opportunities. As a member of the ShipBob Team, you will benefit from an environment where everything is achievable. We aim to be a place where you can: Write Your Career Story. Because we are solving some of the most difficult problems in global commerce, you have the opportunity to write the story that will make your career. Experience Global Impact and Global Connection. At ShipBob we benefit from diverse cultures and perspectives in service of the global community. Grow With An Ownership Mindset. We believe that great innovation comes from great transparency. We are more resilient and more creative when we have an inclusive and transparent culture where everyone knows our strengths and opportunities. Write Your Career Story. Because we are solving some of the most difficult problems in global commerce, you have the opportunity to write the story that will make your career. Write Your Career Story Experience Global Impact and Global Connection. At ShipBob we benefit from diverse cultures and perspectives in service of the global community. Experience Global Impact and Global Connection. Grow With An Ownership Mindset. We believe that great innovation comes from great transparency. We are more resilient and more creative when we have an inclusive and transparent culture where everyone knows our strengths and opportunities. Grow With An Ownership Mindset. Title: Data Engineer III Title: Location: Remote in these states: AZ, CA, CO, FL, GA, KS, KY, IA, ID, IL, IN, MA, ME, MI, MN, MO, NC, NH, NJ, NV, NY, OH, OR, PA, RI, SC, SD, TN, TX, VA, VT, WA, WI Location: Role Description: Role Description: As a Data Engineer III at ShipBob, you will be working within a team handling all facets of the database environment, from assisting engineering teams in table design, to building databases, to working with our data warehouse and ETL. We are looking for someone who enjoys working in a fast paced environment, who can think on his/her feet, and produce results. What you'll do: What you'll do: Focus on growing as an data engineer in an Azure SQL environment Performing SQL programming and performance tuning Capable of taking well-defined sub-tasks and completing these tasks. Creating Power BI visualizations and paginated reports Modeling and implementing databases and warehouses Demonstrates they are a high energy individual. Effective in communicating status to the team. Exhibits ShipBobs's core values, focuses on understanding and living these values. Accepts feedback graciously and learns from everything they do Other duties/responsibilities as necessary. Focus on growing as an data engineer in an Azure SQL environment Performing SQL programming and performance tuning Capable of taking well-defined sub-tasks and completing these tasks. Creating Power BI visualizations and paginated reports Modeling and implementing databases and warehouses Demonstrates they are a high energy individual. Effective in communicating status to the team. Exhibits ShipBobs's core values, focuses on understanding and living these values. Accepts feedback graciously and learns from everything they do Other duties/responsibilities as necessary. What you'll bring to the table: What you'll bring to the table: 6-9 Years of Experience Excellent problem solving skills Excellent SQL skills Excellent communication skills Performance oriented mindset Ability to work quickly and collaboratively in a fast-paced, entrepreneurial environment Experience in the following: SQL Performance Tuning and Optimization Data Modeling Azure SQL Administration, including Security Powershell Azure Data Factory Nice to have: Power BI Ability to own small well scoped projects and implement them Experience in Azure Synapse and Cosmos A passion for databases and an understanding that solutions you implement will affect our entire suite of applications Experience with big data Experience with Azure DevOps Experience with Git Experience with Agile Experience with Azure DevOps Experience with Azure Functions Mongo Atlas experience Spark 6-9 Years of Experience Excellent problem solving skills Excellent SQL skills Excellent communication skills Performance oriented mindset Ability to work quickly and collaboratively in a fast-paced, entrepreneurial environment Experience in the following: SQL Performance Tuning and Optimization Data Modeling Azure SQL Administration, including Security Powershell Azure Data Factory SQL Performance Tuning and Optimization Data Modeling Azure SQL Administration, including Security Powershell Azure Data Factory Nice to have: Power BI Ability to own small well scoped projects and implement them Experience in Azure Synapse and Cosmos A passion for databases and an understanding that solutions you implement will affect our entire suite of applications Experience with big data Experience with Azure DevOps Experience with Git Experience with Agile Experience with Azure DevOps Experience with Azure Functions Mongo Atlas experience Spark Power BI Ability to own small well scoped projects and implement them Experience in Azure Synapse and Cosmos A passion for databases and an understanding that solutions you implement will affect our entire suite of applications Experience with big data Experience with Azure DevOps Experience with Git Experience with Agile Experience with Azure DevOps Experience with Azure Functions Mongo Atlas experience Spark Classification: Exempt Classification: Reports to: Manager, Database Administration Reports to: Perks & Benefits: Perks & Benefits: Medical, Dental, Vision & Basic Life Insurance Paid Maternity/Parental Leave Program Flexible Time Off Program Paid Sick Leave and Paid Emergency Leave Floating Holidays (2 days/year) Wellness Days (1 day/quarter) 401K Match Competitive Salary, Performance Bonus & Equity Variety of voluntary benefits, such as, short term disability Referral Bonus Program Fun Culture >>> Check us out on Instagram (@lifeatshipbob) Medical, Dental, Vision & Basic Life Insurance Paid Maternity/Parental Leave Program Flexible Time Off Program Paid Sick Leave and Paid Emergency Leave Floating Holidays (2 days/year) Wellness Days (1 day/quarter) 401K Match Competitive Salary, Performance Bonus & Equity Variety of voluntary benefits, such as, short term disability Referral Bonus Program Fun Culture >>> Check us out on Instagram (@lifeatshipbob) ShipBob believes in transparency while providing a competitive total compensation package with a pay for performance approach. The expected base pay range for this position is $116,645 - $194,408. ShipBob believes in transparency while providing a competitive total compensation package with a pay for performance approach. The expected base pay range for this position is $116,645 - $194,408. ShipBob believes in transparency while providing a competitive total compensation package with a pay for performance approach. The expected base pay range for this position is $116,645 - $194,408. We recognize that people come with a wealth of experience and talent beyond just the technical requirements of a job. If your experience is close to what you see listed here, please still consider applying. Diversity of experience and skills combined with passion is a key to innovation and excellence; therefore, we encourage people from all backgrounds to apply to our positions. About You: At ShipBob, we're looking to bring on board people who embody our core values: Be Mission-Driven. We want team members that are passionate about helping entrepreneurs improve their business, and bring that passion every day. Be Humble. We have ambitious goals, and our team members understand that success or failure depends on us working together and leaving egos at the door. Be Resilient. Logistics is a complicated business. So is software. We value team members that never give up and keep iterating until a problem is solved. Be a Creative Problem Solver. As a startup, we value smart, innovative solutions to complex problems. We fall in love with the problem, not our ""favorite"" solution. Be Safety Minded. It's not just talk; it's the way you work. About Us: ShipBob is a cloud-based logistics platform that partners with over 7,000+ e-commerce businesses to help make their entrepreneurial dreams a reality. We offer a full suite of fulfillment solutions for our merchants, including the ability to improve their transit times, shipping costs and deliver best in class experience to their customers. With an almost 100% accuracy rate in fulfilling orders and orders shipped on time, our merchants can count on us to deliver excellent service. As one of the fastest growing tech companies in Chicago with over $300M+ raised from blue-chip investors like Menlo Ventures, Bain Capital Ventures, Hyde Park Venture Partners and SoftBank Vision Fund 2, our goal is to continue to be the #1 best fulfillment technology in the industry. ShipBob provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws. This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation and training. We recognize that people come with a wealth of experience and talent beyond just the technical requirements of a job. If your experience is close to what you see listed here, please still consider applying. Diversity of experience and skills combined with passion is a key to innovation and excellence; therefore, we encourage people from all backgrounds to apply to our positions. About You: About You: At ShipBob, we're looking to bring on board people who embody our core values: Be Mission-Driven. We want team members that are passionate about helping entrepreneurs improve their business, and bring that passion every day. Be Humble. We have ambitious goals, and our team members understand that success or failure depends on us working together and leaving egos at the door. Be Resilient. Logistics is a complicated business. So is software. We value team members that never give up and keep iterating until a problem is solved. Be a Creative Problem Solver. As a startup, we value smart, innovative solutions to complex problems. We fall in love with the problem, not our ""favorite"" solution. Be Safety Minded. It's not just talk; it's the way you work. Be Mission-Driven. We want team members that are passionate about helping entrepreneurs improve their business, and bring that passion every day. Mission-Driven. Be Humble. We have ambitious goals, and our team members understand that success or failure depends on us working together and leaving egos at the door. Humble Be Resilient. Logistics is a complicated business. So is software. We value team members that never give up and keep iterating until a problem is solved. Resilient. Be a Creative Problem Solver. As a startup, we value smart, innovative solutions to complex problems. We fall in love with the problem, not our ""favorite"" solution. Creative Problem Solver. Be Safety Minded. It's not just talk; it's the way you work. Be Safety Minded. About Us: About Us: ShipBob is a cloud-based logistics platform that partners with over 7,000+ e-commerce businesses to help make their entrepreneurial dreams a reality. We offer a full suite of fulfillment solutions for our merchants, including the ability to improve their transit times, shipping costs and deliver best in class experience to their customers. With an almost 100% accuracy rate in fulfilling orders and orders shipped on time, our merchants can count on us to deliver excellent service. As one of the fastest growing tech companies in Chicago with over $300M+ raised from blue-chip investors like Menlo Ventures, Bain Capital Ventures, Hyde Park Venture Partners and SoftBank Vision Fund 2, our goal is to continue to be the #1 best fulfillment technology in the industry. ShipBob provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws. ShipBob provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws. This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation and training. This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation and training.",4.4,1001 to 5000 Employees,2014,Company - Private,Shipping & Trucking,Transportation & Logistics,$100 to $500 million (USD)
"Dallas, TX",Junior Data Engineer,$73K - $107K (Glassdoor est.),HYR Global Source Inc,"Job Title - Junior Data Engineer Location - Dallas TX Hybrid ( No Relocation ) Tax Term (W2, C2C) - W2 / C2C  Position Type(Contract/permanent) - Contract Project Duration - Long term 12+ months    Job description - The candidate needs to be hungry to learn and will be working with the Data team, looking for someone who is willing to learn and is motivated to grow within. Scripting – Python, KSH, Powershell Knowledge about Linux, and Windows Server Operating System AWS basic knowledge Job Title - Junior Data Engineer Location - Dallas TX Hybrid ( No Relocation ) Tax Term (W2, C2C) - W2 / C2C Position Type(Contract/permanent) - Contract Project Duration - Long term 12+ months Job description - The candidate needs to be hungry to learn and will be working with the Data team, looking for someone who is willing to learn and is motivated to grow within. Scripting – Python, KSH, Powershell Knowledge about Linux, and Windows Server Operating System AWS basic knowledge Scripting – Python, KSH, Powershell Knowledge about Linux, and Windows Server Operating System AWS basic knowledge Scripting – Python, KSH, Powershell Knowledge about Linux, and Windows Server Operating System AWS basic knowledge",4.8,51 to 200 Employees,2013,Company - Private,Information Technology Support Services,Information Technology,Unknown / Non-Applicable
"Charlotte, NC","Staff Software Engineer, Data",$110K (Employer est.),NAVEX Global,"It's fun to work in a company where people truly BELIEVE in what they're doing! We're committed to bringing passion and customer focus to the business. We're committed to bringing passion and customer focus to the business. Position Summary: Position Summary: At NAVEX, you will help design and implement our NAVEX One data platform part of our newest engineering team. Our Product Engineering team shares a passion for designing quality solutions, embracing new technologies and delivering powerful products that help our customers protect their reputation and bottom line. As a Data Staff Software Engineer, you will influence technical designs and implement our new data platform. You will focus on quality implementation while guiding the other data engineers. You will help us build a data platform that will ingest other teams’ content and then provide application specific data sets. We are looking for a candidate who is strong in data engineering. In this role, you will have ample opportunity to explore new value-added capabilities, invest in data development and tool research, mentor software developers and grow your career all while balancing your life priorities. We Offer You: We Offer You: An Inspiring Culture. Invested teammates, belonging groups, and a socially determined culture Meaningful Work. Innovative products and solutions with real life impact for people and organizations Career Growth. Stellar training and an unwavering commitment to your growth and success Life Flexibility. Time to care for yourself, your loved ones, and your community Industry Leadership. A highly reputable, fast growing and consistently profitable organization Real Rewards. Competitive and transparent pay practices, wellbeing programs and benefits with choice An Inspiring Culture. Invested teammates, belonging groups, and a socially determined culture An Inspiring Culture. Meaningful Work. Innovative products and solutions with real life impact for people and organizations Meaningful Work. Career Growth. Stellar training and an unwavering commitment to your growth and success Career Growth. Life Flexibility. Time to care for yourself, your loved ones, and your community Life Flexibility. Industry Leadership. A highly reputable, fast growing and consistently profitable organization Industry Leadership. Real Rewards. Competitive and transparent pay practices, wellbeing programs and benefits with choice Real Rewards. What You Will Do: What You Will Do: Work with a team of data engineers and be accountable for designs and high-quality deliveries as an individual contributor Help team members grow by mentoring newer engineers Participate in the innovative advancements of our product platform and collaborate with our awesome agile team members Promote opportunities for refactoring and identify areas of optimization Research and leverage commercial products, libraries, and tools that can be used to solve problems Participate in design sessions with other engineers, architects, and product managers, providing constructive and honest feedback during sprint retrospectives with a team mindset Use automation, including continuous integration, automated deployments, and automated unit and functional testing Work with a team of data engineers and be accountable for designs and high-quality deliveries as an individual contributor Help team members grow by mentoring newer engineers Participate in the innovative advancements of our product platform and collaborate with our awesome agile team members Promote opportunities for refactoring and identify areas of optimization Research and leverage commercial products, libraries, and tools that can be used to solve problems Participate in design sessions with other engineers, architects, and product managers, providing constructive and honest feedback during sprint retrospectives with a team mindset Use automation, including continuous integration, automated deployments, and automated unit and functional testing What You Will Need: What You Will Need: A Bachelor’s degree in Computer Science or be good enough that we won’t notice through equivalent prior work-related experience 5+ years’ experience in an Agile, full-stack software development environment with a focus on big data designs and implementations, ideally with SaaS and/or micro service-based systems Expert knowledge of data management and pipeline systems, practices, and standards Expert analytical and design skills, including the ability to abstract information requirements from real-world processes to understand information flows in computer systems Expertise in the fields of data transformations (ELT, ETL), data quality, data cleansing, and data profiling using dbt Labs’ DBT Expertise in Data Cataloging and Master Data Management concepts Expertise in both SQL and NoSQL implementations; experience with Microsoft SQL Server, Snowflake, and Postgres database platforms Experience with SQL profiling, performance tuning, and data ingestion into Data Warehouses Strong problem solving and critical thinking skills with the ability to identify and influence others on the best solution Ability to work well in a team environment and attitude to focus on team specific goals and objectives Excellent verbal and written communication skills and a commitment to engage and collaborate with people across a variety of levels with diverse backgrounds A Bachelor’s degree in Computer Science or be good enough that we won’t notice through equivalent prior work-related experience 5+ years’ experience in an Agile, full-stack software development environment with a focus on big data designs and implementations, ideally with SaaS and/or micro service-based systems Expert knowledge of data management and pipeline systems, practices, and standards Expert analytical and design skills, including the ability to abstract information requirements from real-world processes to understand information flows in computer systems Expertise in the fields of data transformations (ELT, ETL), data quality, data cleansing, and data profiling using dbt Labs’ DBT Expertise in Data Cataloging and Master Data Management concepts Expertise in both SQL and NoSQL implementations; experience with Microsoft SQL Server, Snowflake, and Postgres database platforms Experience with SQL profiling, performance tuning, and data ingestion into Data Warehouses Strong problem solving and critical thinking skills with the ability to identify and influence others on the best solution Ability to work well in a team environment and attitude to focus on team specific goals and objectives Excellent verbal and written communication skills and a commitment to engage and collaborate with people across a variety of levels with diverse backgrounds We believe each member of our team deserves to see a path forward to achieving their career and financial goals. Each team member is required to have a career plan in place and reviewed with their manager after six months with our team. The minimum starting pay range for this role is $110,000 per annum with 5% MBO. Pay progression is based on performance. Each team member is required to have a career plan in place and reviewed with their manager after six months with our team. The minimum starting pay range for this role is $110,000 per annum with 5% MBO. Pay progression is based on performance. Our pay programs are just one element of our commitment to Be the ONE place you want to thrive in life. Check out NAVEX’s career page to learn about our innovative people programs designed to create one powerful life experience for YOU! NAVEX is an equal opportunity employer, including disability/vets. If you like wild growth and working with happy, enthusiastic over-achievers, you'll enjoy your career with us! If you like wild growth and working with happy, enthusiastic over-achievers, you'll enjoy your career with us! Job Type: Full-time Benefits: 401(k) Dental insurance Health insurance Paid time off Vision insurance 401(k) Dental insurance Health insurance Paid time off Vision insurance Schedule: Day shift Monday to Friday Day shift Monday to Friday Experience: Software Engineering: 5 years (Required) Snowflake: 2 years (Required) Software Engineering: 5 years (Required) Snowflake: 2 years (Required) Work Location: Hybrid remote in Charlotte, NC 28277",3.5,1001 to 5000 Employees,2012,Company - Private,Enterprise Software & Network Solutions,Information Technology,$100 to $500 million (USD)
"Urbandale, IA",Data Engineer,$66K - $89K (Glassdoor est.),RFA Engineering,"RFA Engineering (www.rfamec.com) supports industry-leading clients through the full software development lifecycle to build cutting-edge precision agriculture, machine guidance, vehicle automation and autonomy applications. We are seeking passionate, talented engineers to work on exciting projects using the latest tools and technologies including robotics, computer-vision, machine learning, IoT, cloud computing, and much more. Collaborate with a team of industry experts onsite at our client's world-class engineering center and contribute to developing innovative solutions that drive sustainable agriculture practices. This is a full-time position with a full benefit package listed below that includes opportunities for professional growth, direct hire by our customers, and additional opportunities within our own organization. Data Engineer You will be working with a high-velocity team of multi-disciplined engineers, developers, and architects that are developing big data pipelines to enable next-generation autonomous vehicles. Responsibilities Developing and maintaining data pipelines that ingest and post-process video streams, sensor data logs, and related metadata to be stored and used for ML model training, regression testing, and analysis. Collaborate with data platform developers to perform functional verification and integration validation of data transformation processes. Develop and document common practices for data registration and config generation Participate in code reviews, code optimization, and refactoring Requirements Bachelor's degree or higher in Software Engineering, Computer Engineering, Electrical Engineering, Computer Science, or similar field or industry experience 3+ years of Python coding experience, familiar with utilizing packages such as pandas, boto3, subprocess, argparse, json, csv, os 3+ years Git/GitHub experience and knowledge of branching best practices 2+ years experience with AWS cloud tools such as S3, ECR, Lambda, SQS/SNS, CloudFormation, or IAM role. 2+ years experience with SQL, relational databases, and tools such as MongoDB, AWS Athena, or Databricks Delta Lake 1+ year experience with large-scale data operations and transformation utilizing tools like PySpark, Kubeflow, or Docker containers Desired Attributes Familiarity with CAN J1939 communication protocol Excellent interpersonal communication skills with the ability to lead conversations and collaborate Strong, demonstrated problem-solving and debugging skills Ability to influence others without authority Ability to work in a well-organized manner, provide clear status updates, and deliver results in a timely manner Visa sponsorship is NOT available for this position. About RFA Engineering RFA Engineering has provided product development and engineering services to industry leading customers since 1943. Our primary focus is the development of off highway equipment including agricultural, construction, mining, recreational, industrial, and special machines. Our work includes concept development, product design, documentation, problem-solving, simulation, optimization, and testing of components, systems and complete machines. Our engineering staff is located at our Engineering Center in Minneapolis, branch office in Dubuque, IA, and at numerous customer sites throughout the U.S.  Competitive Benefits Health and Dental Insurance TelaDoc Healthiest You Supplemental Vision Insurance Company Paid Life Insurance Company Paid Long-Term Disability Short-term Disability Retirement Savings Account (Traditional 401k & Roth 401k) Flexible Spending Plan Dependent Care HSA for Medical Expenses Bonus Plan (Exempt Employees Only) Paid Time Off (PTO) Paid Holidays Bereavement Leave Employee Assistance Programs (EAP) Education Assistance Equal Opportunity and Veteran Friendly Education Preferred Bachelors of Science or better in Data Science or related field Bachelors of Science or better in Computer Engineering Bachelors of Science or better in Computer Science or related field Skills Preferred Data Analytics Data Mining Data Analytics Big Data Data Analytics RFA Engineering (www.rfamec.com) supports industry-leading clients through the full software development lifecycle to build cutting-edge precision agriculture, machine guidance, vehicle automation and autonomy applications. We are seeking passionate, talented engineers to work on exciting projects using the latest tools and technologies including robotics, computer-vision, machine learning, IoT, cloud computing, and much more. Collaborate with a team of industry experts onsite at our client's world-class engineering center and contribute to developing innovative solutions that drive sustainable agriculture practices. RFA Engineering ( www.rfamec.com ) This is a full-time position with a full benefit package listed below that includes opportunities for professional growth, direct hire by our customers, and additional opportunities within our own organization. This is a full-time position with a full benefit package full benefit package listed below that includes opportunities for professional growth, direct hire by our customers, and additional opportunities within our own organization. Data Engineer Data Engineer You will be working with a high-velocity team of multi-disciplined engineers, developers, and architects that are developing big data pipelines to enable next-generation autonomous vehicles. Responsibilities Responsibilities Developing and maintaining data pipelines that ingest and post-process video streams, sensor data logs, and related metadata to be stored and used for ML model training, regression testing, and analysis. Collaborate with data platform developers to perform functional verification and integration validation of data transformation processes. Develop and document common practices for data registration and config generation Participate in code reviews, code optimization, and refactoring Developing and maintaining data pipelines that ingest and post-process video streams, sensor data logs, and related metadata to be stored and used for ML model training, regression testing, and analysis. Collaborate with data platform developers to perform functional verification and integration validation of data transformation processes. Develop and document common practices for data registration and config generation Participate in code reviews, code optimization, and refactoring Requirements Requirements Bachelor's degree or higher in Software Engineering, Computer Engineering, Electrical Engineering, Computer Science, or similar field or industry experience 3+ years of Python coding experience, familiar with utilizing packages such as pandas, boto3, subprocess, argparse, json, csv, os 3+ years Git/GitHub experience and knowledge of branching best practices 2+ years experience with AWS cloud tools such as S3, ECR, Lambda, SQS/SNS, CloudFormation, or IAM role. 2+ years experience with SQL, relational databases, and tools such as MongoDB, AWS Athena, or Databricks Delta Lake 1+ year experience with large-scale data operations and transformation utilizing tools like PySpark, Kubeflow, or Docker containers Bachelor's degree or higher in Software Engineering, Computer Engineering, Electrical Engineering, Computer Science, or similar field or industry experience 3+ years of Python coding experience, familiar with utilizing packages such as pandas, boto3, subprocess, argparse, json, csv, os 3+ years Git/GitHub experience and knowledge of branching best practices 2+ years experience with AWS cloud tools such as S3, ECR, Lambda, SQS/SNS, CloudFormation, or IAM role. 2+ years experience with SQL, relational databases, and tools such as MongoDB, AWS Athena, or Databricks Delta Lake 1+ year experience with large-scale data operations and transformation utilizing tools like PySpark, Kubeflow, or Docker containers Desired Attributes Desired Attributes Familiarity with CAN J1939 communication protocol Excellent interpersonal communication skills with the ability to lead conversations and collaborate Strong, demonstrated problem-solving and debugging skills Ability to influence others without authority Ability to work in a well-organized manner, provide clear status updates, and deliver results in a timely manner Familiarity with CAN J1939 communication protocol Excellent interpersonal communication skills with the ability to lead conversations and collaborate Strong, demonstrated problem-solving and debugging skills Ability to influence others without authority Ability to work in a well-organized manner, provide clear status updates, and deliver results in a timely manner Visa sponsorship is NOT available for this position. Visa sponsorship is NOT available for this position. About RFA Engineering About RFA Engineering RFA Engineering has provided product development and engineering services to industry leading customers since 1943. Our primary focus is the development of off highway equipment including agricultural, construction, mining, recreational, industrial, and special machines. Our work includes concept development, product design, documentation, problem-solving, simulation, optimization, and testing of components, systems and complete machines. Our engineering staff is located at our Engineering Center in Minneapolis, branch office in Dubuque, IA, and at numerous customer sites throughout the U.S.  Competitive Benefits Competitive Benefits Health and Dental Insurance TelaDoc Healthiest You Supplemental Vision Insurance Company Paid Life Insurance Company Paid Long-Term Disability Short-term Disability Retirement Savings Account (Traditional 401k & Roth 401k) Flexible Spending Plan Dependent Care HSA for Medical Expenses Bonus Plan (Exempt Employees Only) Paid Time Off (PTO) Paid Holidays Bereavement Leave Employee Assistance Programs (EAP) Education Assistance Health and Dental Insurance TelaDoc Healthiest You Supplemental Vision Insurance Company Paid Life Insurance Company Paid Long-Term Disability Short-term Disability Retirement Savings Account (Traditional 401k & Roth 401k) Flexible Spending Plan Dependent Care HSA for Medical Expenses Bonus Plan (Exempt Employees Only) Paid Time Off (PTO) Paid Holidays Bereavement Leave Employee Assistance Programs (EAP) Education Assistance Equal Opportunity and Veteran Friendly Equal Opportunity and Veteran Friendly Education Education Bachelors of Science or better in Data Science or related field Bachelors of Science or better in Computer Engineering Bachelors of Science or better in Computer Science or related field Bachelors of Science or better in Data Science or related field Bachelors of Science or better in Computer Engineering Bachelors of Science or better in Computer Science or related field Skills Skills Data Analytics Data Mining Data Analytics Big Data Data Analytics Data Analytics Data Mining Data Analytics Big Data Data Analytics",4.0,201 to 500 Employees,1967,Company - Private,Architectural & Engineering Services,"Construction, Repair & Maintenance Services",$5 to $25 million (USD)
Remote,Cloud Data Engineer,$69.61 - $80.00 Per Hour (Employer est.),Cloudrex,"Top skills: Must-Haves (Concepts & Tools): Must-Haves (Concepts & Tools): AWS cloud—KMS, S3, Glue, Lambda etc. Deployed data pipelines Java, Python or PySpark hands on development experience AWS cloud—KMS, S3, Glue, Lambda etc. Deployed data pipelines Java, Python or PySpark hands on development experience Nice-to-Haves (Concepts & Tools): Nice-to-Haves (Concepts & Tools): Prior ETL migration experience from on prem. To cloud Exposure to even driven streaming Prior ETL migration experience from on prem. To cloud Exposure to even driven streaming Job Types: Full-time, Contract, Temporary Pay: $69.61 - $80.00 per hour Experience level: 7 years 8 years 9 years 7 years 8 years 9 years Work Location: Remote",N/A,,,,,,
"Fort Gordon, GA",Data Engineer,$95K - $120K (Employer est.),A3 Missions LLC`,"Required TS/SCI Clearance Excellent written & oral communication, research, and analytic skills Excellent written & oral communication, research, and analytic skills Expert ability to manage personnel, requirements, and coordination of projects Expert ability to manage personnel, requirements, and coordination of projects Expert capabilities to research, create, develop, and deliver professional briefings, multimedia presentations, and written reports Expert capabilities to research, create, develop, and deliver professional briefings, multimedia presentations, and written reports Experience utilizing programming languages such as SAS, R, Java, C, MATLAB, ScaLa, or Python; experience accelerating large data transactions across industry- leading GPU architectures to answer analytic questions Experience utilizing programming languages such as SAS, R, Java, C, MATLAB, ScaLa, or Python; experience accelerating large data transactions across industry- leading GPU architectures to answer analytic questions Experience with assessments, enterprise data integration, governance, and metrics, including the application of metadata management techniques and ability to interrogate databases efficiently using SQL Experience with assessments, enterprise data integration, governance, and metrics, including the application of metadata management techniques and ability to interrogate databases efficiently using SQL Experience with tradecraft and publication; ability to coordinate and support cross- community meetings and working groups; assimilate large volumes of information, and independently produce reports using data science focused libraries such as Pandas, Scikit, TensorFlow and Gensim to answer analytical questions Experience with tradecraft and publication; ability to coordinate and support cross- community meetings and working groups; assimilate large volumes of information, and independently produce reports using data science focused libraries such as Pandas, Scikit, TensorFlow and Gensim to answer analytical questions Desired Experience Knowledge of Army structure and defense level intelligence operations: intelligence collection, fusion, analysis, production, and dissemination for intelligence databases and products Knowledge of Army structure and defense level intelligence operations: intelligence collection, fusion, analysis, production, and dissemination for intelligence databases and products Knowledge and experience with intelligence operations and in assisting with drafting expert assessments across operations priorities on behalf of the stakeholder Knowledge and experience with intelligence operations and in assisting with drafting expert assessments across operations priorities on behalf of the stakeholder Specialized training from any intelligence collection and analysis school or certification to include GEOINT Professional Certification (GPC-F, GPC_IA-II, GPC_GA-II, GPC_IS-II, etc.) Specialized training from any intelligence collection and analysis school or certification to include GEOINT Professional Certification (GPC-F, GPC_IA-II, GPC_GA-II, GPC_IS-II, etc.) Knowledge and understanding of the National System for GEOINT (NSG) and Intelligence Community; knowledge of private sector data science/analytics, machine learning, and data visualization communities Knowledge and understanding of the National System for GEOINT (NSG) and Intelligence Community; knowledge of private sector data science/analytics, machine learning, and data visualization communities Education MA or MS in Data Science, Data Analytics, Informatics, Statistics, or related field AND 5 years CURRENT Intelligence Analysis experience; OR BA or BS in Data Science, Data Analytics, Informatics, Statistics, or related field AND 7 years CURRENT Intelligence Analysis experience; Job Types: Full-time, Contract Pay: $95,000.00 - $120,000.00 per year Benefits: 401(k) 401(k) matching Dental insurance Health insurance Life insurance Paid time off Parental leave Vision insurance 401(k) 401(k) matching Dental insurance Health insurance Life insurance Paid time off Parental leave Vision insurance Schedule: 8 hour shift 8 hour shift Experience: SQL: 1 year (Preferred) Informatica: 1 year (Preferred) Data warehouse: 1 year (Preferred) SQL: 1 year (Preferred) Informatica: 1 year (Preferred) Data warehouse: 1 year (Preferred) Security clearance: Top Secret (Required) Top Secret (Required) Work Location: In person",5.0,1 to 50 Employees,2017,Company - Private,National Agencies,Government & Public Administration,$5 to $25 million (USD)
Remote,Data Engineer,N/A,Clinical Ink,"Company Information Clinical ink is the global life science company that brings data, technology, and patient science together to unlock clinical discovery. Our deep therapeutic-area expertise, coupled with Direct Data Capture, eCOA, eConsent, telehealth, neurocognitive testing, and digital biomarkers advancement, drive the industry standard for data precision and usher in a new generation of clinical trials. With offices in Philadelphia, PA, Winston Salem, NC, and Iowa City, IA, Clinical ink is rewriting the clinical development experience.  Job Description Clinical ink is seeking a Data Engineer to join our Data Team based remotely across the United States! The Data Engineer will work to develop solutions used in applications for clinical trials. The ideal candidate will be a minimum of two years of experience as a software engineer and prior experience working with a variety of tools and frameworks. The Data Engineer's responsibilities include: Develop data engineering solutions used in applications for clinical trial data collection that both make data available for further use and generate value out of data Contribute to the methodology by which advanced analytics projects are delivered to clients and codify the tooling needed to support them Build and support tools that allow data analysts and data scientists to work in complex projects Implement quality, availability, and integrity of code, solutions, and respective systems and follow best practices related to data integrity, security, scalability, etc. Participate in code inspections, reviews, and other activities to ensure quality Qualifications Bachelors in Mathematics, Statistics, Computer Engineering, Computer Science, or related field of study 2-3 years of experience in software engineering, working on multi-discipline teams Experience with a variety of tools and frameworks such as Snowflake, Airflow, Spark, Kafka, RedShift, Sage Maker, Kubernetes, etc., AWS ecosystem (Lambda, Glue, S3, E2C, etc.), programming tools and querying languages (i.e., Python, C++, SQL, Scala, Java, etc.) At least 2+ years of experience with Python Data modelling and database development experience required Data visualization experience preferred in Tableau and/or AWS QuickSight Nice to have experience with issue tracking tools such as JIRA and Confluence Ability to think creatively and take initiative; ability to learn new technical topics and develop new technical skills quickly Willingness to learn and explore bleeding-edge/cutting-edge technologies Additional Information Clinical ink is an equal opportunity employer and does not discriminate against otherwise qualified applicants on the basis of race, color, creed, religion, ancestry, age, sex, marital status, national origin, disability or handicap, or veteran status. www.clinicalink.com Company Information Clinical ink is the global life science company that brings data, technology, and patient science together to unlock clinical discovery. Our deep therapeutic-area expertise, coupled with Direct Data Capture, eCOA, eConsent, telehealth, neurocognitive testing, and digital biomarkers advancement, drive the industry standard for data precision and usher in a new generation of clinical trials. With offices in Philadelphia, PA, Winston Salem, NC, and Iowa City, IA, Clinical ink is rewriting the clinical development experience.  Job Description Clinical ink is seeking a Data Engineer to join our Data Team based remotely across the United States! The Data Engineer will work to develop solutions used in applications for clinical trials. The ideal candidate will be a minimum of two years of experience as a software engineer and prior experience working with a variety of tools and frameworks. The Data Engineer's responsibilities include: Company Information Job Description Develop data engineering solutions used in applications for clinical trial data collection that both make data available for further use and generate value out of data Contribute to the methodology by which advanced analytics projects are delivered to clients and codify the tooling needed to support them Build and support tools that allow data analysts and data scientists to work in complex projects Implement quality, availability, and integrity of code, solutions, and respective systems and follow best practices related to data integrity, security, scalability, etc. Participate in code inspections, reviews, and other activities to ensure quality Develop data engineering solutions used in applications for clinical trial data collection that both make data available for further use and generate value out of data Contribute to the methodology by which advanced analytics projects are delivered to clients and codify the tooling needed to support them Build and support tools that allow data analysts and data scientists to work in complex projects Implement quality, availability, and integrity of code, solutions, and respective systems and follow best practices related to data integrity, security, scalability, etc. Participate in code inspections, reviews, and other activities to ensure quality Qualifications Qualifications Bachelors in Mathematics, Statistics, Computer Engineering, Computer Science, or related field of study 2-3 years of experience in software engineering, working on multi-discipline teams Experience with a variety of tools and frameworks such as Snowflake, Airflow, Spark, Kafka, RedShift, Sage Maker, Kubernetes, etc., AWS ecosystem (Lambda, Glue, S3, E2C, etc.), programming tools and querying languages (i.e., Python, C++, SQL, Scala, Java, etc.) At least 2+ years of experience with Python Data modelling and database development experience required Data visualization experience preferred in Tableau and/or AWS QuickSight Nice to have experience with issue tracking tools such as JIRA and Confluence Ability to think creatively and take initiative; ability to learn new technical topics and develop new technical skills quickly Willingness to learn and explore bleeding-edge/cutting-edge technologies Bachelors in Mathematics, Statistics, Computer Engineering, Computer Science, or related field of study 2-3 years of experience in software engineering, working on multi-discipline teams Experience with a variety of tools and frameworks such as Snowflake, Airflow, Spark, Kafka, RedShift, Sage Maker, Kubernetes, etc., AWS ecosystem (Lambda, Glue, S3, E2C, etc.), programming tools and querying languages (i.e., Python, C++, SQL, Scala, Java, etc.) At least 2+ years of experience with Python Data modelling and database development experience required Data visualization experience preferred in Tableau and/or AWS QuickSight Nice to have experience with issue tracking tools such as JIRA and Confluence Ability to think creatively and take initiative; ability to learn new technical topics and develop new technical skills quickly Willingness to learn and explore bleeding-edge/cutting-edge technologies Additional Information Clinical ink is an equal opportunity employer and does not discriminate against otherwise qualified applicants on the basis of race, color, creed, religion, ancestry, age, sex, marital status, national origin, disability or handicap, or veteran status. www.clinicalink.com Additional Information",4.2,201 to 500 Employees,2007,Company - Private,Enterprise Software & Network Solutions,Information Technology,Unknown / Non-Applicable
Remote,Data Engineer,N/A,LEAFWELL,"Description: Who is Leafwell Leafwell is a rapidly growing technology and data company that set out to increase access, education, and research into cannabis and to advance its application as medicine. An exciting opportunity for a Data Engineer to join our growing team has arisen. What to Expect as a Data Engineer at Leafwell As a Data Engineer at Leafwell, you will be in charge of creating and orchestrating the Leafwell data pipeline, which means gathering data, creating & automating data transformations and producing actionable insights for Leafwell’s internal stakeholders. You will support critical testing and rollout of new data features. The Data Engineer will build and maintain systems that inform Leafwell’s business stakeholders about Key Performance Indicators (KPIs) and suggest data-driven strategies to optimize those metrics. Essential Duties and Responsibilities The Data Engineer will perform the following responsibilities: Acquire, assemble, transform and analyze data Create, manage and orchestrate the data pipeline and it’s infrastructure Present findings, trends, and suggested optimizations Identify new opportunities and threats to the company's business model Update and revise reports, queries, and analytic procedures as necessary Design and implement tracking so that optimization efforts can be measured Identify inefficiencies in data processes and automate where appropriate Write and update international SOPs and internal documentation Support IT systems management with testing, validation, and user support Proactively identify initiatives for data-related improvements Why Leafwell At Leafwell, we are passionate about our work and seek out employees who contribute the same level of dedication and enthusiasm. We are only as good as the people we hire, so we aim to be the best employer in order to attract the top talent in the industry. Do we have your Interest? Requirements: Our Ideal Candidate Our Ideal Candidate will possess the following: Bachelor's degree in Computer Science, Mathematics, Economics, Information Systems, or another quantitative field 3-5 years of relevant professional experience in Data Engineering / Analytics Advance working knowledge and experience in SQL and relational databases Strong data management abilities, including data analysis, standardization, cleansing, querying, and consolidation of data Data visualization experience (e.g Tableau, Looker, etc.) Exercises self-reliance, initiative, and leadership while keeping stakeholders properly informed and involved Reliably manage numerous duties during a workday, necessitating interactions with people located across the world Technically competent, with the ability to quickly learn new processes and programs, and utilize various software applications Excellent communication and interpersonal skills; ability to work with and appeal to a wide variety of personalities and professional tendencies Bonus points if you have experience in the following: Cannabis knowledge; industry experience is a plus Used Data Orchestration tools like Dagster or Airflow Used dbt or comparable data transformation software Git Python or R programming Amazon Web Services PostgreSQL and RedShift CRM products Data Visualization in Metabase Effective project management skills and comfort utilizing a project management platform in collaboration with other team members Data Science projects Benefits Highlights Our benefits include, but are not limited to: Remote first - Most of our employees are 100% remote. Positions requiring travel enjoy a hybrid environment. Rapid Growth - Our company is rapidly expanding, and our employees are advancing with us! Training and Development - We foster a culture of learning. We encourage our employees to take part in educational, training, and development opportunities. Leafwell is dedicated to bringing together people from diverse cultures and perspectives. We work hard to provide a welcoming atmosphere where everyone may flourish, have a sense of belonging, and collaborate effectively. As an equal opportunity employer, we do not tolerate any illegal discrimination against job applicants based on their race, color, religion, veteran status, sex, parental status, gender identity or expression, transgender status, sexual orientation, national origin, age, disability, or genetic information. We are committed to going above and above in creating diversity within our firm and follow the regulations upheld by the EEOC. Who is Leafwell Who is Leafwell Leafwell is a rapidly growing technology and data company that set out to increase access, education, and research into cannabis and to advance its application as medicine. An exciting opportunity for a Data Engineer to join our growing team has arisen. What to Expect as a Data Engineer at Leafwell What to Expect as a Data Engineer at Leafwell As a Data Engineer at Leafwell, you will be in charge of creating and orchestrating the Leafwell data pipeline, which means gathering data, creating & automating data transformations and producing actionable insights for Leafwell’s internal stakeholders. You will support critical testing and rollout of new data features. The Data Engineer will build and maintain systems that inform Leafwell’s business stakeholders about Key Performance Indicators (KPIs) and suggest data-driven strategies to optimize those metrics. Essential Duties and Responsibilities Essential Duties and Responsibilities The Data Engineer will perform the following responsibilities: Acquire, assemble, transform and analyze data Create, manage and orchestrate the data pipeline and it’s infrastructure Present findings, trends, and suggested optimizations Identify new opportunities and threats to the company's business model Update and revise reports, queries, and analytic procedures as necessary Design and implement tracking so that optimization efforts can be measured Identify inefficiencies in data processes and automate where appropriate Write and update international SOPs and internal documentation Support IT systems management with testing, validation, and user support Proactively identify initiatives for data-related improvements Acquire, assemble, transform and analyze data Create, manage and orchestrate the data pipeline and it’s infrastructure Present findings, trends, and suggested optimizations Identify new opportunities and threats to the company's business model Update and revise reports, queries, and analytic procedures as necessary Design and implement tracking so that optimization efforts can be measured Identify inefficiencies in data processes and automate where appropriate Write and update international SOPs and internal documentation Support IT systems management with testing, validation, and user support Proactively identify initiatives for data-related improvements Why Leafwell Why Leafwell At Leafwell, we are passionate about our work and seek out employees who contribute the same level of dedication and enthusiasm. We are only as good as the people we hire, so we aim to be the best employer in order to attract the top talent in the industry. Do we have your Interest? Do we have your Interest? Do we have your Interest? Our Ideal Candidate Our Ideal Candidate Our Ideal Candidate will possess the following: Bachelor's degree in Computer Science, Mathematics, Economics, Information Systems, or another quantitative field 3-5 years of relevant professional experience in Data Engineering / Analytics Advance working knowledge and experience in SQL and relational databases Strong data management abilities, including data analysis, standardization, cleansing, querying, and consolidation of data Data visualization experience (e.g Tableau, Looker, etc.) Exercises self-reliance, initiative, and leadership while keeping stakeholders properly informed and involved Reliably manage numerous duties during a workday, necessitating interactions with people located across the world Technically competent, with the ability to quickly learn new processes and programs, and utilize various software applications Excellent communication and interpersonal skills; ability to work with and appeal to a wide variety of personalities and professional tendencies Bachelor's degree in Computer Science, Mathematics, Economics, Information Systems, or another quantitative field 3-5 years of relevant professional experience in Data Engineering / Analytics Advance working knowledge and experience in SQL and relational databases Strong data management abilities, including data analysis, standardization, cleansing, querying, and consolidation of data Data visualization experience (e.g Tableau, Looker, etc.) Exercises self-reliance, initiative, and leadership while keeping stakeholders properly informed and involved Reliably manage numerous duties during a workday, necessitating interactions with people located across the world Technically competent, with the ability to quickly learn new processes and programs, and utilize various software applications Excellent communication and interpersonal skills; ability to work with and appeal to a wide variety of personalities and professional tendencies Bonus points if you have experience in the following: Cannabis knowledge; industry experience is a plus Used Data Orchestration tools like Dagster or Airflow Used dbt or comparable data transformation software Git Python or R programming Amazon Web Services PostgreSQL and RedShift CRM products Data Visualization in Metabase Effective project management skills and comfort utilizing a project management platform in collaboration with other team members Data Science projects Cannabis knowledge; industry experience is a plus Used Data Orchestration tools like Dagster or Airflow Used dbt or comparable data transformation software Git Python or R programming Amazon Web Services PostgreSQL and RedShift CRM products Data Visualization in Metabase Effective project management skills and comfort utilizing a project management platform in collaboration with other team members Data Science projects Benefits Highlights Benefits Highlights Our benefits include, but are not limited to: Remote first - Most of our employees are 100% remote. Positions requiring travel enjoy a hybrid environment. Rapid Growth - Our company is rapidly expanding, and our employees are advancing with us! Training and Development - We foster a culture of learning. We encourage our employees to take part in educational, training, and development opportunities. Remote first - Most of our employees are 100% remote. Positions requiring travel enjoy a hybrid environment. Rapid Growth - Our company is rapidly expanding, and our employees are advancing with us! Training and Development - We foster a culture of learning. We encourage our employees to take part in educational, training, and development opportunities. Leafwell is dedicated to bringing together people from diverse cultures and perspectives. We work hard to provide a welcoming atmosphere where everyone may flourish, have a sense of belonging, and collaborate effectively. As an equal opportunity employer, we do not tolerate any illegal discrimination against job applicants based on their race, color, religion, veteran status, sex, parental status, gender identity or expression, transgender status, sexual orientation, national origin, age, disability, or genetic information. We are committed to going above and above in creating diversity within our firm and follow the regulations upheld by the EEOC. Leafwell is dedicated to bringing together people from diverse cultures and perspectives. We work hard to provide a welcoming atmosphere where everyone may flourish, have a sense of belonging, and collaborate effectively. As an equal opportunity employer, we do not tolerate any illegal discrimination against job applicants based on their race, color, religion, veteran status, sex, parental status, gender identity or expression, transgender status, sexual orientation, national origin, age, disability, or genetic information. We are committed to going above and above in creating diversity within our firm and follow the regulations upheld by the EEOC.",4.0,51 to 200 Employees,N/A,Company - Public,Information Technology Support Services,Information Technology,Unknown / Non-Applicable
"Irving, TX",GCP Data Engineer,$60.00 Per Hour (Employer est.),High5,"Job Title: GCP Data Engineer - Onsite Location: Irving, Texas ( 75039) Pay Rate: $60/hr / 65/hr  Job Description We are seeking a highly skilled and motivated ETL/ELT GCP Pipeline Engineer to join our team and contribute to the development of a brand-new product/application. As an ETL/ELT Pipeline Engineer, you will be responsible for designing and building efficient data integration pipelines using GCP native tools or open-source technologies such as Python and PySpark. A candidate with expertise in building APIs would be advantageous.  Responsibilities: Designing and developing robust ETL/ELT pipelines to extract, transform, and load data from various sources into our new product/application. Collaborating with cross-functional teams to understand data requirements and translate them into effective pipeline designs. Implementing data quality checks and ensuring the accuracy, completeness, and consistency of data throughout the pipeline process. Optimizing and tuning pipeline performance to ensure efficient data processing and delivery. Working experience in Airflow DAG Working with cloud-based technologies, particularly Google Cloud Platform (GCP), and leveraging its native tools for data integration. Integrating data from diverse sources and formats, including structured, semi-structured, and unstructured data. Identifying and resolving data integration and transformation issues, ensuring the smooth and reliable flow of data. Staying up-to-date with industry trends and emerging technologies in the ETL/ELT space to continuously improve pipeline efficiency and effectiveness.  Requirements:  Strong experience in designing and building ETL/ELT pipelines using GCP native tools or open-source technologies such as Python and PySpark. Familiarity with ETL tools like Ab Initio is a plus, but not a must. Proven ability to work on a brand-new product/application, demonstrating adaptability and problem-solving skills in a dynamic environment. Solid understanding of data integration concepts, data modeling, and database systems. Proficiency in SQL and experience working with various databases and data formats. Excellent analytical and problem-solving skills, with the ability to troubleshoot and resolve complex data-related issues. Strong communication and collaboration skills to effectively work with cross-functional teams and stakeholders. Detail-oriented with a focus on delivering high-quality results within project timelines. Bachelor's or Master's degree in Computer Science, Engineering, or a related field. If you are a motivated and talented ETL/ELT Pipeline Engineer looking to join a dynamic team working on an exciting new product, we would love to hear from you. Apply now and be part of our journey in revolutionizing data integration and analytics. Job Title: GCP Data Engineer - Onsite Job Title: GCP Data Engineer - Onsite Location: Irving, Texas ( 75039) Location: Irving, Texas ( 75039) Pay Rate: $60/hr / 65/hr Pay Rate: $60/hr / 65/hr Job Description Job Description We are seeking a highly skilled and motivated ETL/ELT GCP Pipeline Engineer to join our team and contribute to the development of a brand-new product/application. As an ETL/ELT Pipeline Engineer, you will be responsible for designing and building efficient data integration pipelines using GCP native tools or open-source technologies such as Python and PySpark. A candidate with expertise in building APIs would be advantageous. Responsibilities: Responsibilities: Designing and developing robust ETL/ELT pipelines to extract, transform, and load data from various sources into our new product/application. Collaborating with cross-functional teams to understand data requirements and translate them into effective pipeline designs. Implementing data quality checks and ensuring the accuracy, completeness, and consistency of data throughout the pipeline process. Optimizing and tuning pipeline performance to ensure efficient data processing and delivery. Working experience in Airflow DAG Working with cloud-based technologies, particularly Google Cloud Platform (GCP), and leveraging its native tools for data integration. Integrating data from diverse sources and formats, including structured, semi-structured, and unstructured data. Identifying and resolving data integration and transformation issues, ensuring the smooth and reliable flow of data. Staying up-to-date with industry trends and emerging technologies in the ETL/ELT space to continuously improve pipeline efficiency and effectiveness. Designing and developing robust ETL/ELT pipelines to extract, transform, and load data from various sources into our new product/application. Collaborating with cross-functional teams to understand data requirements and translate them into effective pipeline designs. Implementing data quality checks and ensuring the accuracy, completeness, and consistency of data throughout the pipeline process. Optimizing and tuning pipeline performance to ensure efficient data processing and delivery. Working experience in Airflow DAG Working with cloud-based technologies, particularly Google Cloud Platform (GCP), and leveraging its native tools for data integration. Integrating data from diverse sources and formats, including structured, semi-structured, and unstructured data. Identifying and resolving data integration and transformation issues, ensuring the smooth and reliable flow of data. Staying up-to-date with industry trends and emerging technologies in the ETL/ELT space to continuously improve pipeline efficiency and effectiveness. Requirements: Strong experience in designing and building ETL/ELT pipelines using GCP native tools or open-source technologies such as Python and PySpark. Familiarity with ETL tools like Ab Initio is a plus, but not a must. Proven ability to work on a brand-new product/application, demonstrating adaptability and problem-solving skills in a dynamic environment. Solid understanding of data integration concepts, data modeling, and database systems. Proficiency in SQL and experience working with various databases and data formats. Excellent analytical and problem-solving skills, with the ability to troubleshoot and resolve complex data-related issues. Strong communication and collaboration skills to effectively work with cross-functional teams and stakeholders. Detail-oriented with a focus on delivering high-quality results within project timelines. Bachelor's or Master's degree in Computer Science, Engineering, or a related field. If you are a motivated and talented ETL/ELT Pipeline Engineer looking to join a dynamic team working on an exciting new product, we would love to hear from you. Apply now and be part of our journey in revolutionizing data integration and analytics. Strong experience in designing and building ETL/ELT pipelines using GCP native tools or open-source technologies such as Python and PySpark. Familiarity with ETL tools like Ab Initio is a plus, but not a must. Proven ability to work on a brand-new product/application, demonstrating adaptability and problem-solving skills in a dynamic environment. Solid understanding of data integration concepts, data modeling, and database systems. Proficiency in SQL and experience working with various databases and data formats. Excellent analytical and problem-solving skills, with the ability to troubleshoot and resolve complex data-related issues. Strong communication and collaboration skills to effectively work with cross-functional teams and stakeholders. Detail-oriented with a focus on delivering high-quality results within project timelines. Bachelor's or Master's degree in Computer Science, Engineering, or a related field. If you are a motivated and talented ETL/ELT Pipeline Engineer looking to join a dynamic team working on an exciting new product, we would love to hear from you. Apply now and be part of our journey in revolutionizing data integration and analytics.",3.7,Unknown,N/A,Company - Private,N/A,N/A,Unknown / Non-Applicable
Remote,Sr. Data Engineer,N/A,Teamware Solutions (quantum leap consulting).,"Job Duties: # Experience in SQL Programming language Cosmos Scope scripting. # Knowledge of Big Data pipelines Data Engineering. # Working experience on Azure DevOps is a must # Working Knowledge on MSBI stack on Azure. # Working Knowledge on Azure Data factory, Azure Data Lake and Azure Data lake storage. # Hands-on in Visualization like PowerBI. # Implement end-end data pipelines using cosmos Azure Data factory. # Should have good analytical thinking and Problem solving. # Experience with Data quality implementations assessing data correctness, completeness, uniqueness etc # Experience working on PII, GDPR, handling sensitive data (encryption/decryption) # Good communication and co-ordination skills. # Able to work as Individual contributor. # Requirement Analysis. # Create Maintain and Enhance Big Data Pipeline. # Daily status reporting, interacting with Leads. # Version control ADO GIT, CI CD. # Marketing Campaign experiences. # Data Platform, Product telemetry, Analytical thinking. # Data Validation of the new streams. # Data quality check of the new streams. # Monitoring of data pipeline created in Azure Data factory. # updating the Tech spec and wiki page for each implementation of pipeline. # Updating ADO on daily basis. Job Type: Full-time Schedule: 8 hour shift 8 hour shift Work Location: Remote",4.5,1001 to 5000 Employees,2003,Company - Private,Information Technology Support Services,Information Technology,Unknown / Non-Applicable
Remote,Data Engineer,$100K - $140K (Employer est.),Oddball,"Oddball believes that the best products are built when companies understand and value the things they are working on. We value learning and growth and the ability to make a big impact at a small company. We believe that we can make big changes happen and improve the daily lives of millions of people by bringing quality software to the federal space. We are hiring a Data Engineer to work on a pivotal Federal program that is making a positive impact on millions of Americans' daily lives. What you'll be doing: What you'll be doing: Migrating custom-built Python ETL and ELT pipelines into a modern orchestration framework with Argo Workflows and dbt. Improving observability, discoverability, governance, and implementing a common data integrity and data quality testing framework using tools such as dbt, OpenMetadata and Great Expectations. Constructing reliable and performant high-volume ETL or ELT pipelines for sensitive healthcare data. Contributing to and maintaining legacy ETL and ELT data pipelines. Proactively monitoring data pipelines for potential problems and debugging issues if they arise. Working with third party data owners to figure out ingestion strategies and cadence. Helping to model data at various stages of refinement, curation, and enrichment to best suit different downstream targets and marts. Migrating custom-built Python ETL and ELT pipelines into a modern orchestration framework with Argo Workflows and dbt. Improving observability, discoverability, governance, and implementing a common data integrity and data quality testing framework using tools such as dbt, OpenMetadata and Great Expectations. Constructing reliable and performant high-volume ETL or ELT pipelines for sensitive healthcare data. Contributing to and maintaining legacy ETL and ELT data pipelines. Proactively monitoring data pipelines for potential problems and debugging issues if they arise. Working with third party data owners to figure out ingestion strategies and cadence. Helping to model data at various stages of refinement, curation, and enrichment to best suit different downstream targets and marts. What you’ll bring: What you’ll bring: 5+ years of proven data and performance engineering experience. Expert in SQL. Experience with Python and other programming languages such as JavaScript, TypeScript, Rust, and Go. Orchestration experience with tools like Argo Workflows, AirFlow, Dagster, Prefect, or Luigi. Experience using databases like RedShift, Postgres, and Snowflake. A proven track record of developing custom-built data/analytics solutions and experience in supporting large scale programs. Experience with system analysis, data analysis or programming, using a variety of computer languages and procedures. Experience with Docker, Kubernetes, and AWS (services like EKS, Kinesis, Athena, EKS, S3, RDS, Aurora, Lambda, ECS, EventBridge, SQS, etc) Experience working in Agile environments. 5+ years of proven data and performance engineering experience. Expert in SQL. Experience with Python and other programming languages such as JavaScript, TypeScript, Rust, and Go. Orchestration experience with tools like Argo Workflows, AirFlow, Dagster, Prefect, or Luigi. Experience using databases like RedShift, Postgres, and Snowflake. A proven track record of developing custom-built data/analytics solutions and experience in supporting large scale programs. Experience with system analysis, data analysis or programming, using a variety of computer languages and procedures. Experience with Docker, Kubernetes, and AWS (services like EKS, Kinesis, Athena, EKS, S3, RDS, Aurora, Lambda, ECS, EventBridge, SQS, etc) Experience working in Agile environments. Requirements: Requirements: Must be a US Citizen and able to work domestically Must be able to attain low-level security clearance Must be a US Citizen and able to work domestically Must be able to attain low-level security clearance Education: Education: Bachelor’s Degree Bachelor’s Degree Benefits: Benefits: Fully remote Tech & Education Stipend Comprehensive Benefits Package Company Match 401(k) plan Flexible PTO, Paid Holidays Fully remote Tech & Education Stipend Comprehensive Benefits Package Company Match 401(k) plan Flexible PTO, Paid Holidays Oddball is an Equal Opportunity Employer and does not discriminate against applicants based on race, religion, color, disability, medical condition, legally protected genetic information, national origin, gender, sexual orientation, marital status, gender identity or expression, sex (including pregnancy, childbirth or related medical conditions), age, veteran status or other legally protected characteristics. Any applicant with a mental or physical disability who requires an accommodation during the application process should contact an Oddball HR representative to request such an accommodation by emailing hello@Oddball.io Job Type: Full-time Pay: $100,000.00 - $140,000.00 per year Benefits: 401(k) 401(k) matching Dental insurance Flexible schedule Flexible spending account Health insurance Health savings account Paid time off Parental leave Professional development assistance Referral program Retirement plan Vision insurance 401(k) 401(k) matching Dental insurance Flexible schedule Flexible spending account Health insurance Health savings account Paid time off Parental leave Professional development assistance Referral program Retirement plan Vision insurance Compensation package: Bonus pay Bonus pay Experience level: 5 years 5 years Schedule: Monday to Friday Monday to Friday Education: Bachelor's (Preferred) Bachelor's (Preferred) Work Location: Remote",4.6,1 to 50 Employees,N/A,Company - Private,N/A,N/A,Unknown / Non-Applicable
Remote,Data Engineer,$120K - $145K (Employer est.),Pumpkin,"Hi from Pumpkin! Pumpkin is a pet care company on a mission to help make ‘the best pet care possible fur all’. We want to revolutionize pet healthcare by making it easier for families to provide their pets with the wellness and medical care they need throughout their lives. Launched by Zoetis (ZTS), a Fortune 500 company that’s the world’s largest animal health company, Pumpkin is an early-stage startup with big dreams! As a pack, we share agility, guts, collaboration, and a relentless pursuit in creating a healthier, happier world for pets and their people.  Pumpkin is looking for an experienced Data Engineer to join our growing data and analytics team. In this role, you’ll enhance data accessibility, establishing a dependable platform that enables decentralized teams to discover insights and create impactful product features. As a part of our data team, you will significantly influence the evolution of our data platform. Your input will directly affect Pumpkin's capacity to reveal insights and enrich our knowledge of pet owners and their pets. If you're passionate about data, enjoy problem-solving, and are excited to improve pets' lives, we would love to hear from you!  This role is remote for those outside of a commutable distance of the New York City office. Some travel will be required (quarterly to NYC office). Individuals located in the NYC area will be considered hybrid, with a requirement of 2 days a week in the office.  #LI-Remote What You'll Do: Collaborate with the team to create reliable pipelines that efficiently gather, transform, and load data into our data platform, ensuring seamless information flow between systems. Utilize your skills in SQL, Python, and tools like dbt to structure data for accurate insights that meet business needs. Maintain clear documentation for pipelines and databases, fostering an informed and efficient team. Implement checks to identify and correct errors, ensuring the integrity of data for analysis and modeling. Drive results through effective communication with fellow colleagues and stakeholders, working closely to exchange ideas. Create innovative data practices, showing curiosity for emerging tools and identify enhancements to our data engineering processes. What We're Looking For: 4+ years of hands-on experience in designing and implementing diverse data pipelines and transformations, including data integrations, ETL/ELT pipelines, streaming analytics, and/or big data analytics. Proficiency in SQL and Python for data querying and manipulation. Familiarity with ETL tools like Fivetran or similar, and experience with cloud platforms (e.g., AWS, Aurora, Postgres) and relevant data services (e.g. Airflow, etc.) Familiarity with version control systems (e.g., Git) and CI/CD pipelines. Strong understanding of database concepts, with practical experience in both relational and NoSQL databases. Demonstrated ability to learn and adapt to new tools and technologies effectively. Skilled problem solver with a track record of resolving complex data-related issues. Excellent collaboration skills, capable of effectively engaging with both technical and non-technical stakeholders to gather requirements and deliver valuable insights. Bonus Points: Previous experience creating data assets for reporting and visualization tools like Tableau, Looker, etc. Familiarity with customer-level data sources like Segment/mParticle, Stripe, Salesforce, etc. Experience working in an agile startup environment, with hybrid/remote teams across a variety of functions (marketing, sales, operations, engineering, etc.) Benefits and Perks: Comprehensive contributions to medical, dental and vision for colleagues and dependents. Generous PTO and Paid Holidays 401k with company match Pumpkin Insurance and preventative care for every pet in your family The opportunity to join a team where every team member has the autonomy and support they need to boss their role and make empowered decisions Pet Friendly Offices, Stocked Kitchens, Team Workouts, Team Outings and much more! Pumpkin's Core Values: Pets Come First: Put what’s best for pets at the center of everything we do Trust the Pack: Help families make empowered pet care decisions Jump Fences: If there’s a will, there’s always a way over them Share Toys: Be generous with our gifts and amazing things will happen Dig New Holes: Challenge conventions to create the future of pet health care  The compensation for this position ranges from $120,000- $145,000 (annually). Compensation may vary outside the listed range, based on a number of factors including but not limited to location, qualifications, performance, skills and experience. The compensation range listed is just one component of Pumpkin’s total compensation package for employees. Total compensation packages, depending on the position, may also include incentive compensation, discretionary bonuses, and other short or long-term incentives. If hired, employees will be in an ‘at-will position’ and Pumpkin reserves the right to modify compensation (as well as any other discretionary payment or benefit program) at any time, including for reasons related to individual performance, company or individual department/team performance, and market factors.  Newly hired staff must be fully vaccinated by their start date. Applicants unable to comply with this policy due to an underlying medical condition or sincerely held religious belief may be eligible for an accommodation, unless such an accommodation would be unduly burdensome or present a direct threat to the applicant, our employees, or the members of our greater community.  Pumpkin is proud to be an affirmative action and equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, age, protected veteran status, or any other characteristic protected by federal, state or local laws. Hi from Pumpkin! Hi from Pumpkin! Pumpkin is a pet care company on a mission to help make ‘the best pet care possible fur all’. We want to revolutionize pet healthcare by making it easier for families to provide their pets with the wellness and medical care they need throughout their lives. Launched by Zoetis (ZTS), a Fortune 500 company that’s the world’s largest animal health company, Pumpkin is an early-stage startup with big dreams! As a pack, we share agility, guts, collaboration, and a relentless pursuit in creating a healthier, happier world for pets and their people. Pumpkin is looking for an experienced Data Engineer to join our growing data and analytics team. In this role, you’ll enhance data accessibility, establishing a dependable platform that enables decentralized teams to discover insights and create impactful product features. As a part of our data team, you will significantly influence the evolution of our data platform. Your input will directly affect Pumpkin's capacity to reveal insights and enrich our knowledge of pet owners and their pets. If you're passionate about data, enjoy problem-solving, and are excited to improve pets' lives, we would love to hear from you! This role is remote for those outside of a commutable distance of the New York City office. Some travel will be required (quarterly to NYC office). Individuals located in the NYC area will be considered hybrid, with a requirement of 2 days a week in the office. This role is remote for those outside of a commutable distance of the New York City office. Some travel will be required (quarterly to NYC office). Individuals located in the NYC area will be considered hybrid, with a requirement of 2 days a week in the office. #LI-Remote What You'll Do: What You'll Do: Collaborate with the team to create reliable pipelines that efficiently gather, transform, and load data into our data platform, ensuring seamless information flow between systems. Utilize your skills in SQL, Python, and tools like dbt to structure data for accurate insights that meet business needs. Maintain clear documentation for pipelines and databases, fostering an informed and efficient team. Implement checks to identify and correct errors, ensuring the integrity of data for analysis and modeling. Drive results through effective communication with fellow colleagues and stakeholders, working closely to exchange ideas. Create innovative data practices, showing curiosity for emerging tools and identify enhancements to our data engineering processes. Collaborate with the team to create reliable pipelines that efficiently gather, transform, and load data into our data platform, ensuring seamless information flow between systems. Utilize your skills in SQL, Python, and tools like dbt to structure data for accurate insights that meet business needs. Maintain clear documentation for pipelines and databases, fostering an informed and efficient team. Implement checks to identify and correct errors, ensuring the integrity of data for analysis and modeling. Drive results through effective communication with fellow colleagues and stakeholders, working closely to exchange ideas. Create innovative data practices, showing curiosity for emerging tools and identify enhancements to our data engineering processes. What We're Looking For: What We're Looking For: 4+ years of hands-on experience in designing and implementing diverse data pipelines and transformations, including data integrations, ETL/ELT pipelines, streaming analytics, and/or big data analytics. Proficiency in SQL and Python for data querying and manipulation. Familiarity with ETL tools like Fivetran or similar, and experience with cloud platforms (e.g., AWS, Aurora, Postgres) and relevant data services (e.g. Airflow, etc.) Familiarity with version control systems (e.g., Git) and CI/CD pipelines. Strong understanding of database concepts, with practical experience in both relational and NoSQL databases. Demonstrated ability to learn and adapt to new tools and technologies effectively. Skilled problem solver with a track record of resolving complex data-related issues. Excellent collaboration skills, capable of effectively engaging with both technical and non-technical stakeholders to gather requirements and deliver valuable insights. 4+ years of hands-on experience in designing and implementing diverse data pipelines and transformations, including data integrations, ETL/ELT pipelines, streaming analytics, and/or big data analytics. Proficiency in SQL and Python for data querying and manipulation. Familiarity with ETL tools like Fivetran or similar, and experience with cloud platforms (e.g., AWS, Aurora, Postgres) and relevant data services (e.g. Airflow, etc.) Familiarity with version control systems (e.g., Git) and CI/CD pipelines. Strong understanding of database concepts, with practical experience in both relational and NoSQL databases. Demonstrated ability to learn and adapt to new tools and technologies effectively. Skilled problem solver with a track record of resolving complex data-related issues. Excellent collaboration skills, capable of effectively engaging with both technical and non-technical stakeholders to gather requirements and deliver valuable insights. Bonus Points: Bonus Points: Previous experience creating data assets for reporting and visualization tools like Tableau, Looker, etc. Familiarity with customer-level data sources like Segment/mParticle, Stripe, Salesforce, etc. Experience working in an agile startup environment, with hybrid/remote teams across a variety of functions (marketing, sales, operations, engineering, etc.) Previous experience creating data assets for reporting and visualization tools like Tableau, Looker, etc. Familiarity with customer-level data sources like Segment/mParticle, Stripe, Salesforce, etc. Experience working in an agile startup environment, with hybrid/remote teams across a variety of functions (marketing, sales, operations, engineering, etc.) Benefits and Perks: Benefits and Perks: Comprehensive contributions to medical, dental and vision for colleagues and dependents. Generous PTO and Paid Holidays 401k with company match Pumpkin Insurance and preventative care for every pet in your family The opportunity to join a team where every team member has the autonomy and support they need to boss their role and make empowered decisions Pet Friendly Offices, Stocked Kitchens, Team Workouts, Team Outings and much more! Comprehensive contributions to medical, dental and vision for colleagues and dependents. Generous PTO and Paid Holidays 401k with company match Pumpkin Insurance and preventative care for every pet in your family The opportunity to join a team where every team member has the autonomy and support they need to boss their role and make empowered decisions Pet Friendly Offices, Stocked Kitchens, Team Workouts, Team Outings and much more! Pumpkin's Core Values: Pumpkin's Core Values: Pets Come First: Put what’s best for pets at the center of everything we do Pets Come First: Trust the Pack: Help families make empowered pet care decisions Trust the Pack: Jump Fences: If there’s a will, there’s always a way over them Jump Fences: Share Toys: Be generous with our gifts and amazing things will happen Share Toys: Dig New Holes: Challenge conventions to create the future of pet health care Dig New Holes: The compensation for this position ranges from $120,000- $145,000 (annually). Compensation may vary outside the listed range, based on a number of factors including but not limited to location, qualifications, performance, skills and experience. The compensation range listed is just one component of Pumpkin’s total compensation package for employees. Total compensation packages, depending on the position, may also include incentive compensation, discretionary bonuses, and other short or long-term incentives. If hired, employees will be in an ‘at-will position’ and Pumpkin reserves the right to modify compensation (as well as any other discretionary payment or benefit program) at any time, including for reasons related to individual performance, company or individual department/team performance, and market factors. Newly hired staff must be fully vaccinated by their start date. Applicants unable to comply with this policy due to an underlying medical condition or sincerely held religious belief may be eligible for an accommodation, unless such an accommodation would be unduly burdensome or present a direct threat to the applicant, our employees, or the members of our greater community. Newly hired staff must be fully vaccinated by their start date. Applicants unable to comply with this policy due to an underlying medical condition or sincerely held religious belief may be eligible for an accommodation, unless such an accommodation would be unduly burdensome or present a direct threat to the applicant, our employees, or the members of our greater community. Newly hired staff must be fully vaccinated by their start date. Applicants unable to comply with this policy due to an underlying medical condition or sincerely held religious belief may be eligible for an accommodation, unless such an accommodation would be unduly burdensome or present a direct threat to the applicant, our employees, or the members of our greater community. Pumpkin is proud to be an affirmative action and equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, age, protected veteran status, or any other characteristic protected by federal, state or local laws.",4.7,51 to 200 Employees,2014,Company - Private,Banking & Lending,Financial Services,Unknown / Non-Applicable
Remote,Data Engineer,N/A,Komodo Health,"We Breathe Life Into Data At Komodo Health, our mission is to reduce the global burden of disease. And we believe that smarter use of data is essential to this mission. That's why we built the Healthcare Map — the industry's largest, most complete, precise view of the U.S. healthcare system — by combining de-identified, real-world patient data with innovative algorithms and decades of clinical experience. The Healthcare Map serves as our foundation for a powerful suite of software applications, helping us answer healthcare's most complex questions for our partners. Across the healthcare ecosystem, we're helping our clients unlock critical insights to track detailed patient behaviors and treatment patterns, identify gaps in care, address unmet patient needs, and reduce the global burden of disease. As we pursue these goals, it remains essential to us that we stay grounded in our values: be awesome, seek growth, deliver ""wow,"" and enjoy the ride. At Komodo, you will be joining a team of ambitious, supportive Dragons with diverse backgrounds but a shared passion to deliver on our mission to reduce the burden of disease — and enjoy the journey along the way. The Opportunity at Komodo Health Komodo Health leverages the latest data engineering technology such as Spark, Airflow, and Snowflake to tackle some of healthcare's biggest challenges by transforming extraordinary amounts of data into rich and meaningful insights. As a Data Engineer on the Platform team, you will help lead the development of Komodo Health's platform-enabled workflow tools. The Komodo platform powers all of Komodo's current and future workflow analytical applications and enables 3rd party builders to integrate with, extend, customize, or build on the platform. Reporting directly to the Engineering Manager, you will be solving complex data challenges while designing and implementing data processing and transformation at a scale that powers state-of-the-art interactive product experiences. You will enable smarter, more innovative uses of healthcare data by building robust data pipelines and implementing data best practices. Looking back on your first 12 months at Komodo Health, you will have… Worked on foundational pieces of our data platform architecture, pipelines, analytics, and services underlying our platform Partnered with Engineering team members, Product Managers, Data Scientists, and customer-facing teams to understand and deliver python packages as well as web-based python services Designed and developed reliable data pipelines that transform data at scale, orchestrated jobs via Airflow, using SQL and Python in Snowflake and/or Spark Helped implement continuous improvements to our data governance practices and implemented data quality improvements Implemented technical enhancements to our CI/CD processes and/or built tooling to ensure data consistency and quality Gained an understanding of the broader Komodo Health data landscape and being part of architectural decisions for the Healthcare Analytics and Platform as a Service offerings What you bring to Komodo: Expertise in writing enterprise-level code and contributing to large data pipelining and API processing with Python Experience with SQL and query design on large, complex datasets Ability to use a variety of relational, NoSQL, Postgres, and/or MPP databases (ideally Snowflake on AWS) and leading data modeling, schema design, and data storage best practices Demonstrated proficiency in designing and developing with distributed data processing platforms like Spark and pipeline orchestration tools like Airflow A thirst for knowledge, willingness to learn, and a growth-oriented mindset Committed to fostering an inclusive environment where your teammates feel motivated to succeed Experience enhancing CI/CD build tooling in a containerized environment, from deployment pipelines (Jenkins, etc), infrastructure as code (Terraform, Cloudformation), and configuration management via Docker and Kubernetes Excellent cross-team communication and collaboration skills Compensation at Komodo Health We are committed to providing competitive compensation for all roles at Komodo Health. We carefully consider multiple factors when determining compensation, including your skills, experience, and location while balancing internal equity relative to peers at the company. The targeted base salary range for the Data Engineer role is $123,900 to $186,300 plus a competitive bonus and equity package. #LI-Remote #LI-JK1 Where You'll Work Komodo Health has a hybrid work model; we recognize the power of choice and importance of flexibility for the well-being of both our company and our individual Dragons. Roles may be completely remote based anywhere in the country listed, remote but based in a specific region, or local (commuting distance) to one of our hubs in San Francisco, New York City, or Chicago with remote work options. What We Offer On top of our commitment to providing competitive, fair pay for all roles at Komodo Health, we're proud to offer robust and inclusive benefits to all Dragons at Komodo Health. We offer global time off programs, extensive internal and external career development and learning opportunities, multiple affinity groups celebrating our team's diversity, and an annual wellness and productivity stipend to support you in being your healthiest, best self. Equal Opportunity Statement Komodo Health provides equal employment opportunities to all applicants and employees. We prohibit discrimination and harassment of any type with regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state, or local laws. We Breathe Life Into Data At Komodo Health, our mission is to reduce the global burden of disease. And we believe that smarter use of data is essential to this mission. That's why we built the Healthcare Map — the industry's largest, most complete, precise view of the U.S. healthcare system — by combining de-identified, real-world patient data with innovative algorithms and decades of clinical experience. The Healthcare Map serves as our foundation for a powerful suite of software applications, helping us answer healthcare's most complex questions for our partners. Across the healthcare ecosystem, we're helping our clients unlock critical insights to track detailed patient behaviors and treatment patterns, identify gaps in care, address unmet patient needs, and reduce the global burden of disease. As we pursue these goals, it remains essential to us that we stay grounded in our values: be awesome, seek growth, deliver ""wow,"" and enjoy the ride. At Komodo, you will be joining a team of ambitious, supportive Dragons with diverse backgrounds but a shared passion to deliver on our mission to reduce the burden of disease — and enjoy the journey along the way. We Breathe Life Into Data We Breathe Life Into Data At Komodo Health, our mission is to reduce the global burden of disease. And we believe that smarter use of data is essential to this mission. That's why we built the Healthcare Map — the industry's largest, most complete, precise view of the U.S. healthcare system — by combining de-identified, real-world patient data with innovative algorithms and decades of clinical experience. The Healthcare Map serves as our foundation for a powerful suite of software applications, helping us answer healthcare's most complex questions for our partners. Across the healthcare ecosystem, we're helping our clients unlock critical insights to track detailed patient behaviors and treatment patterns, identify gaps in care, address unmet patient needs, and reduce the global burden of disease. As we pursue these goals, it remains essential to us that we stay grounded in our values: be awesome, seek growth, deliver ""wow,"" and enjoy the ride. At Komodo, you will be joining a team of ambitious, supportive Dragons with diverse backgrounds but a shared passion to deliver on our mission to reduce the burden of disease — and enjoy the journey along the way. The Opportunity at Komodo Health The Opportunity at Komodo Health Komodo Health leverages the latest data engineering technology such as Spark, Airflow, and Snowflake to tackle some of healthcare's biggest challenges by transforming extraordinary amounts of data into rich and meaningful insights. As a Data Engineer on the Platform team, you will help lead the development of Komodo Health's platform-enabled workflow tools. The Komodo platform powers all of Komodo's current and future workflow analytical applications and enables 3rd party builders to integrate with, extend, customize, or build on the platform. Reporting directly to the Engineering Manager, you will be solving complex data challenges while designing and implementing data processing and transformation at a scale that powers state-of-the-art interactive product experiences. You will enable smarter, more innovative uses of healthcare data by building robust data pipelines and implementing data best practices. Looking back on your first 12 months at Komodo Health, you will have… Looking back on your first 12 months at Komodo Health, you will have… Worked on foundational pieces of our data platform architecture, pipelines, analytics, and services underlying our platform Partnered with Engineering team members, Product Managers, Data Scientists, and customer-facing teams to understand and deliver python packages as well as web-based python services Designed and developed reliable data pipelines that transform data at scale, orchestrated jobs via Airflow, using SQL and Python in Snowflake and/or Spark Helped implement continuous improvements to our data governance practices and implemented data quality improvements Implemented technical enhancements to our CI/CD processes and/or built tooling to ensure data consistency and quality Gained an understanding of the broader Komodo Health data landscape and being part of architectural decisions for the Healthcare Analytics and Platform as a Service offerings Worked on foundational pieces of our data platform architecture, pipelines, analytics, and services underlying our platform Partnered with Engineering team members, Product Managers, Data Scientists, and customer-facing teams to understand and deliver python packages as well as web-based python services Designed and developed reliable data pipelines that transform data at scale, orchestrated jobs via Airflow, using SQL and Python in Snowflake and/or Spark Helped implement continuous improvements to our data governance practices and implemented data quality improvements Implemented technical enhancements to our CI/CD processes and/or built tooling to ensure data consistency and quality Gained an understanding of the broader Komodo Health data landscape and being part of architectural decisions for the Healthcare Analytics and Platform as a Service offerings What you bring to Komodo: What you bring to Komodo: Expertise in writing enterprise-level code and contributing to large data pipelining and API processing with Python Experience with SQL and query design on large, complex datasets Ability to use a variety of relational, NoSQL, Postgres, and/or MPP databases (ideally Snowflake on AWS) and leading data modeling, schema design, and data storage best practices Demonstrated proficiency in designing and developing with distributed data processing platforms like Spark and pipeline orchestration tools like Airflow A thirst for knowledge, willingness to learn, and a growth-oriented mindset Committed to fostering an inclusive environment where your teammates feel motivated to succeed Experience enhancing CI/CD build tooling in a containerized environment, from deployment pipelines (Jenkins, etc), infrastructure as code (Terraform, Cloudformation), and configuration management via Docker and Kubernetes Excellent cross-team communication and collaboration skills Expertise in writing enterprise-level code and contributing to large data pipelining and API processing with Python Experience with SQL and query design on large, complex datasets Ability to use a variety of relational, NoSQL, Postgres, and/or MPP databases (ideally Snowflake on AWS) and leading data modeling, schema design, and data storage best practices Demonstrated proficiency in designing and developing with distributed data processing platforms like Spark and pipeline orchestration tools like Airflow A thirst for knowledge, willingness to learn, and a growth-oriented mindset Committed to fostering an inclusive environment where your teammates feel motivated to succeed Experience enhancing CI/CD build tooling in a containerized environment, from deployment pipelines (Jenkins, etc), infrastructure as code (Terraform, Cloudformation), and configuration management via Docker and Kubernetes Excellent cross-team communication and collaboration skills Compensation at Komodo Health Compensation at Komodo Health We are committed to providing competitive compensation for all roles at Komodo Health. We carefully consider multiple factors when determining compensation, including your skills, experience, and location while balancing internal equity relative to peers at the company. The targeted base salary range for the Data Engineer role is $123,900 to $186,300 plus a competitive bonus and equity package. #LI-Remote #LI-JK1 Where You'll Work Komodo Health has a hybrid work model; we recognize the power of choice and importance of flexibility for the well-being of both our company and our individual Dragons. Roles may be completely remote based anywhere in the country listed, remote but based in a specific region, or local (commuting distance) to one of our hubs in San Francisco, New York City, or Chicago with remote work options. What We Offer On top of our commitment to providing competitive, fair pay for all roles at Komodo Health, we're proud to offer robust and inclusive benefits to all Dragons at Komodo Health. We offer global time off programs, extensive internal and external career development and learning opportunities, multiple affinity groups celebrating our team's diversity, and an annual wellness and productivity stipend to support you in being your healthiest, best self. Equal Opportunity Statement Komodo Health provides equal employment opportunities to all applicants and employees. We prohibit discrimination and harassment of any type with regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state, or local laws. Where You'll Work Where You'll Work Komodo Health has a hybrid work model; we recognize the power of choice and importance of flexibility for the well-being of both our company and our individual Dragons. Roles may be completely remote based anywhere in the country listed, remote but based in a specific region, or local (commuting distance) to one of our hubs in San Francisco, New York City, or Chicago with remote work options. What We Offer What We Offer On top of our commitment to providing competitive, fair pay for all roles at Komodo Health, we're proud to offer robust and inclusive benefits to all Dragons at Komodo Health. We offer global time off programs, extensive internal and external career development and learning opportunities, multiple affinity groups celebrating our team's diversity, and an annual wellness and productivity stipend to support you in being your healthiest, best self. Equal Opportunity Statement Equal Opportunity Statement Komodo Health provides equal employment opportunities to all applicants and employees. We prohibit discrimination and harassment of any type with regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state, or local laws.",3.9,501 to 1000 Employees,2014,Company - Private,Enterprise Software & Network Solutions,Information Technology,Unknown / Non-Applicable
"Santa Ana, CA",Data Engineer,$83K - $117K (Glassdoor est.),Orange County's Credit Union,"Great Opportunity At Orange County's Credit Union Must reside in the state of CA, AZ, NV or TX. Are you looking to join a dynamic, fast-paced team environment with a culture of collaboration and belonging? If so, let’s talk. Orange County's Credit Union is now seeking a talented and driven individual to accelerate our efforts and be a major part of our team and culture. Our team members are grounded in core values, have a strong capacity to learn, the energy to get things done, and bring real world experiences to help us think in new ways. Orange County's Credit Union actively invest in our team members to support their long-term growth so they can continue to advance our mission and achieve their highest potential. Are you passionate about the future? Are you a data engineer who specializes in wrangling a wide range of data into versatile, accessible sources of knowledge? If yes, PLEASE APPLY IMMEDATELY! More about Orange County's Credit Union: Workplace Excellence. Through our associates' opinions and voices, Orange County's Credit Union is proud to be recognized year over year as one of the best places to work in Orange County and is a recipient of the Peter Barron Stark Best of the Best Award for highest associate satisfaction in the workplace. As a leading financial service provider with over 80 years of experience serving 117,000+ members, Orange County's Credit Union is currently over $2 billion in assets & growing. Generous benefits include paid health insurance, time-off benefits, 401(k), and a professional, friendly work environment (with remote and hybrid options) focused on achieving goals, recognizing successes and excelling at member service. Putting People First: Connect, Discover, Deliver & Wow is Orange County’s Credit Union mantra. If you’re passionate about serving people, this role is rewarding, brings purpose, and the opportunity to make a difference! Overview: Are you our next Data Engineer? With your strong understanding of financial services operations (e.g., banking, asset management, insurance, mortgage, consumer & business lending) you will be part of an innovative team that will architect, design and implement data analytics solutions to further advance the organization’s strategic goals. In collaboration with Business and IT partners, you will support our efforts in re-engineering, optimizing and advancing the organization’s data platform with modern cloud-based analytics technologies that will address near-term and future business needs. Be part of our vision to advance data-driven insights and decision making for our mission driven organization. Essential Functions: Collaborate with delivery team and engage with organizational stakeholders (Business and IT) to design, develop and deliver end-to-end enterprise data analytics solutions to enable data-driven insights and decision making. Translate business requirements to technical solutions by applying technical knowledge and strong business acumen. Solve business problems and complex data requirements/challenges by incorporating standards and best practices into engineering solutions, and leveraging modern data science programming languages (e.g., SQL, Python, R, Scala, SAS) and Azure data and analytics services. Develop and implement database designs (logical and physical) and data models (normalized and dimensional) to support the new analytics platform. Design, implement and maintain data ingestion/integration and end-to-end data pipeline processes using Azure technologies for a wide variety of traditional and non-traditional data sources/formats (structured, unstructured, and semi-structured) through various protocols (e.g., REST, SOAP, SFTP, MQ, etc). Technical Must Haves for this Role: 5+ years of experience in Information Technology within a medium to large enterprise with complex business and IT environment. 3+ years of experience as a Data Engineer working with cross-functional teams (within IT and/or Business) on enterprise level business intelligence/data analytics implementations using Azure cloud-based analytics platforms/technologies (including hands-on experience with Microsoft/Azure stack, e.g., Synapse, Data Factory, Data Bricks, Data Lake, Data Catalog, SSIS, SQL, etc., and NoSQL databases). 3+ years of hands-on experience in designing, implementing and maintaining data ingestion /integration and end-to-end data pipeline using Azure technologies for a wide variety of traditional and non-traditional data sources/formats (structured, unstructured, and semi-structured) through various protocols (e.g., REST, SOAP, SFTP, MQ, etc.). 2+ years of experience working with and developing database designs (logical and physical) and data models (normalized and dimensional) for data warehouse, data marts and operational data stores. 2+ years of hands-on experience working with data science programming languages (e.g., SQL, Python, R, Scala, SAS). Experience working in an agile delivery environment with working knowledge of continuous integration/continuous delivery (CI/CD) and DevOps practices.  The targeted hourly range is $36.55 - $54.83. Final offer will be determined based on experience, education, training/certifications and specialized skills.  We perform thorough background checks and credit checks. EOE. Great Opportunity At Orange County's Credit Union Must reside in the state of CA, AZ, NV or TX. Are you looking to join a dynamic, fast-paced team environment with a culture of collaboration and belonging? If so, let’s talk. Orange County's Credit Union is now seeking a talented and driven individual to accelerate our efforts and be a major part of our team and culture. Our team members are grounded in core values, have a strong capacity to learn, the energy to get things done, and bring real world experiences to help us think in new ways. Orange County's Credit Union actively invest in our team members to support their long-term growth so they can continue to advance our mission and achieve their highest potential. Are you passionate about the future? Are you a data engineer who specializes in wrangling a wide range of data into versatile, accessible sources of knowledge? If yes, PLEASE APPLY IMMEDATELY! More about Orange County's Credit Union: Workplace Excellence. Through our associates' opinions and voices, Orange County's Credit Union is proud to be recognized year over year as one of the best places to work in Orange County and is a recipient of the Peter Barron Stark Best of the Best Award for highest associate satisfaction in the workplace. As a leading financial service provider with over 80 years of experience serving 117,000+ members, Orange County's Credit Union is currently over $2 billion in assets & growing. Generous benefits include paid health insurance, time-off benefits, 401(k), and a professional, friendly work environment (with remote and hybrid options) focused on achieving goals, recognizing successes and excelling at member service. Putting People First: Connect, Discover, Deliver & Wow is Orange County’s Credit Union mantra. If you’re passionate about serving people, this role is rewarding, brings purpose, and the opportunity to make a difference! Overview: Are you our next Data Engineer? With your strong understanding of financial services operations (e.g., banking, asset management, insurance, mortgage, consumer & business lending) you will be part of an innovative team that will architect, design and implement data analytics solutions to further advance the organization’s strategic goals. In collaboration with Business and IT partners, you will support our efforts in re-engineering, optimizing and advancing the organization’s data platform with modern cloud-based analytics technologies that will address near-term and future business needs. Be part of our vision to advance data-driven insights and decision making for our mission driven organization. Essential Functions: Collaborate with delivery team and engage with organizational stakeholders (Business and IT) to design, develop and deliver end-to-end enterprise data analytics solutions to enable data-driven insights and decision making. Translate business requirements to technical solutions by applying technical knowledge and strong business acumen. Solve business problems and complex data requirements/challenges by incorporating standards and best practices into engineering solutions, and leveraging modern data science programming languages (e.g., SQL, Python, R, Scala, SAS) and Azure data and analytics services. Develop and implement database designs (logical and physical) and data models (normalized and dimensional) to support the new analytics platform. Design, implement and maintain data ingestion/integration and end-to-end data pipeline processes using Azure technologies for a wide variety of traditional and non-traditional data sources/formats (structured, unstructured, and semi-structured) through various protocols (e.g., REST, SOAP, SFTP, MQ, etc). Technical Must Haves for this Role: 5+ years of experience in Information Technology within a medium to large enterprise with complex business and IT environment. 3+ years of experience as a Data Engineer working with cross-functional teams (within IT and/or Business) on enterprise level business intelligence/data analytics implementations using Azure cloud-based analytics platforms/technologies (including hands-on experience with Microsoft/Azure stack, e.g., Synapse, Data Factory, Data Bricks, Data Lake, Data Catalog, SSIS, SQL, etc., and NoSQL databases). 3+ years of hands-on experience in designing, implementing and maintaining data ingestion /integration and end-to-end data pipeline using Azure technologies for a wide variety of traditional and non-traditional data sources/formats (structured, unstructured, and semi-structured) through various protocols (e.g., REST, SOAP, SFTP, MQ, etc.). 2+ years of experience working with and developing database designs (logical and physical) and data models (normalized and dimensional) for data warehouse, data marts and operational data stores. 2+ years of hands-on experience working with data science programming languages (e.g., SQL, Python, R, Scala, SAS). Experience working in an agile delivery environment with working knowledge of continuous integration/continuous delivery (CI/CD) and DevOps practices.  The targeted hourly range is $36.55 - $54.83. Final offer will be determined based on experience, education, training/certifications and specialized skills.  We perform thorough background checks and credit checks. EOE. Great Opportunity At Orange County's Credit Union Must reside in the state of CA, AZ, NV or TX. Are you looking to join a dynamic, fast-paced team environment with a culture of collaboration and belonging? If so, let’s talk. Orange County's Credit Union is now seeking a talented and driven individual to accelerate our efforts and be a major part of our team and culture. Our team members are grounded in core values, have a strong capacity to learn, the energy to get things done, and bring real world experiences to help us think in new ways. Orange County's Credit Union actively invest in our team members to support their long-term growth so they can continue to advance our mission and achieve their highest potential. Are you passionate about the future? Are you a data engineer who specializes in wrangling a wide range of data into versatile, accessible sources of knowledge? If yes, PLEASE APPLY IMMEDATELY! More about Orange County's Credit Union: Workplace Excellence. Through our associates' opinions and voices, Orange County's Credit Union is proud to be recognized year over year as one of the best places to work in Orange County and is a recipient of the Peter Barron Stark Best of the Best Award for highest associate satisfaction in the workplace. As a leading financial service provider with over 80 years of experience serving 117,000+ members, Orange County's Credit Union is currently over $2 billion in assets & growing. Generous benefits include paid health insurance, time-off benefits, 401(k), and a professional, friendly work environment (with remote and hybrid options) focused on achieving goals, recognizing successes and excelling at member service. Putting People First: Connect, Discover, Deliver & Wow is Orange County’s Credit Union mantra. If you’re passionate about serving people, this role is rewarding, brings purpose, and the opportunity to make a difference! Overview: Are you our next Data Engineer? With your strong understanding of financial services operations (e.g., banking, asset management, insurance, mortgage, consumer & business lending) you will be part of an innovative team that will architect, design and implement data analytics solutions to further advance the organization’s strategic goals. In collaboration with Business and IT partners, you will support our efforts in re-engineering, optimizing and advancing the organization’s data platform with modern cloud-based analytics technologies that will address near-term and future business needs. Be part of our vision to advance data-driven insights and decision making for our mission driven organization. Essential Functions: Collaborate with delivery team and engage with organizational stakeholders (Business and IT) to design, develop and deliver end-to-end enterprise data analytics solutions to enable data-driven insights and decision making. Translate business requirements to technical solutions by applying technical knowledge and strong business acumen. Solve business problems and complex data requirements/challenges by incorporating standards and best practices into engineering solutions, and leveraging modern data science programming languages (e.g., SQL, Python, R, Scala, SAS) and Azure data and analytics services. Develop and implement database designs (logical and physical) and data models (normalized and dimensional) to support the new analytics platform. Design, implement and maintain data ingestion/integration and end-to-end data pipeline processes using Azure technologies for a wide variety of traditional and non-traditional data sources/formats (structured, unstructured, and semi-structured) through various protocols (e.g., REST, SOAP, SFTP, MQ, etc). Technical Must Haves for this Role: 5+ years of experience in Information Technology within a medium to large enterprise with complex business and IT environment. 3+ years of experience as a Data Engineer working with cross-functional teams (within IT and/or Business) on enterprise level business intelligence/data analytics implementations using Azure cloud-based analytics platforms/technologies (including hands-on experience with Microsoft/Azure stack, e.g., Synapse, Data Factory, Data Bricks, Data Lake, Data Catalog, SSIS, SQL, etc., and NoSQL databases). 3+ years of hands-on experience in designing, implementing and maintaining data ingestion /integration and end-to-end data pipeline using Azure technologies for a wide variety of traditional and non-traditional data sources/formats (structured, unstructured, and semi-structured) through various protocols (e.g., REST, SOAP, SFTP, MQ, etc.). 2+ years of experience working with and developing database designs (logical and physical) and data models (normalized and dimensional) for data warehouse, data marts and operational data stores. 2+ years of hands-on experience working with data science programming languages (e.g., SQL, Python, R, Scala, SAS). Experience working in an agile delivery environment with working knowledge of continuous integration/continuous delivery (CI/CD) and DevOps practices.  The targeted hourly range is $36.55 - $54.83. Final offer will be determined based on experience, education, training/certifications and specialized skills.  We perform thorough background checks and credit checks. EOE. Great Opportunity At Orange County's Credit Union Must reside in the state of CA, AZ, NV or TX. Are you looking to join a dynamic, fast-paced team environment with a culture of collaboration and belonging? If so, let’s talk. Orange County's Credit Union is now seeking a talented and driven individual to accelerate our efforts and be a major part of our team and culture. Our team members are grounded in core values, have a strong capacity to learn, the energy to get things done, and bring real world experiences to help us think in new ways. Orange County's Credit Union actively invest in our team members to support their long-term growth so they can continue to advance our mission and achieve their highest potential. Are you passionate about the future? Are you a data engineer who specializes in wrangling a wide range of data into versatile, accessible sources of knowledge? If yes, PLEASE APPLY IMMEDATELY! More about Orange County's Credit Union: Workplace Excellence. Through our associates' opinions and voices, Orange County's Credit Union is proud to be recognized year over year as one of the best places to work in Orange County and is a recipient of the Peter Barron Stark Best of the Best Award for highest associate satisfaction in the workplace. As a leading financial service provider with over 80 years of experience serving 117,000+ members, Orange County's Credit Union is currently over $2 billion in assets & growing. Generous benefits include paid health insurance, time-off benefits, 401(k), and a professional, friendly work environment (with remote and hybrid options) focused on achieving goals, recognizing successes and excelling at member service. Putting People First: Connect, Discover, Deliver & Wow is Orange County’s Credit Union mantra. If you’re passionate about serving people, this role is rewarding, brings purpose, and the opportunity to make a difference! Overview: Are you our next Data Engineer? With your strong understanding of financial services operations (e.g., banking, asset management, insurance, mortgage, consumer & business lending) you will be part of an innovative team that will architect, design and implement data analytics solutions to further advance the organization’s strategic goals. In collaboration with Business and IT partners, you will support our efforts in re-engineering, optimizing and advancing the organization’s data platform with modern cloud-based analytics technologies that will address near-term and future business needs. Be part of our vision to advance data-driven insights and decision making for our mission driven organization. Essential Functions: Collaborate with delivery team and engage with organizational stakeholders (Business and IT) to design, develop and deliver end-to-end enterprise data analytics solutions to enable data-driven insights and decision making. Translate business requirements to technical solutions by applying technical knowledge and strong business acumen. Solve business problems and complex data requirements/challenges by incorporating standards and best practices into engineering solutions, and leveraging modern data science programming languages (e.g., SQL, Python, R, Scala, SAS) and Azure data and analytics services. Develop and implement database designs (logical and physical) and data models (normalized and dimensional) to support the new analytics platform. Design, implement and maintain data ingestion/integration and end-to-end data pipeline processes using Azure technologies for a wide variety of traditional and non-traditional data sources/formats (structured, unstructured, and semi-structured) through various protocols (e.g., REST, SOAP, SFTP, MQ, etc). Technical Must Haves for this Role: 5+ years of experience in Information Technology within a medium to large enterprise with complex business and IT environment. 3+ years of experience as a Data Engineer working with cross-functional teams (within IT and/or Business) on enterprise level business intelligence/data analytics implementations using Azure cloud-based analytics platforms/technologies (including hands-on experience with Microsoft/Azure stack, e.g., Synapse, Data Factory, Data Bricks, Data Lake, Data Catalog, SSIS, SQL, etc., and NoSQL databases). 3+ years of hands-on experience in designing, implementing and maintaining data ingestion /integration and end-to-end data pipeline using Azure technologies for a wide variety of traditional and non-traditional data sources/formats (structured, unstructured, and semi-structured) through various protocols (e.g., REST, SOAP, SFTP, MQ, etc.). 2+ years of experience working with and developing database designs (logical and physical) and data models (normalized and dimensional) for data warehouse, data marts and operational data stores. 2+ years of hands-on experience working with data science programming languages (e.g., SQL, Python, R, Scala, SAS). Experience working in an agile delivery environment with working knowledge of continuous integration/continuous delivery (CI/CD) and DevOps practices.  The targeted hourly range is $36.55 - $54.83. Final offer will be determined based on experience, education, training/certifications and specialized skills.  We perform thorough background checks and credit checks. EOE. Great Opportunity At Orange County's Credit Union Must reside in the state of CA, AZ, NV or TX. Are you looking to join a dynamic, fast-paced team environment with a culture of collaboration and belonging? If so, let’s talk. Orange County's Credit Union is now seeking a talented and driven individual to accelerate our efforts and be a major part of our team and culture. Our team members are grounded in core values, have a strong capacity to learn, the energy to get things done, and bring real world experiences to help us think in new ways. Orange County's Credit Union actively invest in our team members to support their long-term growth so they can continue to advance our mission and achieve their highest potential. Are you passionate about the future? Are you a data engineer who specializes in wrangling a wide range of data into versatile, accessible sources of knowledge? If yes, PLEASE APPLY IMMEDATELY! More about Orange County's Credit Union: Workplace Excellence. Through our associates' opinions and voices, Orange County's Credit Union is proud to be recognized year over year as one of the best places to work in Orange County and is a recipient of the Peter Barron Stark Best of the Best Award for highest associate satisfaction in the workplace. As a leading financial service provider with over 80 years of experience serving 117,000+ members, Orange County's Credit Union is currently over $2 billion in assets & growing. Generous benefits include paid health insurance, time-off benefits, 401(k), and a professional, friendly work environment (with remote and hybrid options) focused on achieving goals, recognizing successes and excelling at member service. Putting People First: Connect, Discover, Deliver & Wow is Orange County’s Credit Union mantra. If you’re passionate about serving people, this role is rewarding, brings purpose, and the opportunity to make a difference! Overview: Are you our next Data Engineer? With your strong understanding of financial services operations (e.g., banking, asset management, insurance, mortgage, consumer & business lending) you will be part of an innovative team that will architect, design and implement data analytics solutions to further advance the organization’s strategic goals. In collaboration with Business and IT partners, you will support our efforts in re-engineering, optimizing and advancing the organization’s data platform with modern cloud-based analytics technologies that will address near-term and future business needs. Be part of our vision to advance data-driven insights and decision making for our mission driven organization. Essential Functions: Collaborate with delivery team and engage with organizational stakeholders (Business and IT) to design, develop and deliver end-to-end enterprise data analytics solutions to enable data-driven insights and decision making. Translate business requirements to technical solutions by applying technical knowledge and strong business acumen. Solve business problems and complex data requirements/challenges by incorporating standards and best practices into engineering solutions, and leveraging modern data science programming languages (e.g., SQL, Python, R, Scala, SAS) and Azure data and analytics services. Develop and implement database designs (logical and physical) and data models (normalized and dimensional) to support the new analytics platform. Design, implement and maintain data ingestion/integration and end-to-end data pipeline processes using Azure technologies for a wide variety of traditional and non-traditional data sources/formats (structured, unstructured, and semi-structured) through various protocols (e.g., REST, SOAP, SFTP, MQ, etc). Technical Must Haves for this Role: 5+ years of experience in Information Technology within a medium to large enterprise with complex business and IT environment. 3+ years of experience as a Data Engineer working with cross-functional teams (within IT and/or Business) on enterprise level business intelligence/data analytics implementations using Azure cloud-based analytics platforms/technologies (including hands-on experience with Microsoft/Azure stack, e.g., Synapse, Data Factory, Data Bricks, Data Lake, Data Catalog, SSIS, SQL, etc., and NoSQL databases). 3+ years of hands-on experience in designing, implementing and maintaining data ingestion /integration and end-to-end data pipeline using Azure technologies for a wide variety of traditional and non-traditional data sources/formats (structured, unstructured, and semi-structured) through various protocols (e.g., REST, SOAP, SFTP, MQ, etc.). 2+ years of experience working with and developing database designs (logical and physical) and data models (normalized and dimensional) for data warehouse, data marts and operational data stores. 2+ years of hands-on experience working with data science programming languages (e.g., SQL, Python, R, Scala, SAS). Experience working in an agile delivery environment with working knowledge of continuous integration/continuous delivery (CI/CD) and DevOps practices.  The targeted hourly range is $36.55 - $54.83. Final offer will be determined based on experience, education, training/certifications and specialized skills.  We perform thorough background checks and credit checks. EOE. Great Opportunity At Orange County's Credit Union Must reside in the state of CA, AZ, NV or TX. Are you looking to join a dynamic, fast-paced team environment with a culture of collaboration and belonging? If so, let’s talk. Orange County's Credit Union is now seeking a talented and driven individual to accelerate our efforts and be a major part of our team and culture. Our team members are grounded in core values, have a strong capacity to learn, the energy to get things done, and bring real world experiences to help us think in new ways. Orange County's Credit Union actively invest in our team members to support their long-term growth so they can continue to advance our mission and achieve their highest potential. Are you passionate about the future? Are you a data engineer who specializes in wrangling a wide range of data into versatile, accessible sources of knowledge? If yes, PLEASE APPLY IMMEDATELY! More about Orange County's Credit Union: Workplace Excellence. Through our associates' opinions and voices, Orange County's Credit Union is proud to be recognized year over year as one of the best places to work in Orange County and is a recipient of the Peter Barron Stark Best of the Best Award for highest associate satisfaction in the workplace. As a leading financial service provider with over 80 years of experience serving 117,000+ members, Orange County's Credit Union is currently over $2 billion in assets & growing. Generous benefits include paid health insurance, time-off benefits, 401(k), and a professional, friendly work environment (with remote and hybrid options) focused on achieving goals, recognizing successes and excelling at member service. Putting People First: Connect, Discover, Deliver & Wow is Orange County’s Credit Union mantra. If you’re passionate about serving people, this role is rewarding, brings purpose, and the opportunity to make a difference! Overview: Are you our next Data Engineer? With your strong understanding of financial services operations (e.g., banking, asset management, insurance, mortgage, consumer & business lending) you will be part of an innovative team that will architect, design and implement data analytics solutions to further advance the organization’s strategic goals. In collaboration with Business and IT partners, you will support our efforts in re-engineering, optimizing and advancing the organization’s data platform with modern cloud-based analytics technologies that will address near-term and future business needs. Be part of our vision to advance data-driven insights and decision making for our mission driven organization. Essential Functions: Collaborate with delivery team and engage with organizational stakeholders (Business and IT) to design, develop and deliver end-to-end enterprise data analytics solutions to enable data-driven insights and decision making. Translate business requirements to technical solutions by applying technical knowledge and strong business acumen. Solve business problems and complex data requirements/challenges by incorporating standards and best practices into engineering solutions, and leveraging modern data science programming languages (e.g., SQL, Python, R, Scala, SAS) and Azure data and analytics services. Develop and implement database designs (logical and physical) and data models (normalized and dimensional) to support the new analytics platform. Design, implement and maintain data ingestion/integration and end-to-end data pipeline processes using Azure technologies for a wide variety of traditional and non-traditional data sources/formats (structured, unstructured, and semi-structured) through various protocols (e.g., REST, SOAP, SFTP, MQ, etc). Technical Must Haves for this Role: 5+ years of experience in Information Technology within a medium to large enterprise with complex business and IT environment. 3+ years of experience as a Data Engineer working with cross-functional teams (within IT and/or Business) on enterprise level business intelligence/data analytics implementations using Azure cloud-based analytics platforms/technologies (including hands-on experience with Microsoft/Azure stack, e.g., Synapse, Data Factory, Data Bricks, Data Lake, Data Catalog, SSIS, SQL, etc., and NoSQL databases). 3+ years of hands-on experience in designing, implementing and maintaining data ingestion /integration and end-to-end data pipeline using Azure technologies for a wide variety of traditional and non-traditional data sources/formats (structured, unstructured, and semi-structured) through various protocols (e.g., REST, SOAP, SFTP, MQ, etc.). 2+ years of experience working with and developing database designs (logical and physical) and data models (normalized and dimensional) for data warehouse, data marts and operational data stores. 2+ years of hands-on experience working with data science programming languages (e.g., SQL, Python, R, Scala, SAS). Experience working in an agile delivery environment with working knowledge of continuous integration/continuous delivery (CI/CD) and DevOps practices.  The targeted hourly range is $36.55 - $54.83. Final offer will be determined based on experience, education, training/certifications and specialized skills.  We perform thorough background checks and credit checks. EOE. Great Opportunity At Orange County's Credit Union Must reside in the state of CA, AZ, NV or TX. Are you looking to join a dynamic, fast-paced team environment with a culture of collaboration and belonging? If so, let’s talk. Orange County's Credit Union is now seeking a talented and driven individual to accelerate our efforts and be a major part of our team and culture. Our team members are grounded in core values, have a strong capacity to learn, the energy to get things done, and bring real world experiences to help us think in new ways. Orange County's Credit Union actively invest in our team members to support their long-term growth so they can continue to advance our mission and achieve their highest potential. Are you passionate about the future? Are you a data engineer who specializes in wrangling a wide range of data into versatile, accessible sources of knowledge? If yes, PLEASE APPLY IMMEDATELY! More about Orange County's Credit Union: Workplace Excellence. Through our associates' opinions and voices, Orange County's Credit Union is proud to be recognized year over year as one of the best places to work in Orange County and is a recipient of the Peter Barron Stark Best of the Best Award for highest associate satisfaction in the workplace. As a leading financial service provider with over 80 years of experience serving 117,000+ members, Orange County's Credit Union is currently over $2 billion in assets & growing. Generous benefits include paid health insurance, time-off benefits, 401(k), and a professional, friendly work environment (with remote and hybrid options) focused on achieving goals, recognizing successes and excelling at member service. Putting People First: Connect, Discover, Deliver & Wow is Orange County’s Credit Union mantra. If you’re passionate about serving people, this role is rewarding, brings purpose, and the opportunity to make a difference! Overview: Are you our next Data Engineer? With your strong understanding of financial services operations (e.g., banking, asset management, insurance, mortgage, consumer & business lending) you will be part of an innovative team that will architect, design and implement data analytics solutions to further advance the organization’s strategic goals. In collaboration with Business and IT partners, you will support our efforts in re-engineering, optimizing and advancing the organization’s data platform with modern cloud-based analytics technologies that will address near-term and future business needs. Be part of our vision to advance data-driven insights and decision making for our mission driven organization. Essential Functions: Collaborate with delivery team and engage with organizational stakeholders (Business and IT) to design, develop and deliver end-to-end enterprise data analytics solutions to enable data-driven insights and decision making. Translate business requirements to technical solutions by applying technical knowledge and strong business acumen. Solve business problems and complex data requirements/challenges by incorporating standards and best practices into engineering solutions, and leveraging modern data science programming languages (e.g., SQL, Python, R, Scala, SAS) and Azure data and analytics services. Develop and implement database designs (logical and physical) and data models (normalized and dimensional) to support the new analytics platform. Design, implement and maintain data ingestion/integration and end-to-end data pipeline processes using Azure technologies for a wide variety of traditional and non-traditional data sources/formats (structured, unstructured, and semi-structured) through various protocols (e.g., REST, SOAP, SFTP, MQ, etc). Technical Must Haves for this Role: 5+ years of experience in Information Technology within a medium to large enterprise with complex business and IT environment. 3+ years of experience as a Data Engineer working with cross-functional teams (within IT and/or Business) on enterprise level business intelligence/data analytics implementations using Azure cloud-based analytics platforms/technologies (including hands-on experience with Microsoft/Azure stack, e.g., Synapse, Data Factory, Data Bricks, Data Lake, Data Catalog, SSIS, SQL, etc., and NoSQL databases). 3+ years of hands-on experience in designing, implementing and maintaining data ingestion /integration and end-to-end data pipeline using Azure technologies for a wide variety of traditional and non-traditional data sources/formats (structured, unstructured, and semi-structured) through various protocols (e.g., REST, SOAP, SFTP, MQ, etc.). 2+ years of experience working with and developing database designs (logical and physical) and data models (normalized and dimensional) for data warehouse, data marts and operational data stores. 2+ years of hands-on experience working with data science programming languages (e.g., SQL, Python, R, Scala, SAS). Experience working in an agile delivery environment with working knowledge of continuous integration/continuous delivery (CI/CD) and DevOps practices.  The targeted hourly range is $36.55 - $54.83. Final offer will be determined based on experience, education, training/certifications and specialized skills.  We perform thorough background checks and credit checks. EOE. Great Opportunity At Orange County's Credit Union Must reside in the state of CA, AZ, NV or TX. Are you looking to join a dynamic, fast-paced team environment with a culture of collaboration and belonging? If so, let’s talk. Orange County's Credit Union is now seeking a talented and driven individual to accelerate our efforts and be a major part of our team and culture. Our team members are grounded in core values, have a strong capacity to learn, the energy to get things done, and bring real world experiences to help us think in new ways. Orange County's Credit Union actively invest in our team members to support their long-term growth so they can continue to advance our mission and achieve their highest potential. Are you passionate about the future? Are you a data engineer who specializes in wrangling a wide range of data into versatile, accessible sources of knowledge? If yes, PLEASE APPLY IMMEDATELY! More about Orange County's Credit Union: Workplace Excellence. Through our associates' opinions and voices, Orange County's Credit Union is proud to be recognized year over year as one of the best places to work in Orange County and is a recipient of the Peter Barron Stark Best of the Best Award for highest associate satisfaction in the workplace. As a leading financial service provider with over 80 years of experience serving 117,000+ members, Orange County's Credit Union is currently over $2 billion in assets & growing. Generous benefits include paid health insurance, time-off benefits, 401(k), and a professional, friendly work environment (with remote and hybrid options) focused on achieving goals, recognizing successes and excelling at member service. Putting People First: Connect, Discover, Deliver & Wow is Orange County’s Credit Union mantra. If you’re passionate about serving people, this role is rewarding, brings purpose, and the opportunity to make a difference! Overview: Are you our next Data Engineer? With your strong understanding of financial services operations (e.g., banking, asset management, insurance, mortgage, consumer & business lending) you will be part of an innovative team that will architect, design and implement data analytics solutions to further advance the organization’s strategic goals. In collaboration with Business and IT partners, you will support our efforts in re-engineering, optimizing and advancing the organization’s data platform with modern cloud-based analytics technologies that will address near-term and future business needs. Be part of our vision to advance data-driven insights and decision making for our mission driven organization. Essential Functions: Collaborate with delivery team and engage with organizational stakeholders (Business and IT) to design, develop and deliver end-to-end enterprise data analytics solutions to enable data-driven insights and decision making. Translate business requirements to technical solutions by applying technical knowledge and strong business acumen. Solve business problems and complex data requirements/challenges by incorporating standards and best practices into engineering solutions, and leveraging modern data science programming languages (e.g., SQL, Python, R, Scala, SAS) and Azure data and analytics services. Develop and implement database designs (logical and physical) and data models (normalized and dimensional) to support the new analytics platform. Design, implement and maintain data ingestion/integration and end-to-end data pipeline processes using Azure technologies for a wide variety of traditional and non-traditional data sources/formats (structured, unstructured, and semi-structured) through various protocols (e.g., REST, SOAP, SFTP, MQ, etc). Technical Must Haves for this Role: 5+ years of experience in Information Technology within a medium to large enterprise with complex business and IT environment. 3+ years of experience as a Data Engineer working with cross-functional teams (within IT and/or Business) on enterprise level business intelligence/data analytics implementations using Azure cloud-based analytics platforms/technologies (including hands-on experience with Microsoft/Azure stack, e.g., Synapse, Data Factory, Data Bricks, Data Lake, Data Catalog, SSIS, SQL, etc., and NoSQL databases). 3+ years of hands-on experience in designing, implementing and maintaining data ingestion /integration and end-to-end data pipeline using Azure technologies for a wide variety of traditional and non-traditional data sources/formats (structured, unstructured, and semi-structured) through various protocols (e.g., REST, SOAP, SFTP, MQ, etc.). 2+ years of experience working with and developing database designs (logical and physical) and data models (normalized and dimensional) for data warehouse, data marts and operational data stores. 2+ years of hands-on experience working with data science programming languages (e.g., SQL, Python, R, Scala, SAS). Experience working in an agile delivery environment with working knowledge of continuous integration/continuous delivery (CI/CD) and DevOps practices.  The targeted hourly range is $36.55 - $54.83. Final offer will be determined based on experience, education, training/certifications and specialized skills.  We perform thorough background checks and credit checks. EOE. Great Opportunity At Orange County's Credit Union Must reside in the state of CA, AZ, NV or TX. Are you looking to join a dynamic, fast-paced team environment with a culture of collaboration and belonging? If so, let’s talk. Orange County's Credit Union is now seeking a talented and driven individual to accelerate our efforts and be a major part of our team and culture. Our team members are grounded in core values, have a strong capacity to learn, the energy to get things done, and bring real world experiences to help us think in new ways. Orange County's Credit Union actively invest in our team members to support their long-term growth so they can continue to advance our mission and achieve their highest potential. Are you passionate about the future? Are you a data engineer who specializes in wrangling a wide range of data into versatile, accessible sources of knowledge? If yes, PLEASE APPLY IMMEDATELY! More about Orange County's Credit Union: Workplace Excellence. Through our associates' opinions and voices, Orange County's Credit Union is proud to be recognized year over year as one of the best places to work in Orange County and is a recipient of the Peter Barron Stark Best of the Best Award for highest associate satisfaction in the workplace. As a leading financial service provider with over 80 years of experience serving 117,000+ members, Orange County's Credit Union is currently over $2 billion in assets & growing. Generous benefits include paid health insurance, time-off benefits, 401(k), and a professional, friendly work environment (with remote and hybrid options) focused on achieving goals, recognizing successes and excelling at member service. Putting People First: Connect, Discover, Deliver & Wow is Orange County’s Credit Union mantra. If you’re passionate about serving people, this role is rewarding, brings purpose, and the opportunity to make a difference! Overview: Are you our next Data Engineer? With your strong understanding of financial services operations (e.g., banking, asset management, insurance, mortgage, consumer & business lending) you will be part of an innovative team that will architect, design and implement data analytics solutions to further advance the organization’s strategic goals. In collaboration with Business and IT partners, you will support our efforts in re-engineering, optimizing and advancing the organization’s data platform with modern cloud-based analytics technologies that will address near-term and future business needs. Be part of our vision to advance data-driven insights and decision making for our mission driven organization. Essential Functions: Collaborate with delivery team and engage with organizational stakeholders (Business and IT) to design, develop and deliver end-to-end enterprise data analytics solutions to enable data-driven insights and decision making. Translate business requirements to technical solutions by applying technical knowledge and strong business acumen. Solve business problems and complex data requirements/challenges by incorporating standards and best practices into engineering solutions, and leveraging modern data science programming languages (e.g., SQL, Python, R, Scala, SAS) and Azure data and analytics services. Develop and implement database designs (logical and physical) and data models (normalized and dimensional) to support the new analytics platform. Design, implement and maintain data ingestion/integration and end-to-end data pipeline processes using Azure technologies for a wide variety of traditional and non-traditional data sources/formats (structured, unstructured, and semi-structured) through various protocols (e.g., REST, SOAP, SFTP, MQ, etc). Technical Must Haves for this Role: 5+ years of experience in Information Technology within a medium to large enterprise with complex business and IT environment. 3+ years of experience as a Data Engineer working with cross-functional teams (within IT and/or Business) on enterprise level business intelligence/data analytics implementations using Azure cloud-based analytics platforms/technologies (including hands-on experience with Microsoft/Azure stack, e.g., Synapse, Data Factory, Data Bricks, Data Lake, Data Catalog, SSIS, SQL, etc., and NoSQL databases). 3+ years of hands-on experience in designing, implementing and maintaining data ingestion /integration and end-to-end data pipeline using Azure technologies for a wide variety of traditional and non-traditional data sources/formats (structured, unstructured, and semi-structured) through various protocols (e.g., REST, SOAP, SFTP, MQ, etc.). 2+ years of experience working with and developing database designs (logical and physical) and data models (normalized and dimensional) for data warehouse, data marts and operational data stores. 2+ years of hands-on experience working with data science programming languages (e.g., SQL, Python, R, Scala, SAS). Experience working in an agile delivery environment with working knowledge of continuous integration/continuous delivery (CI/CD) and DevOps practices.  The targeted hourly range is $36.55 - $54.83. Final offer will be determined based on experience, education, training/certifications and specialized skills.  We perform thorough background checks and credit checks. EOE. Must reside in the state of CA, AZ, NV or TX. Are you looking to join a dynamic, fast-paced team environment with a culture of collaboration and belonging? If so, let’s talk. Orange County's Credit Union is now seeking a talented and driven individual to accelerate our efforts and be a major part of our team and culture. Our team members are grounded in core values, have a strong capacity to learn, the energy to get things done, and bring real world experiences to help us think in new ways. Orange County's Credit Union actively invest in our team members to support their long-term growth so they can continue to advance our mission and achieve their highest potential. Are you passionate about the future? Are you a data engineer who specializes in wrangling a wide range of data into versatile, accessible sources of knowledge? If yes, PLEASE APPLY IMMEDATELY! More about Orange County's Credit Union: Workplace Excellence. Through our associates' opinions and voices, Orange County's Credit Union is proud to be recognized year over year as one of the best places to work in Orange County and is a recipient of the Peter Barron Stark Best of the Best Award for highest associate satisfaction in the workplace. As a leading financial service provider with over 80 years of experience serving 117,000+ members, Orange County's Credit Union is currently over $2 billion in assets & growing. Generous benefits include paid health insurance, time-off benefits, 401(k), and a professional, friendly work environment (with remote and hybrid options) focused on achieving goals, recognizing successes and excelling at member service. Putting People First: Connect, Discover, Deliver & Wow is Orange County’s Credit Union mantra. If you’re passionate about serving people, this role is rewarding, brings purpose, and the opportunity to make a difference! Overview: Are you our next Data Engineer? With your strong understanding of financial services operations (e.g., banking, asset management, insurance, mortgage, consumer & business lending) you will be part of an innovative team that will architect, design and implement data analytics solutions to further advance the organization’s strategic goals. In collaboration with Business and IT partners, you will support our efforts in re-engineering, optimizing and advancing the organization’s data platform with modern cloud-based analytics technologies that will address near-term and future business needs. Be part of our vision to advance data-driven insights and decision making for our mission driven organization. Essential Functions: Collaborate with delivery team and engage with organizational stakeholders (Business and IT) to design, develop and deliver end-to-end enterprise data analytics solutions to enable data-driven insights and decision making. Translate business requirements to technical solutions by applying technical knowledge and strong business acumen. Solve business problems and complex data requirements/challenges by incorporating standards and best practices into engineering solutions, and leveraging modern data science programming languages (e.g., SQL, Python, R, Scala, SAS) and Azure data and analytics services. Develop and implement database designs (logical and physical) and data models (normalized and dimensional) to support the new analytics platform. Design, implement and maintain data ingestion/integration and end-to-end data pipeline processes using Azure technologies for a wide variety of traditional and non-traditional data sources/formats (structured, unstructured, and semi-structured) through various protocols (e.g., REST, SOAP, SFTP, MQ, etc). Technical Must Haves for this Role: 5+ years of experience in Information Technology within a medium to large enterprise with complex business and IT environment. 3+ years of experience as a Data Engineer working with cross-functional teams (within IT and/or Business) on enterprise level business intelligence/data analytics implementations using Azure cloud-based analytics platforms/technologies (including hands-on experience with Microsoft/Azure stack, e.g., Synapse, Data Factory, Data Bricks, Data Lake, Data Catalog, SSIS, SQL, etc., and NoSQL databases). 3+ years of hands-on experience in designing, implementing and maintaining data ingestion /integration and end-to-end data pipeline using Azure technologies for a wide variety of traditional and non-traditional data sources/formats (structured, unstructured, and semi-structured) through various protocols (e.g., REST, SOAP, SFTP, MQ, etc.). 2+ years of experience working with and developing database designs (logical and physical) and data models (normalized and dimensional) for data warehouse, data marts and operational data stores. 2+ years of hands-on experience working with data science programming languages (e.g., SQL, Python, R, Scala, SAS). Experience working in an agile delivery environment with working knowledge of continuous integration/continuous delivery (CI/CD) and DevOps practices.  The targeted hourly range is $36.55 - $54.83. Final offer will be determined based on experience, education, training/certifications and specialized skills.  We perform thorough background checks and credit checks. EOE. Must reside in the state of CA, AZ, NV or TX. Are you looking to join a dynamic, fast-paced team environment with a culture of collaboration and belonging? If so, let’s talk. Orange County's Credit Union is now seeking a talented and driven individual to accelerate our efforts and be a major part of our team and culture. Our team members are grounded in core values, have a strong capacity to learn, the energy to get things done, and bring real world experiences to help us think in new ways. Orange County's Credit Union actively invest in our team members to support their long-term growth so they can continue to advance our mission and achieve their highest potential. Are you passionate about the future? Are you a data engineer who specializes in wrangling a wide range of data into versatile, accessible sources of knowledge? If yes, PLEASE APPLY IMMEDATELY! More about Orange County's Credit Union: Workplace Excellence. Through our associates' opinions and voices, Orange County's Credit Union is proud to be recognized year over year as one of the best places to work in Orange County and is a recipient of the Peter Barron Stark Best of the Best Award for highest associate satisfaction in the workplace. As a leading financial service provider with over 80 years of experience serving 117,000+ members, Orange County's Credit Union is currently over $2 billion in assets & growing. Generous benefits include paid health insurance, time-off benefits, 401(k), and a professional, friendly work environment (with remote and hybrid options) focused on achieving goals, recognizing successes and excelling at member service. Putting People First: Connect, Discover, Deliver & Wow is Orange County’s Credit Union mantra. If you’re passionate about serving people, this role is rewarding, brings purpose, and the opportunity to make a difference! Overview: Are you our next Data Engineer? With your strong understanding of financial services operations (e.g., banking, asset management, insurance, mortgage, consumer & business lending) you will be part of an innovative team that will architect, design and implement data analytics solutions to further advance the organization’s strategic goals. In collaboration with Business and IT partners, you will support our efforts in re-engineering, optimizing and advancing the organization’s data platform with modern cloud-based analytics technologies that will address near-term and future business needs. Be part of our vision to advance data-driven insights and decision making for our mission driven organization. Essential Functions: Collaborate with delivery team and engage with organizational stakeholders (Business and IT) to design, develop and deliver end-to-end enterprise data analytics solutions to enable data-driven insights and decision making. Translate business requirements to technical solutions by applying technical knowledge and strong business acumen. Solve business problems and complex data requirements/challenges by incorporating standards and best practices into engineering solutions, and leveraging modern data science programming languages (e.g., SQL, Python, R, Scala, SAS) and Azure data and analytics services. Develop and implement database designs (logical and physical) and data models (normalized and dimensional) to support the new analytics platform. Design, implement and maintain data ingestion/integration and end-to-end data pipeline processes using Azure technologies for a wide variety of traditional and non-traditional data sources/formats (structured, unstructured, and semi-structured) through various protocols (e.g., REST, SOAP, SFTP, MQ, etc). Technical Must Haves for this Role: 5+ years of experience in Information Technology within a medium to large enterprise with complex business and IT environment. 3+ years of experience as a Data Engineer working with cross-functional teams (within IT and/or Business) on enterprise level business intelligence/data analytics implementations using Azure cloud-based analytics platforms/technologies (including hands-on experience with Microsoft/Azure stack, e.g., Synapse, Data Factory, Data Bricks, Data Lake, Data Catalog, SSIS, SQL, etc., and NoSQL databases). 3+ years of hands-on experience in designing, implementing and maintaining data ingestion /integration and end-to-end data pipeline using Azure technologies for a wide variety of traditional and non-traditional data sources/formats (structured, unstructured, and semi-structured) through various protocols (e.g., REST, SOAP, SFTP, MQ, etc.). 2+ years of experience working with and developing database designs (logical and physical) and data models (normalized and dimensional) for data warehouse, data marts and operational data stores. 2+ years of hands-on experience working with data science programming languages (e.g., SQL, Python, R, Scala, SAS). Experience working in an agile delivery environment with working knowledge of continuous integration/continuous delivery (CI/CD) and DevOps practices.  The targeted hourly range is $36.55 - $54.83. Final offer will be determined based on experience, education, training/certifications and specialized skills.  We perform thorough background checks and credit checks. EOE. Must reside in the state of CA, AZ, NV or TX. Are you looking to join a dynamic, fast-paced team environment with a culture of collaboration and belonging? If so, let’s talk. Orange County's Credit Union is now seeking a talented and driven individual to accelerate our efforts and be a major part of our team and culture. Our team members are grounded in core values, have a strong capacity to learn, the energy to get things done, and bring real world experiences to help us think in new ways. Orange County's Credit Union actively invest in our team members to support their long-term growth so they can continue to advance our mission and achieve their highest potential. Are you passionate about the future? Are you a data engineer who specializes in wrangling a wide range of data into versatile, accessible sources of knowledge? If yes, PLEASE APPLY IMMEDATELY! More about Orange County's Credit Union: Workplace Excellence. Through our associates' opinions and voices, Orange County's Credit Union is proud to be recognized year over year as one of the best places to work in Orange County and is a recipient of the Peter Barron Stark Best of the Best Award for highest associate satisfaction in the workplace. As a leading financial service provider with over 80 years of experience serving 117,000+ members, Orange County's Credit Union is currently over $2 billion in assets & growing. Generous benefits include paid health insurance, time-off benefits, 401(k), and a professional, friendly work environment (with remote and hybrid options) focused on achieving goals, recognizing successes and excelling at member service. Putting People First: Connect, Discover, Deliver & Wow is Orange County’s Credit Union mantra. If you’re passionate about serving people, this role is rewarding, brings purpose, and the opportunity to make a difference! Overview: Are you our next Data Engineer? With your strong understanding of financial services operations (e.g., banking, asset management, insurance, mortgage, consumer & business lending) you will be part of an innovative team that will architect, design and implement data analytics solutions to further advance the organization’s strategic goals. In collaboration with Business and IT partners, you will support our efforts in re-engineering, optimizing and advancing the organization’s data platform with modern cloud-based analytics technologies that will address near-term and future business needs. Be part of our vision to advance data-driven insights and decision making for our mission driven organization. Essential Functions: Collaborate with delivery team and engage with organizational stakeholders (Business and IT) to design, develop and deliver end-to-end enterprise data analytics solutions to enable data-driven insights and decision making. Translate business requirements to technical solutions by applying technical knowledge and strong business acumen. Solve business problems and complex data requirements/challenges by incorporating standards and best practices into engineering solutions, and leveraging modern data science programming languages (e.g., SQL, Python, R, Scala, SAS) and Azure data and analytics services. Develop and implement database designs (logical and physical) and data models (normalized and dimensional) to support the new analytics platform. Design, implement and maintain data ingestion/integration and end-to-end data pipeline processes using Azure technologies for a wide variety of traditional and non-traditional data sources/formats (structured, unstructured, and semi-structured) through various protocols (e.g., REST, SOAP, SFTP, MQ, etc). Technical Must Haves for this Role: 5+ years of experience in Information Technology within a medium to large enterprise with complex business and IT environment. 3+ years of experience as a Data Engineer working with cross-functional teams (within IT and/or Business) on enterprise level business intelligence/data analytics implementations using Azure cloud-based analytics platforms/technologies (including hands-on experience with Microsoft/Azure stack, e.g., Synapse, Data Factory, Data Bricks, Data Lake, Data Catalog, SSIS, SQL, etc., and NoSQL databases). 3+ years of hands-on experience in designing, implementing and maintaining data ingestion /integration and end-to-end data pipeline using Azure technologies for a wide variety of traditional and non-traditional data sources/formats (structured, unstructured, and semi-structured) through various protocols (e.g., REST, SOAP, SFTP, MQ, etc.). 2+ years of experience working with and developing database designs (logical and physical) and data models (normalized and dimensional) for data warehouse, data marts and operational data stores. 2+ years of hands-on experience working with data science programming languages (e.g., SQL, Python, R, Scala, SAS). Experience working in an agile delivery environment with working knowledge of continuous integration/continuous delivery (CI/CD) and DevOps practices.  The targeted hourly range is $36.55 - $54.83. Final offer will be determined based on experience, education, training/certifications and specialized skills.  We perform thorough background checks and credit checks. EOE. Must reside in the state of CA, AZ, NV or TX. Are you looking to join a dynamic, fast-paced team environment with a culture of collaboration and belonging? If so, let’s talk. Orange County's Credit Union is now seeking a talented and driven individual to accelerate our efforts and be a major part of our team and culture. Our team members are grounded in core values, have a strong capacity to learn, the energy to get things done, and bring real world experiences to help us think in new ways. Orange County's Credit Union actively invest in our team members to support their long-term growth so they can continue to advance our mission and achieve their highest potential. Are you passionate about the future? Are you a data engineer who specializes in wrangling a wide range of data into versatile, accessible sources of knowledge? If yes, PLEASE APPLY IMMEDATELY! More about Orange County's Credit Union: Workplace Excellence. Through our associates' opinions and voices, Orange County's Credit Union is proud to be recognized year over year as one of the best places to work in Orange County and is a recipient of the Peter Barron Stark Best of the Best Award for highest associate satisfaction in the workplace. As a leading financial service provider with over 80 years of experience serving 117,000+ members, Orange County's Credit Union is currently over $2 billion in assets & growing. Generous benefits include paid health insurance, time-off benefits, 401(k), and a professional, friendly work environment (with remote and hybrid options) focused on achieving goals, recognizing successes and excelling at member service. Putting People First: Connect, Discover, Deliver & Wow is Orange County’s Credit Union mantra. If you’re passionate about serving people, this role is rewarding, brings purpose, and the opportunity to make a difference! Overview: Are you our next Data Engineer? With your strong understanding of financial services operations (e.g., banking, asset management, insurance, mortgage, consumer & business lending) you will be part of an innovative team that will architect, design and implement data analytics solutions to further advance the organization’s strategic goals. In collaboration with Business and IT partners, you will support our efforts in re-engineering, optimizing and advancing the organization’s data platform with modern cloud-based analytics technologies that will address near-term and future business needs. Be part of our vision to advance data-driven insights and decision making for our mission driven organization. Essential Functions: Collaborate with delivery team and engage with organizational stakeholders (Business and IT) to design, develop and deliver end-to-end enterprise data analytics solutions to enable data-driven insights and decision making. Translate business requirements to technical solutions by applying technical knowledge and strong business acumen. Solve business problems and complex data requirements/challenges by incorporating standards and best practices into engineering solutions, and leveraging modern data science programming languages (e.g., SQL, Python, R, Scala, SAS) and Azure data and analytics services. Develop and implement database designs (logical and physical) and data models (normalized and dimensional) to support the new analytics platform. Design, implement and maintain data ingestion/integration and end-to-end data pipeline processes using Azure technologies for a wide variety of traditional and non-traditional data sources/formats (structured, unstructured, and semi-structured) through various protocols (e.g., REST, SOAP, SFTP, MQ, etc). Technical Must Haves for this Role: 5+ years of experience in Information Technology within a medium to large enterprise with complex business and IT environment. 3+ years of experience as a Data Engineer working with cross-functional teams (within IT and/or Business) on enterprise level business intelligence/data analytics implementations using Azure cloud-based analytics platforms/technologies (including hands-on experience with Microsoft/Azure stack, e.g., Synapse, Data Factory, Data Bricks, Data Lake, Data Catalog, SSIS, SQL, etc., and NoSQL databases). 3+ years of hands-on experience in designing, implementing and maintaining data ingestion /integration and end-to-end data pipeline using Azure technologies for a wide variety of traditional and non-traditional data sources/formats (structured, unstructured, and semi-structured) through various protocols (e.g., REST, SOAP, SFTP, MQ, etc.). 2+ years of experience working with and developing database designs (logical and physical) and data models (normalized and dimensional) for data warehouse, data marts and operational data stores. 2+ years of hands-on experience working with data science programming languages (e.g., SQL, Python, R, Scala, SAS). Experience working in an agile delivery environment with working knowledge of continuous integration/continuous delivery (CI/CD) and DevOps practices.  The targeted hourly range is $36.55 - $54.83. Final offer will be determined based on experience, education, training/certifications and specialized skills.  We perform thorough background checks and credit checks. EOE. Must reside in the state of CA, AZ, NV or TX. Are you looking to join a dynamic, fast-paced team environment with a culture of collaboration and belonging? If so, let’s talk. Orange County's Credit Union is now seeking a talented and driven individual to accelerate our efforts and be a major part of our team and culture. Our team members are grounded in core values, have a strong capacity to learn, the energy to get things done, and bring real world experiences to help us think in new ways. Orange County's Credit Union actively invest in our team members to support their long-term growth so they can continue to advance our mission and achieve their highest potential. Are you passionate about the future? Are you a data engineer who specializes in wrangling a wide range of data into versatile, accessible sources of knowledge? If yes, PLEASE APPLY IMMEDATELY! More about Orange County's Credit Union: Workplace Excellence. Through our associates' opinions and voices, Orange County's Credit Union is proud to be recognized year over year as one of the best places to work in Orange County and is a recipient of the Peter Barron Stark Best of the Best Award for highest associate satisfaction in the workplace. As a leading financial service provider with over 80 years of experience serving 117,000+ members, Orange County's Credit Union is currently over $2 billion in assets & growing. Generous benefits include paid health insurance, time-off benefits, 401(k), and a professional, friendly work environment (with remote and hybrid options) focused on achieving goals, recognizing successes and excelling at member service. Putting People First: Connect, Discover, Deliver & Wow is Orange County’s Credit Union mantra. If you’re passionate about serving people, this role is rewarding, brings purpose, and the opportunity to make a difference! Overview: Are you our next Data Engineer? With your strong understanding of financial services operations (e.g., banking, asset management, insurance, mortgage, consumer & business lending) you will be part of an innovative team that will architect, design and implement data analytics solutions to further advance the organization’s strategic goals. In collaboration with Business and IT partners, you will support our efforts in re-engineering, optimizing and advancing the organization’s data platform with modern cloud-based analytics technologies that will address near-term and future business needs. Be part of our vision to advance data-driven insights and decision making for our mission driven organization. Essential Functions: Collaborate with delivery team and engage with organizational stakeholders (Business and IT) to design, develop and deliver end-to-end enterprise data analytics solutions to enable data-driven insights and decision making. Translate business requirements to technical solutions by applying technical knowledge and strong business acumen. Solve business problems and complex data requirements/challenges by incorporating standards and best practices into engineering solutions, and leveraging modern data science programming languages (e.g., SQL, Python, R, Scala, SAS) and Azure data and analytics services. Develop and implement database designs (logical and physical) and data models (normalized and dimensional) to support the new analytics platform. Design, implement and maintain data ingestion/integration and end-to-end data pipeline processes using Azure technologies for a wide variety of traditional and non-traditional data sources/formats (structured, unstructured, and semi-structured) through various protocols (e.g., REST, SOAP, SFTP, MQ, etc). Technical Must Haves for this Role: 5+ years of experience in Information Technology within a medium to large enterprise with complex business and IT environment. 3+ years of experience as a Data Engineer working with cross-functional teams (within IT and/or Business) on enterprise level business intelligence/data analytics implementations using Azure cloud-based analytics platforms/technologies (including hands-on experience with Microsoft/Azure stack, e.g., Synapse, Data Factory, Data Bricks, Data Lake, Data Catalog, SSIS, SQL, etc., and NoSQL databases). 3+ years of hands-on experience in designing, implementing and maintaining data ingestion /integration and end-to-end data pipeline using Azure technologies for a wide variety of traditional and non-traditional data sources/formats (structured, unstructured, and semi-structured) through various protocols (e.g., REST, SOAP, SFTP, MQ, etc.). 2+ years of experience working with and developing database designs (logical and physical) and data models (normalized and dimensional) for data warehouse, data marts and operational data stores. 2+ years of hands-on experience working with data science programming languages (e.g., SQL, Python, R, Scala, SAS). Experience working in an agile delivery environment with working knowledge of continuous integration/continuous delivery (CI/CD) and DevOps practices.  The targeted hourly range is $36.55 - $54.83. Final offer will be determined based on experience, education, training/certifications and specialized skills.  We perform thorough background checks and credit checks. EOE. Must reside in the state of CA, AZ, NV or TX. Are you looking to join a dynamic, fast-paced team environment with a culture of collaboration and belonging? If so, let’s talk. Orange County's Credit Union is now seeking a talented and driven individual to accelerate our efforts and be a major part of our team and culture. Our team members are grounded in core values, have a strong capacity to learn, the energy to get things done, and bring real world experiences to help us think in new ways. Orange County's Credit Union actively invest in our team members to support their long-term growth so they can continue to advance our mission and achieve their highest potential. Are you passionate about the future? Are you a data engineer who specializes in wrangling a wide range of data into versatile, accessible sources of knowledge? If yes, PLEASE APPLY IMMEDATELY! More about Orange County's Credit Union: Workplace Excellence. Through our associates' opinions and voices, Orange County's Credit Union is proud to be recognized year over year as one of the best places to work in Orange County and is a recipient of the Peter Barron Stark Best of the Best Award for highest associate satisfaction in the workplace. As a leading financial service provider with over 80 years of experience serving 117,000+ members, Orange County's Credit Union is currently over $2 billion in assets & growing. Generous benefits include paid health insurance, time-off benefits, 401(k), and a professional, friendly work environment (with remote and hybrid options) focused on achieving goals, recognizing successes and excelling at member service. Putting People First: Connect, Discover, Deliver & Wow is Orange County’s Credit Union mantra. If you’re passionate about serving people, this role is rewarding, brings purpose, and the opportunity to make a difference! Overview: Are you our next Data Engineer? With your strong understanding of financial services operations (e.g., banking, asset management, insurance, mortgage, consumer & business lending) you will be part of an innovative team that will architect, design and implement data analytics solutions to further advance the organization’s strategic goals. In collaboration with Business and IT partners, you will support our efforts in re-engineering, optimizing and advancing the organization’s data platform with modern cloud-based analytics technologies that will address near-term and future business needs. Be part of our vision to advance data-driven insights and decision making for our mission driven organization. Essential Functions: Collaborate with delivery team and engage with organizational stakeholders (Business and IT) to design, develop and deliver end-to-end enterprise data analytics solutions to enable data-driven insights and decision making. Translate business requirements to technical solutions by applying technical knowledge and strong business acumen. Solve business problems and complex data requirements/challenges by incorporating standards and best practices into engineering solutions, and leveraging modern data science programming languages (e.g., SQL, Python, R, Scala, SAS) and Azure data and analytics services. Develop and implement database designs (logical and physical) and data models (normalized and dimensional) to support the new analytics platform. Design, implement and maintain data ingestion/integration and end-to-end data pipeline processes using Azure technologies for a wide variety of traditional and non-traditional data sources/formats (structured, unstructured, and semi-structured) through various protocols (e.g., REST, SOAP, SFTP, MQ, etc). Technical Must Haves for this Role: 5+ years of experience in Information Technology within a medium to large enterprise with complex business and IT environment. 3+ years of experience as a Data Engineer working with cross-functional teams (within IT and/or Business) on enterprise level business intelligence/data analytics implementations using Azure cloud-based analytics platforms/technologies (including hands-on experience with Microsoft/Azure stack, e.g., Synapse, Data Factory, Data Bricks, Data Lake, Data Catalog, SSIS, SQL, etc., and NoSQL databases). 3+ years of hands-on experience in designing, implementing and maintaining data ingestion /integration and end-to-end data pipeline using Azure technologies for a wide variety of traditional and non-traditional data sources/formats (structured, unstructured, and semi-structured) through various protocols (e.g., REST, SOAP, SFTP, MQ, etc.). 2+ years of experience working with and developing database designs (logical and physical) and data models (normalized and dimensional) for data warehouse, data marts and operational data stores. 2+ years of hands-on experience working with data science programming languages (e.g., SQL, Python, R, Scala, SAS). Experience working in an agile delivery environment with working knowledge of continuous integration/continuous delivery (CI/CD) and DevOps practices.  The targeted hourly range is $36.55 - $54.83. Final offer will be determined based on experience, education, training/certifications and specialized skills.  We perform thorough background checks and credit checks. EOE. Must reside in the state of CA, AZ, NV or TX. Are you looking to join a dynamic, fast-paced team environment with a culture of collaboration and belonging? If so, let’s talk. Orange County's Credit Union is now seeking a talented and driven individual to accelerate our efforts and be a major part of our team and culture. Our team members are grounded in core values, have a strong capacity to learn, the energy to get things done, and bring real world experiences to help us think in new ways. Orange County's Credit Union actively invest in our team members to support their long-term growth so they can continue to advance our mission and achieve their highest potential. Are you passionate about the future? Are you a data engineer who specializes in wrangling a wide range of data into versatile, accessible sources of knowledge? If yes, PLEASE APPLY IMMEDATELY! More about Orange County's Credit Union: Workplace Excellence. Through our associates' opinions and voices, Orange County's Credit Union is proud to be recognized year over year as one of the best places to work in Orange County and is a recipient of the Peter Barron Stark Best of the Best Award for highest associate satisfaction in the workplace. As a leading financial service provider with over 80 years of experience serving 117,000+ members, Orange County's Credit Union is currently over $2 billion in assets & growing. Generous benefits include paid health insurance, time-off benefits, 401(k), and a professional, friendly work environment (with remote and hybrid options) focused on achieving goals, recognizing successes and excelling at member service. Putting People First: Connect, Discover, Deliver & Wow is Orange County’s Credit Union mantra. If you’re passionate about serving people, this role is rewarding, brings purpose, and the opportunity to make a difference! Overview: Are you our next Data Engineer? With your strong understanding of financial services operations (e.g., banking, asset management, insurance, mortgage, consumer & business lending) you will be part of an innovative team that will architect, design and implement data analytics solutions to further advance the organization’s strategic goals. In collaboration with Business and IT partners, you will support our efforts in re-engineering, optimizing and advancing the organization’s data platform with modern cloud-based analytics technologies that will address near-term and future business needs. Be part of our vision to advance data-driven insights and decision making for our mission driven organization. Essential Functions: Collaborate with delivery team and engage with organizational stakeholders (Business and IT) to design, develop and deliver end-to-end enterprise data analytics solutions to enable data-driven insights and decision making. Translate business requirements to technical solutions by applying technical knowledge and strong business acumen. Solve business problems and complex data requirements/challenges by incorporating standards and best practices into engineering solutions, and leveraging modern data science programming languages (e.g., SQL, Python, R, Scala, SAS) and Azure data and analytics services. Develop and implement database designs (logical and physical) and data models (normalized and dimensional) to support the new analytics platform. Design, implement and maintain data ingestion/integration and end-to-end data pipeline processes using Azure technologies for a wide variety of traditional and non-traditional data sources/formats (structured, unstructured, and semi-structured) through various protocols (e.g., REST, SOAP, SFTP, MQ, etc). Technical Must Haves for this Role: 5+ years of experience in Information Technology within a medium to large enterprise with complex business and IT environment. 3+ years of experience as a Data Engineer working with cross-functional teams (within IT and/or Business) on enterprise level business intelligence/data analytics implementations using Azure cloud-based analytics platforms/technologies (including hands-on experience with Microsoft/Azure stack, e.g., Synapse, Data Factory, Data Bricks, Data Lake, Data Catalog, SSIS, SQL, etc., and NoSQL databases). 3+ years of hands-on experience in designing, implementing and maintaining data ingestion /integration and end-to-end data pipeline using Azure technologies for a wide variety of traditional and non-traditional data sources/formats (structured, unstructured, and semi-structured) through various protocols (e.g., REST, SOAP, SFTP, MQ, etc.). 2+ years of experience working with and developing database designs (logical and physical) and data models (normalized and dimensional) for data warehouse, data marts and operational data stores. 2+ years of hands-on experience working with data science programming languages (e.g., SQL, Python, R, Scala, SAS). Experience working in an agile delivery environment with working knowledge of continuous integration/continuous delivery (CI/CD) and DevOps practices.  The targeted hourly range is $36.55 - $54.83. Final offer will be determined based on experience, education, training/certifications and specialized skills.  We perform thorough background checks and credit checks. EOE. Must reside in the state of CA, AZ, NV or TX. Are you looking to join a dynamic, fast-paced team environment with a culture of collaboration and belonging? If so, let’s talk. Orange County's Credit Union is now seeking a talented and driven individual to accelerate our efforts and be a major part of our team and culture. Our team members are grounded in core values, have a strong capacity to learn, the energy to get things done, and bring real world experiences to help us think in new ways. Orange County's Credit Union actively invest in our team members to support their long-term growth so they can continue to advance our mission and achieve their highest potential. Are you passionate about the future? Are you a data engineer who specializes in wrangling a wide range of data into versatile, accessible sources of knowledge? If yes, PLEASE APPLY IMMEDATELY! More about Orange County's Credit Union: Workplace Excellence. Through our associates' opinions and voices, Orange County's Credit Union is proud to be recognized year over year as one of the best places to work in Orange County and is a recipient of the Peter Barron Stark Best of the Best Award for highest associate satisfaction in the workplace. As a leading financial service provider with over 80 years of experience serving 117,000+ members, Orange County's Credit Union is currently over $2 billion in assets & growing. Generous benefits include paid health insurance, time-off benefits, 401(k), and a professional, friendly work environment (with remote and hybrid options) focused on achieving goals, recognizing successes and excelling at member service. Putting People First: Connect, Discover, Deliver & Wow is Orange County’s Credit Union mantra. If you’re passionate about serving people, this role is rewarding, brings purpose, and the opportunity to make a difference! Overview: Are you our next Data Engineer? With your strong understanding of financial services operations (e.g., banking, asset management, insurance, mortgage, consumer & business lending) you will be part of an innovative team that will architect, design and implement data analytics solutions to further advance the organization’s strategic goals. In collaboration with Business and IT partners, you will support our efforts in re-engineering, optimizing and advancing the organization’s data platform with modern cloud-based analytics technologies that will address near-term and future business needs. Be part of our vision to advance data-driven insights and decision making for our mission driven organization. Essential Functions: Collaborate with delivery team and engage with organizational stakeholders (Business and IT) to design, develop and deliver end-to-end enterprise data analytics solutions to enable data-driven insights and decision making. Translate business requirements to technical solutions by applying technical knowledge and strong business acumen. Solve business problems and complex data requirements/challenges by incorporating standards and best practices into engineering solutions, and leveraging modern data science programming languages (e.g., SQL, Python, R, Scala, SAS) and Azure data and analytics services. Develop and implement database designs (logical and physical) and data models (normalized and dimensional) to support the new analytics platform. Design, implement and maintain data ingestion/integration and end-to-end data pipeline processes using Azure technologies for a wide variety of traditional and non-traditional data sources/formats (structured, unstructured, and semi-structured) through various protocols (e.g., REST, SOAP, SFTP, MQ, etc). Technical Must Haves for this Role: 5+ years of experience in Information Technology within a medium to large enterprise with complex business and IT environment. 3+ years of experience as a Data Engineer working with cross-functional teams (within IT and/or Business) on enterprise level business intelligence/data analytics implementations using Azure cloud-based analytics platforms/technologies (including hands-on experience with Microsoft/Azure stack, e.g., Synapse, Data Factory, Data Bricks, Data Lake, Data Catalog, SSIS, SQL, etc., and NoSQL databases). 3+ years of hands-on experience in designing, implementing and maintaining data ingestion /integration and end-to-end data pipeline using Azure technologies for a wide variety of traditional and non-traditional data sources/formats (structured, unstructured, and semi-structured) through various protocols (e.g., REST, SOAP, SFTP, MQ, etc.). 2+ years of experience working with and developing database designs (logical and physical) and data models (normalized and dimensional) for data warehouse, data marts and operational data stores. 2+ years of hands-on experience working with data science programming languages (e.g., SQL, Python, R, Scala, SAS). Experience working in an agile delivery environment with working knowledge of continuous integration/continuous delivery (CI/CD) and DevOps practices.  The targeted hourly range is $36.55 - $54.83. Final offer will be determined based on experience, education, training/certifications and specialized skills.  We perform thorough background checks and credit checks. EOE. Must reside in the state of CA, AZ, NV or TX. Are you looking to join a dynamic, fast-paced team environment with a culture of collaboration and belonging? If so, let’s talk. Orange County's Credit Union is now seeking a talented and driven individual to accelerate our efforts and be a major part of our team and culture. Our team members are grounded in core values, have a strong capacity to learn, the energy to get things done, and bring real world experiences to help us think in new ways. Orange County's Credit Union actively invest in our team members to support their long-term growth so they can continue to advance our mission and achieve their highest potential. Are you passionate about the future? Are you a data engineer who specializes in wrangling a wide range of data into versatile, accessible sources of knowledge? If yes, PLEASE APPLY IMMEDATELY! More about Orange County's Credit Union: Workplace Excellence. Through our associates' opinions and voices, Orange County's Credit Union is proud to be recognized year over year as one of the best places to work in Orange County and is a recipient of the Peter Barron Stark Best of the Best Award for highest associate satisfaction in the workplace. As a leading financial service provider with over 80 years of experience serving 117,000+ members, Orange County's Credit Union is currently over $2 billion in assets & growing. Generous benefits include paid health insurance, time-off benefits, 401(k), and a professional, friendly work environment (with remote and hybrid options) focused on achieving goals, recognizing successes and excelling at member service. Putting People First: Connect, Discover, Deliver & Wow is Orange County’s Credit Union mantra. If you’re passionate about serving people, this role is rewarding, brings purpose, and the opportunity to make a difference! Overview: Are you our next Data Engineer? With your strong understanding of financial services operations (e.g., banking, asset management, insurance, mortgage, consumer & business lending) you will be part of an innovative team that will architect, design and implement data analytics solutions to further advance the organization’s strategic goals. In collaboration with Business and IT partners, you will support our efforts in re-engineering, optimizing and advancing the organization’s data platform with modern cloud-based analytics technologies that will address near-term and future business needs. Be part of our vision to advance data-driven insights and decision making for our mission driven organization. Essential Functions: Collaborate with delivery team and engage with organizational stakeholders (Business and IT) to design, develop and deliver end-to-end enterprise data analytics solutions to enable data-driven insights and decision making. Translate business requirements to technical solutions by applying technical knowledge and strong business acumen. Solve business problems and complex data requirements/challenges by incorporating standards and best practices into engineering solutions, and leveraging modern data science programming languages (e.g., SQL, Python, R, Scala, SAS) and Azure data and analytics services. Develop and implement database designs (logical and physical) and data models (normalized and dimensional) to support the new analytics platform. Design, implement and maintain data ingestion/integration and end-to-end data pipeline processes using Azure technologies for a wide variety of traditional and non-traditional data sources/formats (structured, unstructured, and semi-structured) through various protocols (e.g., REST, SOAP, SFTP, MQ, etc). Technical Must Haves for this Role: 5+ years of experience in Information Technology within a medium to large enterprise with complex business and IT environment. 3+ years of experience as a Data Engineer working with cross-functional teams (within IT and/or Business) on enterprise level business intelligence/data analytics implementations using Azure cloud-based analytics platforms/technologies (including hands-on experience with Microsoft/Azure stack, e.g., Synapse, Data Factory, Data Bricks, Data Lake, Data Catalog, SSIS, SQL, etc., and NoSQL databases). 3+ years of hands-on experience in designing, implementing and maintaining data ingestion /integration and end-to-end data pipeline using Azure technologies for a wide variety of traditional and non-traditional data sources/formats (structured, unstructured, and semi-structured) through various protocols (e.g., REST, SOAP, SFTP, MQ, etc.). 2+ years of experience working with and developing database designs (logical and physical) and data models (normalized and dimensional) for data warehouse, data marts and operational data stores. 2+ years of hands-on experience working with data science programming languages (e.g., SQL, Python, R, Scala, SAS). Experience working in an agile delivery environment with working knowledge of continuous integration/continuous delivery (CI/CD) and DevOps practices.  The targeted hourly range is $36.55 - $54.83. Final offer will be determined based on experience, education, training/certifications and specialized skills.  We perform thorough background checks and credit checks. EOE. Must reside in the state of CA, AZ, NV or TX. Are you looking to join a dynamic, fast-paced team environment with a culture of collaboration and belonging? If so, let’s talk. Orange County's Credit Union is now seeking a talented and driven individual to accelerate our efforts and be a major part of our team and culture. Our team members are grounded in core values, have a strong capacity to learn, the energy to get things done, and bring real world experiences to help us think in new ways. Orange County's Credit Union actively invest in our team members to support their long-term growth so they can continue to advance our mission and achieve their highest potential. Are you passionate about the future? Are you a data engineer who specializes in wrangling a wide range of data into versatile, accessible sources of knowledge? If yes, PLEASE APPLY IMMEDATELY! More about Orange County's Credit Union: Workplace Excellence. Through our associates' opinions and voices, Orange County's Credit Union is proud to be recognized year over year as one of the best places to work in Orange County and is a recipient of the Peter Barron Stark Best of the Best Award for highest associate satisfaction in the workplace. As a leading financial service provider with over 80 years of experience serving 117,000+ members, Orange County's Credit Union is currently over $2 billion in assets & growing. Generous benefits include paid health insurance, time-off benefits, 401(k), and a professional, friendly work environment (with remote and hybrid options) focused on achieving goals, recognizing successes and excelling at member service. Putting People First: Connect, Discover, Deliver & Wow is Orange County’s Credit Union mantra. If you’re passionate about serving people, this role is rewarding, brings purpose, and the opportunity to make a difference! Overview: Are you our next Data Engineer? With your strong understanding of financial services operations (e.g., banking, asset management, insurance, mortgage, consumer & business lending) you will be part of an innovative team that will architect, design and implement data analytics solutions to further advance the organization’s strategic goals. In collaboration with Business and IT partners, you will support our efforts in re-engineering, optimizing and advancing the organization’s data platform with modern cloud-based analytics technologies that will address near-term and future business needs. Be part of our vision to advance data-driven insights and decision making for our mission driven organization. Essential Functions: Collaborate with delivery team and engage with organizational stakeholders (Business and IT) to design, develop and deliver end-to-end enterprise data analytics solutions to enable data-driven insights and decision making. Translate business requirements to technical solutions by applying technical knowledge and strong business acumen. Solve business problems and complex data requirements/challenges by incorporating standards and best practices into engineering solutions, and leveraging modern data science programming languages (e.g., SQL, Python, R, Scala, SAS) and Azure data and analytics services. Develop and implement database designs (logical and physical) and data models (normalized and dimensional) to support the new analytics platform. Design, implement and maintain data ingestion/integration and end-to-end data pipeline processes using Azure technologies for a wide variety of traditional and non-traditional data sources/formats (structured, unstructured, and semi-structured) through various protocols (e.g., REST, SOAP, SFTP, MQ, etc). Technical Must Haves for this Role: 5+ years of experience in Information Technology within a medium to large enterprise with complex business and IT environment. 3+ years of experience as a Data Engineer working with cross-functional teams (within IT and/or Business) on enterprise level business intelligence/data analytics implementations using Azure cloud-based analytics platforms/technologies (including hands-on experience with Microsoft/Azure stack, e.g., Synapse, Data Factory, Data Bricks, Data Lake, Data Catalog, SSIS, SQL, etc., and NoSQL databases). 3+ years of hands-on experience in designing, implementing and maintaining data ingestion /integration and end-to-end data pipeline using Azure technologies for a wide variety of traditional and non-traditional data sources/formats (structured, unstructured, and semi-structured) through various protocols (e.g., REST, SOAP, SFTP, MQ, etc.). 2+ years of experience working with and developing database designs (logical and physical) and data models (normalized and dimensional) for data warehouse, data marts and operational data stores. 2+ years of hands-on experience working with data science programming languages (e.g., SQL, Python, R, Scala, SAS). Experience working in an agile delivery environment with working knowledge of continuous integration/continuous delivery (CI/CD) and DevOps practices.  The targeted hourly range is $36.55 - $54.83. Final offer will be determined based on experience, education, training/certifications and specialized skills.  We perform thorough background checks and credit checks. EOE. Must reside in the state of CA, AZ, NV or TX. Are you looking to join a dynamic, fast-paced team environment with a culture of collaboration and belonging? If so, let’s talk. Orange County's Credit Union is now seeking a talented and driven individual to accelerate our efforts and be a major part of our team and culture. Our team members are grounded in core values, have a strong capacity to learn, the energy to get things done, and bring real world experiences to help us think in new ways. Orange County's Credit Union actively invest in our team members to support their long-term growth so they can continue to advance our mission and achieve their highest potential. Are you passionate about the future? Are you a data engineer who specializes in wrangling a wide range of data into versatile, accessible sources of knowledge? If yes, PLEASE APPLY IMMEDATELY! More about Orange County's Credit Union: Workplace Excellence. Through our associates' opinions and voices, Orange County's Credit Union is proud to be recognized year over year as one of the best places to work in Orange County and is a recipient of the Peter Barron Stark Best of the Best Award for highest associate satisfaction in the workplace. As a leading financial service provider with over 80 years of experience serving 117,000+ members, Orange County's Credit Union is currently over $2 billion in assets & growing. Generous benefits include paid health insurance, time-off benefits, 401(k), and a professional, friendly work environment (with remote and hybrid options) focused on achieving goals, recognizing successes and excelling at member service. Putting People First: Connect, Discover, Deliver & Wow is Orange County’s Credit Union mantra. If you’re passionate about serving people, this role is rewarding, brings purpose, and the opportunity to make a difference! Overview: Are you our next Data Engineer? With your strong understanding of financial services operations (e.g., banking, asset management, insurance, mortgage, consumer & business lending) you will be part of an innovative team that will architect, design and implement data analytics solutions to further advance the organization’s strategic goals. In collaboration with Business and IT partners, you will support our efforts in re-engineering, optimizing and advancing the organization’s data platform with modern cloud-based analytics technologies that will address near-term and future business needs. Be part of our vision to advance data-driven insights and decision making for our mission driven organization. Essential Functions: Collaborate with delivery team and engage with organizational stakeholders (Business and IT) to design, develop and deliver end-to-end enterprise data analytics solutions to enable data-driven insights and decision making. Translate business requirements to technical solutions by applying technical knowledge and strong business acumen. Solve business problems and complex data requirements/challenges by incorporating standards and best practices into engineering solutions, and leveraging modern data science programming languages (e.g., SQL, Python, R, Scala, SAS) and Azure data and analytics services. Develop and implement database designs (logical and physical) and data models (normalized and dimensional) to support the new analytics platform. Design, implement and maintain data ingestion/integration and end-to-end data pipeline processes using Azure technologies for a wide variety of traditional and non-traditional data sources/formats (structured, unstructured, and semi-structured) through various protocols (e.g., REST, SOAP, SFTP, MQ, etc). Technical Must Haves for this Role: 5+ years of experience in Information Technology within a medium to large enterprise with complex business and IT environment. 3+ years of experience as a Data Engineer working with cross-functional teams (within IT and/or Business) on enterprise level business intelligence/data analytics implementations using Azure cloud-based analytics platforms/technologies (including hands-on experience with Microsoft/Azure stack, e.g., Synapse, Data Factory, Data Bricks, Data Lake, Data Catalog, SSIS, SQL, etc., and NoSQL databases). 3+ years of hands-on experience in designing, implementing and maintaining data ingestion /integration and end-to-end data pipeline using Azure technologies for a wide variety of traditional and non-traditional data sources/formats (structured, unstructured, and semi-structured) through various protocols (e.g., REST, SOAP, SFTP, MQ, etc.). 2+ years of experience working with and developing database designs (logical and physical) and data models (normalized and dimensional) for data warehouse, data marts and operational data stores. 2+ years of hands-on experience working with data science programming languages (e.g., SQL, Python, R, Scala, SAS). Experience working in an agile delivery environment with working knowledge of continuous integration/continuous delivery (CI/CD) and DevOps practices.  The targeted hourly range is $36.55 - $54.83. Final offer will be determined based on experience, education, training/certifications and specialized skills.  We perform thorough background checks and credit checks. EOE. Must reside in the state of CA, AZ, NV or TX. Are you looking to join a dynamic, fast-paced team environment with a culture of collaboration and belonging? If so, let’s talk. Orange County's Credit Union is now seeking a talented and driven individual to accelerate our efforts and be a major part of our team and culture. Our team members are grounded in core values, have a strong capacity to learn, the energy to get things done, and bring real world experiences to help us think in new ways. Orange County's Credit Union actively invest in our team members to support their long-term growth so they can continue to advance our mission and achieve their highest potential. Are you passionate about the future? Are you a data engineer who specializes in wrangling a wide range of data into versatile, accessible sources of knowledge? If yes, PLEASE APPLY IMMEDATELY! More about Orange County's Credit Union: Workplace Excellence. Through our associates' opinions and voices, Orange County's Credit Union is proud to be recognized year over year as one of the best places to work in Orange County and is a recipient of the Peter Barron Stark Best of the Best Award for highest associate satisfaction in the workplace. As a leading financial service provider with over 80 years of experience serving 117,000+ members, Orange County's Credit Union is currently over $2 billion in assets & growing. Generous benefits include paid health insurance, time-off benefits, 401(k), and a professional, friendly work environment (with remote and hybrid options) focused on achieving goals, recognizing successes and excelling at member service. Putting People First: Connect, Discover, Deliver & Wow is Orange County’s Credit Union mantra. If you’re passionate about serving people, this role is rewarding, brings purpose, and the opportunity to make a difference! Overview: Are you our next Data Engineer? With your strong understanding of financial services operations (e.g., banking, asset management, insurance, mortgage, consumer & business lending) you will be part of an innovative team that will architect, design and implement data analytics solutions to further advance the organization’s strategic goals. In collaboration with Business and IT partners, you will support our efforts in re-engineering, optimizing and advancing the organization’s data platform with modern cloud-based analytics technologies that will address near-term and future business needs. Be part of our vision to advance data-driven insights and decision making for our mission driven organization. Essential Functions: Collaborate with delivery team and engage with organizational stakeholders (Business and IT) to design, develop and deliver end-to-end enterprise data analytics solutions to enable data-driven insights and decision making. Translate business requirements to technical solutions by applying technical knowledge and strong business acumen. Solve business problems and complex data requirements/challenges by incorporating standards and best practices into engineering solutions, and leveraging modern data science programming languages (e.g., SQL, Python, R, Scala, SAS) and Azure data and analytics services. Develop and implement database designs (logical and physical) and data models (normalized and dimensional) to support the new analytics platform. Design, implement and maintain data ingestion/integration and end-to-end data pipeline processes using Azure technologies for a wide variety of traditional and non-traditional data sources/formats (structured, unstructured, and semi-structured) through various protocols (e.g., REST, SOAP, SFTP, MQ, etc). Technical Must Haves for this Role: 5+ years of experience in Information Technology within a medium to large enterprise with complex business and IT environment. 3+ years of experience as a Data Engineer working with cross-functional teams (within IT and/or Business) on enterprise level business intelligence/data analytics implementations using Azure cloud-based analytics platforms/technologies (including hands-on experience with Microsoft/Azure stack, e.g., Synapse, Data Factory, Data Bricks, Data Lake, Data Catalog, SSIS, SQL, etc., and NoSQL databases). 3+ years of hands-on experience in designing, implementing and maintaining data ingestion /integration and end-to-end data pipeline using Azure technologies for a wide variety of traditional and non-traditional data sources/formats (structured, unstructured, and semi-structured) through various protocols (e.g., REST, SOAP, SFTP, MQ, etc.). 2+ years of experience working with and developing database designs (logical and physical) and data models (normalized and dimensional) for data warehouse, data marts and operational data stores. 2+ years of hands-on experience working with data science programming languages (e.g., SQL, Python, R, Scala, SAS). Experience working in an agile delivery environment with working knowledge of continuous integration/continuous delivery (CI/CD) and DevOps practices.  The targeted hourly range is $36.55 - $54.83. Final offer will be determined based on experience, education, training/certifications and specialized skills.  We perform thorough background checks and credit checks. EOE. Must reside in the state of CA, AZ, NV or TX. Are you looking to join a dynamic, fast-paced team environment with a culture of collaboration and belonging? If so, let’s talk. Orange County's Credit Union is now seeking a talented and driven individual to accelerate our efforts and be a major part of our team and culture. Our team members are grounded in core values, have a strong capacity to learn, the energy to get things done, and bring real world experiences to help us think in new ways. Orange County's Credit Union actively invest in our team members to support their long-term growth so they can continue to advance our mission and achieve their highest potential. Are you passionate about the future? Are you a data engineer who specializes in wrangling a wide range of data into versatile, accessible sources of knowledge? If yes, PLEASE APPLY IMMEDATELY! More about Orange County's Credit Union: Workplace Excellence. Through our associates' opinions and voices, Orange County's Credit Union is proud to be recognized year over year as one of the best places to work in Orange County and is a recipient of the Peter Barron Stark Best of the Best Award for highest associate satisfaction in the workplace. As a leading financial service provider with over 80 years of experience serving 117,000+ members, Orange County's Credit Union is currently over $2 billion in assets & growing. Generous benefits include paid health insurance, time-off benefits, 401(k), and a professional, friendly work environment (with remote and hybrid options) focused on achieving goals, recognizing successes and excelling at member service. Putting People First: Connect, Discover, Deliver & Wow is Orange County’s Credit Union mantra. If you’re passionate about serving people, this role is rewarding, brings purpose, and the opportunity to make a difference! Overview: Are you our next Data Engineer? With your strong understanding of financial services operations (e.g., banking, asset management, insurance, mortgage, consumer & business lending) you will be part of an innovative team that will architect, design and implement data analytics solutions to further advance the organization’s strategic goals. In collaboration with Business and IT partners, you will support our efforts in re-engineering, optimizing and advancing the organization’s data platform with modern cloud-based analytics technologies that will address near-term and future business needs. Be part of our vision to advance data-driven insights and decision making for our mission driven organization. Essential Functions: Collaborate with delivery team and engage with organizational stakeholders (Business and IT) to design, develop and deliver end-to-end enterprise data analytics solutions to enable data-driven insights and decision making. Translate business requirements to technical solutions by applying technical knowledge and strong business acumen. Solve business problems and complex data requirements/challenges by incorporating standards and best practices into engineering solutions, and leveraging modern data science programming languages (e.g., SQL, Python, R, Scala, SAS) and Azure data and analytics services. Develop and implement database designs (logical and physical) and data models (normalized and dimensional) to support the new analytics platform. Design, implement and maintain data ingestion/integration and end-to-end data pipeline processes using Azure technologies for a wide variety of traditional and non-traditional data sources/formats (structured, unstructured, and semi-structured) through various protocols (e.g., REST, SOAP, SFTP, MQ, etc). Technical Must Haves for this Role: 5+ years of experience in Information Technology within a medium to large enterprise with complex business and IT environment. 3+ years of experience as a Data Engineer working with cross-functional teams (within IT and/or Business) on enterprise level business intelligence/data analytics implementations using Azure cloud-based analytics platforms/technologies (including hands-on experience with Microsoft/Azure stack, e.g., Synapse, Data Factory, Data Bricks, Data Lake, Data Catalog, SSIS, SQL, etc., and NoSQL databases). 3+ years of hands-on experience in designing, implementing and maintaining data ingestion /integration and end-to-end data pipeline using Azure technologies for a wide variety of traditional and non-traditional data sources/formats (structured, unstructured, and semi-structured) through various protocols (e.g., REST, SOAP, SFTP, MQ, etc.). 2+ years of experience working with and developing database designs (logical and physical) and data models (normalized and dimensional) for data warehouse, data marts and operational data stores. 2+ years of hands-on experience working with data science programming languages (e.g., SQL, Python, R, Scala, SAS). Experience working in an agile delivery environment with working knowledge of continuous integration/continuous delivery (CI/CD) and DevOps practices.  The targeted hourly range is $36.55 - $54.83. Final offer will be determined based on experience, education, training/certifications and specialized skills.  We perform thorough background checks and credit checks. EOE. Must reside in the state of CA, AZ, NV or TX. Are you looking to join a dynamic, fast-paced team environment with a culture of collaboration and belonging? If so, let’s talk. Orange County's Credit Union is now seeking a talented and driven individual to accelerate our efforts and be a major part of our team and culture. Our team members are grounded in core values, have a strong capacity to learn, the energy to get things done, and bring real world experiences to help us think in new ways. Orange County's Credit Union actively invest in our team members to support their long-term growth so they can continue to advance our mission and achieve their highest potential. Are you passionate about the future? Are you a data engineer who specializes in wrangling a wide range of data into versatile, accessible sources of knowledge? If yes, PLEASE APPLY IMMEDATELY! More about Orange County's Credit Union: Workplace Excellence. Through our associates' opinions and voices, Orange County's Credit Union is proud to be recognized year over year as one of the best places to work in Orange County and is a recipient of the Peter Barron Stark Best of the Best Award for highest associate satisfaction in the workplace. As a leading financial service provider with over 80 years of experience serving 117,000+ members, Orange County's Credit Union is currently over $2 billion in assets & growing. Generous benefits include paid health insurance, time-off benefits, 401(k), and a professional, friendly work environment (with remote and hybrid options) focused on achieving goals, recognizing successes and excelling at member service. Putting People First: Connect, Discover, Deliver & Wow is Orange County’s Credit Union mantra. If you’re passionate about serving people, this role is rewarding, brings purpose, and the opportunity to make a difference! Overview: Are you our next Data Engineer? With your strong understanding of financial services operations (e.g., banking, asset management, insurance, mortgage, consumer & business lending) you will be part of an innovative team that will architect, design and implement data analytics solutions to further advance the organization’s strategic goals. In collaboration with Business and IT partners, you will support our efforts in re-engineering, optimizing and advancing the organization’s data platform with modern cloud-based analytics technologies that will address near-term and future business needs. Be part of our vision to advance data-driven insights and decision making for our mission driven organization. Essential Functions: Collaborate with delivery team and engage with organizational stakeholders (Business and IT) to design, develop and deliver end-to-end enterprise data analytics solutions to enable data-driven insights and decision making. Translate business requirements to technical solutions by applying technical knowledge and strong business acumen. Solve business problems and complex data requirements/challenges by incorporating standards and best practices into engineering solutions, and leveraging modern data science programming languages (e.g., SQL, Python, R, Scala, SAS) and Azure data and analytics services. Develop and implement database designs (logical and physical) and data models (normalized and dimensional) to support the new analytics platform. Design, implement and maintain data ingestion/integration and end-to-end data pipeline processes using Azure technologies for a wide variety of traditional and non-traditional data sources/formats (structured, unstructured, and semi-structured) through various protocols (e.g., REST, SOAP, SFTP, MQ, etc). Technical Must Haves for this Role: 5+ years of experience in Information Technology within a medium to large enterprise with complex business and IT environment. 3+ years of experience as a Data Engineer working with cross-functional teams (within IT and/or Business) on enterprise level business intelligence/data analytics implementations using Azure cloud-based analytics platforms/technologies (including hands-on experience with Microsoft/Azure stack, e.g., Synapse, Data Factory, Data Bricks, Data Lake, Data Catalog, SSIS, SQL, etc., and NoSQL databases). 3+ years of hands-on experience in designing, implementing and maintaining data ingestion /integration and end-to-end data pipeline using Azure technologies for a wide variety of traditional and non-traditional data sources/formats (structured, unstructured, and semi-structured) through various protocols (e.g., REST, SOAP, SFTP, MQ, etc.). 2+ years of experience working with and developing database designs (logical and physical) and data models (normalized and dimensional) for data warehouse, data marts and operational data stores. 2+ years of hands-on experience working with data science programming languages (e.g., SQL, Python, R, Scala, SAS). Experience working in an agile delivery environment with working knowledge of continuous integration/continuous delivery (CI/CD) and DevOps practices.  The targeted hourly range is $36.55 - $54.83. Final offer will be determined based on experience, education, training/certifications and specialized skills.  We perform thorough background checks and credit checks. EOE. Must reside in the state of CA, AZ, NV or TX. Are you looking to join a dynamic, fast-paced team environment with a culture of collaboration and belonging? If so, let’s talk. Orange County's Credit Union is now seeking a talented and driven individual to accelerate our efforts and be a major part of our team and culture. Our team members are grounded in core values, have a strong capacity to learn, the energy to get things done, and bring real world experiences to help us think in new ways. Orange County's Credit Union actively invest in our team members to support their long-term growth so they can continue to advance our mission and achieve their highest potential. Are you passionate about the future? Are you a data engineer who specializes in wrangling a wide range of data into versatile, accessible sources of knowledge? If yes, PLEASE APPLY IMMEDATELY! More about Orange County's Credit Union: Workplace Excellence. Through our associates' opinions and voices, Orange County's Credit Union is proud to be recognized year over year as one of the best places to work in Orange County and is a recipient of the Peter Barron Stark Best of the Best Award for highest associate satisfaction in the workplace. As a leading financial service provider with over 80 years of experience serving 117,000+ members, Orange County's Credit Union is currently over $2 billion in assets & growing. Generous benefits include paid health insurance, time-off benefits, 401(k), and a professional, friendly work environment (with remote and hybrid options) focused on achieving goals, recognizing successes and excelling at member service. Putting People First: Connect, Discover, Deliver & Wow is Orange County’s Credit Union mantra. If you’re passionate about serving people, this role is rewarding, brings purpose, and the opportunity to make a difference! Overview: Are you our next Data Engineer? With your strong understanding of financial services operations (e.g., banking, asset management, insurance, mortgage, consumer & business lending) you will be part of an innovative team that will architect, design and implement data analytics solutions to further advance the organization’s strategic goals. In collaboration with Business and IT partners, you will support our efforts in re-engineering, optimizing and advancing the organization’s data platform with modern cloud-based analytics technologies that will address near-term and future business needs. Be part of our vision to advance data-driven insights and decision making for our mission driven organization. Essential Functions: Collaborate with delivery team and engage with organizational stakeholders (Business and IT) to design, develop and deliver end-to-end enterprise data analytics solutions to enable data-driven insights and decision making. Translate business requirements to technical solutions by applying technical knowledge and strong business acumen. Solve business problems and complex data requirements/challenges by incorporating standards and best practices into engineering solutions, and leveraging modern data science programming languages (e.g., SQL, Python, R, Scala, SAS) and Azure data and analytics services. Develop and implement database designs (logical and physical) and data models (normalized and dimensional) to support the new analytics platform. Design, implement and maintain data ingestion/integration and end-to-end data pipeline processes using Azure technologies for a wide variety of traditional and non-traditional data sources/formats (structured, unstructured, and semi-structured) through various protocols (e.g., REST, SOAP, SFTP, MQ, etc). Technical Must Haves for this Role: 5+ years of experience in Information Technology within a medium to large enterprise with complex business and IT environment. 3+ years of experience as a Data Engineer working with cross-functional teams (within IT and/or Business) on enterprise level business intelligence/data analytics implementations using Azure cloud-based analytics platforms/technologies (including hands-on experience with Microsoft/Azure stack, e.g., Synapse, Data Factory, Data Bricks, Data Lake, Data Catalog, SSIS, SQL, etc., and NoSQL databases). 3+ years of hands-on experience in designing, implementing and maintaining data ingestion /integration and end-to-end data pipeline using Azure technologies for a wide variety of traditional and non-traditional data sources/formats (structured, unstructured, and semi-structured) through various protocols (e.g., REST, SOAP, SFTP, MQ, etc.). 2+ years of experience working with and developing database designs (logical and physical) and data models (normalized and dimensional) for data warehouse, data marts and operational data stores. 2+ years of hands-on experience working with data science programming languages (e.g., SQL, Python, R, Scala, SAS). Experience working in an agile delivery environment with working knowledge of continuous integration/continuous delivery (CI/CD) and DevOps practices.  The targeted hourly range is $36.55 - $54.83. Final offer will be determined based on experience, education, training/certifications and specialized skills.  We perform thorough background checks and credit checks. EOE. Must reside in the state of CA, AZ, NV or TX. Are you looking to join a dynamic, fast-paced team environment with a culture of collaboration and belonging? If so, let’s talk. Orange County's Credit Union is now seeking a talented and driven individual to accelerate our efforts and be a major part of our team and culture. Our team members are grounded in core values, have a strong capacity to learn, the energy to get things done, and bring real world experiences to help us think in new ways. Orange County's Credit Union actively invest in our team members to support their long-term growth so they can continue to advance our mission and achieve their highest potential. Are you passionate about the future? Are you a data engineer who specializes in wrangling a wide range of data into versatile, accessible sources of knowledge? If yes, PLEASE APPLY IMMEDATELY! More about Orange County's Credit Union: Workplace Excellence. Through our associates' opinions and voices, Orange County's Credit Union is proud to be recognized year over year as one of the best places to work in Orange County and is a recipient of the Peter Barron Stark Best of the Best Award for highest associate satisfaction in the workplace. As a leading financial service provider with over 80 years of experience serving 117,000+ members, Orange County's Credit Union is currently over $2 billion in assets & growing. Generous benefits include paid health insurance, time-off benefits, 401(k), and a professional, friendly work environment (with remote and hybrid options) focused on achieving goals, recognizing successes and excelling at member service. Putting People First: Connect, Discover, Deliver & Wow is Orange County’s Credit Union mantra. If you’re passionate about serving people, this role is rewarding, brings purpose, and the opportunity to make a difference! Overview: Are you our next Data Engineer? With your strong understanding of financial services operations (e.g., banking, asset management, insurance, mortgage, consumer & business lending) you will be part of an innovative team that will architect, design and implement data analytics solutions to further advance the organization’s strategic goals. In collaboration with Business and IT partners, you will support our efforts in re-engineering, optimizing and advancing the organization’s data platform with modern cloud-based analytics technologies that will address near-term and future business needs. Be part of our vision to advance data-driven insights and decision making for our mission driven organization. Essential Functions: Collaborate with delivery team and engage with organizational stakeholders (Business and IT) to design, develop and deliver end-to-end enterprise data analytics solutions to enable data-driven insights and decision making. Translate business requirements to technical solutions by applying technical knowledge and strong business acumen. Solve business problems and complex data requirements/challenges by incorporating standards and best practices into engineering solutions, and leveraging modern data science programming languages (e.g., SQL, Python, R, Scala, SAS) and Azure data and analytics services. Develop and implement database designs (logical and physical) and data models (normalized and dimensional) to support the new analytics platform. Design, implement and maintain data ingestion/integration and end-to-end data pipeline processes using Azure technologies for a wide variety of traditional and non-traditional data sources/formats (structured, unstructured, and semi-structured) through various protocols (e.g., REST, SOAP, SFTP, MQ, etc). Technical Must Haves for this Role: 5+ years of experience in Information Technology within a medium to large enterprise with complex business and IT environment. 3+ years of experience as a Data Engineer working with cross-functional teams (within IT and/or Business) on enterprise level business intelligence/data analytics implementations using Azure cloud-based analytics platforms/technologies (including hands-on experience with Microsoft/Azure stack, e.g., Synapse, Data Factory, Data Bricks, Data Lake, Data Catalog, SSIS, SQL, etc., and NoSQL databases). 3+ years of hands-on experience in designing, implementing and maintaining data ingestion /integration and end-to-end data pipeline using Azure technologies for a wide variety of traditional and non-traditional data sources/formats (structured, unstructured, and semi-structured) through various protocols (e.g., REST, SOAP, SFTP, MQ, etc.). 2+ years of experience working with and developing database designs (logical and physical) and data models (normalized and dimensional) for data warehouse, data marts and operational data stores. 2+ years of hands-on experience working with data science programming languages (e.g., SQL, Python, R, Scala, SAS). Experience working in an agile delivery environment with working knowledge of continuous integration/continuous delivery (CI/CD) and DevOps practices.  The targeted hourly range is $36.55 - $54.83. Final offer will be determined based on experience, education, training/certifications and specialized skills.  We perform thorough background checks and credit checks. EOE. Must reside in the state of CA, AZ, NV or TX. Are you looking to join a dynamic, fast-paced team environment with a culture of collaboration and belonging? If so, let’s talk. Orange County's Credit Union is now seeking a talented and driven individual to accelerate our efforts and be a major part of our team and culture. Our team members are grounded in core values, have a strong capacity to learn, the energy to get things done, and bring real world experiences to help us think in new ways. Orange County's Credit Union actively invest in our team members to support their long-term growth so they can continue to advance our mission and achieve their highest potential. Are you passionate about the future? Are you a data engineer who specializes in wrangling a wide range of data into versatile, accessible sources of knowledge? If yes, PLEASE APPLY IMMEDATELY! More about Orange County's Credit Union: Workplace Excellence. Through our associates' opinions and voices, Orange County's Credit Union is proud to be recognized year over year as one of the best places to work in Orange County and is a recipient of the Peter Barron Stark Best of the Best Award for highest associate satisfaction in the workplace. As a leading financial service provider with over 80 years of experience serving 117,000+ members, Orange County's Credit Union is currently over $2 billion in assets & growing. Generous benefits include paid health insurance, time-off benefits, 401(k), and a professional, friendly work environment (with remote and hybrid options) focused on achieving goals, recognizing successes and excelling at member service. Putting People First: Connect, Discover, Deliver & Wow is Orange County’s Credit Union mantra. If you’re passionate about serving people, this role is rewarding, brings purpose, and the opportunity to make a difference! Overview: Are you our next Data Engineer? With your strong understanding of financial services operations (e.g., banking, asset management, insurance, mortgage, consumer & business lending) you will be part of an innovative team that will architect, design and implement data analytics solutions to further advance the organization’s strategic goals. In collaboration with Business and IT partners, you will support our efforts in re-engineering, optimizing and advancing the organization’s data platform with modern cloud-based analytics technologies that will address near-term and future business needs. Be part of our vision to advance data-driven insights and decision making for our mission driven organization. Essential Functions: Collaborate with delivery team and engage with organizational stakeholders (Business and IT) to design, develop and deliver end-to-end enterprise data analytics solutions to enable data-driven insights and decision making. Translate business requirements to technical solutions by applying technical knowledge and strong business acumen. Solve business problems and complex data requirements/challenges by incorporating standards and best practices into engineering solutions, and leveraging modern data science programming languages (e.g., SQL, Python, R, Scala, SAS) and Azure data and analytics services. Develop and implement database designs (logical and physical) and data models (normalized and dimensional) to support the new analytics platform. Design, implement and maintain data ingestion/integration and end-to-end data pipeline processes using Azure technologies for a wide variety of traditional and non-traditional data sources/formats (structured, unstructured, and semi-structured) through various protocols (e.g., REST, SOAP, SFTP, MQ, etc). Technical Must Haves for this Role: 5+ years of experience in Information Technology within a medium to large enterprise with complex business and IT environment. 3+ years of experience as a Data Engineer working with cross-functional teams (within IT and/or Business) on enterprise level business intelligence/data analytics implementations using Azure cloud-based analytics platforms/technologies (including hands-on experience with Microsoft/Azure stack, e.g., Synapse, Data Factory, Data Bricks, Data Lake, Data Catalog, SSIS, SQL, etc., and NoSQL databases). 3+ years of hands-on experience in designing, implementing and maintaining data ingestion /integration and end-to-end data pipeline using Azure technologies for a wide variety of traditional and non-traditional data sources/formats (structured, unstructured, and semi-structured) through various protocols (e.g., REST, SOAP, SFTP, MQ, etc.). 2+ years of experience working with and developing database designs (logical and physical) and data models (normalized and dimensional) for data warehouse, data marts and operational data stores. 2+ years of hands-on experience working with data science programming languages (e.g., SQL, Python, R, Scala, SAS). Experience working in an agile delivery environment with working knowledge of continuous integration/continuous delivery (CI/CD) and DevOps practices.  The targeted hourly range is $36.55 - $54.83. Final offer will be determined based on experience, education, training/certifications and specialized skills.  We perform thorough background checks and credit checks. EOE. Must reside in the state of CA, AZ, NV or TX. Are you looking to join a dynamic, fast-paced team environment with a culture of collaboration and belonging? If so, let’s talk. Orange County's Credit Union is now seeking a talented and driven individual to accelerate our efforts and be a major part of our team and culture. Our team members are grounded in core values, have a strong capacity to learn, the energy to get things done, and bring real world experiences to help us think in new ways. Orange County's Credit Union actively invest in our team members to support their long-term growth so they can continue to advance our mission and achieve their highest potential. Are you passionate about the future? Are you a data engineer who specializes in wrangling a wide range of data into versatile, accessible sources of knowledge? If yes, PLEASE APPLY IMMEDATELY! More about Orange County's Credit Union: Workplace Excellence. Through our associates' opinions and voices, Orange County's Credit Union is proud to be recognized year over year as one of the best places to work in Orange County and is a recipient of the Peter Barron Stark Best of the Best Award for highest associate satisfaction in the workplace. As a leading financial service provider with over 80 years of experience serving 117,000+ members, Orange County's Credit Union is currently over $2 billion in assets & growing. Generous benefits include paid health insurance, time-off benefits, 401(k), and a professional, friendly work environment (with remote and hybrid options) focused on achieving goals, recognizing successes and excelling at member service. Putting People First: Connect, Discover, Deliver & Wow is Orange County’s Credit Union mantra. If you’re passionate about serving people, this role is rewarding, brings purpose, and the opportunity to make a difference! Overview: Are you our next Data Engineer? With your strong understanding of financial services operations (e.g., banking, asset management, insurance, mortgage, consumer & business lending) you will be part of an innovative team that will architect, design and implement data analytics solutions to further advance the organization’s strategic goals. In collaboration with Business and IT partners, you will support our efforts in re-engineering, optimizing and advancing the organization’s data platform with modern cloud-based analytics technologies that will address near-term and future business needs. Be part of our vision to advance data-driven insights and decision making for our mission driven organization. Essential Functions: Collaborate with delivery team and engage with organizational stakeholders (Business and IT) to design, develop and deliver end-to-end enterprise data analytics solutions to enable data-driven insights and decision making. Translate business requirements to technical solutions by applying technical knowledge and strong business acumen. Solve business problems and complex data requirements/challenges by incorporating standards and best practices into engineering solutions, and leveraging modern data science programming languages (e.g., SQL, Python, R, Scala, SAS) and Azure data and analytics services. Develop and implement database designs (logical and physical) and data models (normalized and dimensional) to support the new analytics platform. Design, implement and maintain data ingestion/integration and end-to-end data pipeline processes using Azure technologies for a wide variety of traditional and non-traditional data sources/formats (structured, unstructured, and semi-structured) through various protocols (e.g., REST, SOAP, SFTP, MQ, etc). Technical Must Haves for this Role: 5+ years of experience in Information Technology within a medium to large enterprise with complex business and IT environment. 3+ years of experience as a Data Engineer working with cross-functional teams (within IT and/or Business) on enterprise level business intelligence/data analytics implementations using Azure cloud-based analytics platforms/technologies (including hands-on experience with Microsoft/Azure stack, e.g., Synapse, Data Factory, Data Bricks, Data Lake, Data Catalog, SSIS, SQL, etc., and NoSQL databases). 3+ years of hands-on experience in designing, implementing and maintaining data ingestion /integration and end-to-end data pipeline using Azure technologies for a wide variety of traditional and non-traditional data sources/formats (structured, unstructured, and semi-structured) through various protocols (e.g., REST, SOAP, SFTP, MQ, etc.). 2+ years of experience working with and developing database designs (logical and physical) and data models (normalized and dimensional) for data warehouse, data marts and operational data stores. 2+ years of hands-on experience working with data science programming languages (e.g., SQL, Python, R, Scala, SAS). Experience working in an agile delivery environment with working knowledge of continuous integration/continuous delivery (CI/CD) and DevOps practices.  The targeted hourly range is $36.55 - $54.83. Final offer will be determined based on experience, education, training/certifications and specialized skills.  We perform thorough background checks and credit checks. EOE. Are you looking to join a dynamic, fast-paced team environment with a culture of collaboration and belonging? If so, let’s talk. Orange County's Credit Union is now seeking a talented and driven individual to accelerate our efforts and be a major part of our team and culture. Our team members are grounded in core values, have a strong capacity to learn, the energy to get things done, and bring real world experiences to help us think in new ways. Orange County's Credit Union actively invest in our team members to support their long-term growth so they can continue to advance our mission and achieve their highest potential. Are you passionate about the future? Are you a data engineer who specializes in wrangling a wide range of data into versatile, accessible sources of knowledge? If yes, PLEASE APPLY IMMEDATELY! More about Orange County's Credit Union: Workplace Excellence. Through our associates' opinions and voices, Orange County's Credit Union is proud to be recognized year over year as one of the best places to work in Orange County and is a recipient of the Peter Barron Stark Best of the Best Award for highest associate satisfaction in the workplace. As a leading financial service provider with over 80 years of experience serving 117,000+ members, Orange County's Credit Union is currently over $2 billion in assets & growing. Generous benefits include paid health insurance, time-off benefits, 401(k), and a professional, friendly work environment (with remote and hybrid options) focused on achieving goals, recognizing successes and excelling at member service. Putting People First: Connect, Discover, Deliver & Wow is Orange County’s Credit Union mantra. If you’re passionate about serving people, this role is rewarding, brings purpose, and the opportunity to make a difference! Overview: Are you our next Data Engineer? With your strong understanding of financial services operations (e.g., banking, asset management, insurance, mortgage, consumer & business lending) you will be part of an innovative team that will architect, design and implement data analytics solutions to further advance the organization’s strategic goals. In collaboration with Business and IT partners, you will support our efforts in re-engineering, optimizing and advancing the organization’s data platform with modern cloud-based analytics technologies that will address near-term and future business needs. Be part of our vision to advance data-driven insights and decision making for our mission driven organization. Essential Functions: Collaborate with delivery team and engage with organizational stakeholders (Business and IT) to design, develop and deliver end-to-end enterprise data analytics solutions to enable data-driven insights and decision making. Translate business requirements to technical solutions by applying technical knowledge and strong business acumen. Solve business problems and complex data requirements/challenges by incorporating standards and best practices into engineering solutions, and leveraging modern data science programming languages (e.g., SQL, Python, R, Scala, SAS) and Azure data and analytics services. Develop and implement database designs (logical and physical) and data models (normalized and dimensional) to support the new analytics platform. Design, implement and maintain data ingestion/integration and end-to-end data pipeline processes using Azure technologies for a wide variety of traditional and non-traditional data sources/formats (structured, unstructured, and semi-structured) through various protocols (e.g., REST, SOAP, SFTP, MQ, etc). Technical Must Haves for this Role: 5+ years of experience in Information Technology within a medium to large enterprise with complex business and IT environment. 3+ years of experience as a Data Engineer working with cross-functional teams (within IT and/or Business) on enterprise level business intelligence/data analytics implementations using Azure cloud-based analytics platforms/technologies (including hands-on experience with Microsoft/Azure stack, e.g., Synapse, Data Factory, Data Bricks, Data Lake, Data Catalog, SSIS, SQL, etc., and NoSQL databases). 3+ years of hands-on experience in designing, implementing and maintaining data ingestion /integration and end-to-end data pipeline using Azure technologies for a wide variety of traditional and non-traditional data sources/formats (structured, unstructured, and semi-structured) through various protocols (e.g., REST, SOAP, SFTP, MQ, etc.). 2+ years of experience working with and developing database designs (logical and physical) and data models (normalized and dimensional) for data warehouse, data marts and operational data stores. 2+ years of hands-on experience working with data science programming languages (e.g., SQL, Python, R, Scala, SAS). Experience working in an agile delivery environment with working knowledge of continuous integration/continuous delivery (CI/CD) and DevOps practices.  The targeted hourly range is $36.55 - $54.83. Final offer will be determined based on experience, education, training/certifications and specialized skills.  We perform thorough background checks and credit checks. EOE. Are you looking to join a dynamic, fast-paced team environment with a culture of collaboration and belonging? If so, let’s talk. Orange County's Credit Union is now seeking a talented and driven individual to accelerate our efforts and be a major part of our team and culture. Our team members are grounded in core values, have a strong capacity to learn, the energy to get things done, and bring real world experiences to help us think in new ways. Orange County's Credit Union actively invest in our team members to support their long-term growth so they can continue to advance our mission and achieve their highest potential. Are you passionate about the future? Are you a data engineer who specializes in wrangling a wide range of data into versatile, accessible sources of knowledge? If yes, PLEASE APPLY IMMEDATELY! More about Orange County's Credit Union: Workplace Excellence. Through our associates' opinions and voices, Orange County's Credit Union is proud to be recognized year over year as one of the best places to work in Orange County and is a recipient of the Peter Barron Stark Best of the Best Award for highest associate satisfaction in the workplace. As a leading financial service provider with over 80 years of experience serving 117,000+ members, Orange County's Credit Union is currently over $2 billion in assets & growing. Generous benefits include paid health insurance, time-off benefits, 401(k), and a professional, friendly work environment (with remote and hybrid options) focused on achieving goals, recognizing successes and excelling at member service. Putting People First: Connect, Discover, Deliver & Wow is Orange County’s Credit Union mantra. If you’re passionate about serving people, this role is rewarding, brings purpose, and the opportunity to make a difference! Overview: Are you our next Data Engineer? With your strong understanding of financial services operations (e.g., banking, asset management, insurance, mortgage, consumer & business lending) you will be part of an innovative team that will architect, design and implement data analytics solutions to further advance the organization’s strategic goals. In collaboration with Business and IT partners, you will support our efforts in re-engineering, optimizing and advancing the organization’s data platform with modern cloud-based analytics technologies that will address near-term and future business needs. Be part of our vision to advance data-driven insights and decision making for our mission driven organization. Essential Functions: Collaborate with delivery team and engage with organizational stakeholders (Business and IT) to design, develop and deliver end-to-end enterprise data analytics solutions to enable data-driven insights and decision making. Translate business requirements to technical solutions by applying technical knowledge and strong business acumen. Solve business problems and complex data requirements/challenges by incorporating standards and best practices into engineering solutions, and leveraging modern data science programming languages (e.g., SQL, Python, R, Scala, SAS) and Azure data and analytics services. Develop and implement database designs (logical and physical) and data models (normalized and dimensional) to support the new analytics platform. Design, implement and maintain data ingestion/integration and end-to-end data pipeline processes using Azure technologies for a wide variety of traditional and non-traditional data sources/formats (structured, unstructured, and semi-structured) through various protocols (e.g., REST, SOAP, SFTP, MQ, etc). Technical Must Haves for this Role: 5+ years of experience in Information Technology within a medium to large enterprise with complex business and IT environment. 3+ years of experience as a Data Engineer working with cross-functional teams (within IT and/or Business) on enterprise level business intelligence/data analytics implementations using Azure cloud-based analytics platforms/technologies (including hands-on experience with Microsoft/Azure stack, e.g., Synapse, Data Factory, Data Bricks, Data Lake, Data Catalog, SSIS, SQL, etc., and NoSQL databases). 3+ years of hands-on experience in designing, implementing and maintaining data ingestion /integration and end-to-end data pipeline using Azure technologies for a wide variety of traditional and non-traditional data sources/formats (structured, unstructured, and semi-structured) through various protocols (e.g., REST, SOAP, SFTP, MQ, etc.). 2+ years of experience working with and developing database designs (logical and physical) and data models (normalized and dimensional) for data warehouse, data marts and operational data stores. 2+ years of hands-on experience working with data science programming languages (e.g., SQL, Python, R, Scala, SAS). Experience working in an agile delivery environment with working knowledge of continuous integration/continuous delivery (CI/CD) and DevOps practices.  The targeted hourly range is $36.55 - $54.83. Final offer will be determined based on experience, education, training/certifications and specialized skills.  We perform thorough background checks and credit checks. EOE. Are you looking to join a dynamic, fast-paced team environment with a culture of collaboration and belonging? If so, let’s talk. Orange County's Credit Union is now seeking a talented and driven individual to accelerate our efforts and be a major part of our team and culture. Our team members are grounded in core values, have a strong capacity to learn, the energy to get things done, and bring real world experiences to help us think in new ways. Orange County's Credit Union actively invest in our team members to support their long-term growth so they can continue to advance our mission and achieve their highest potential. Are you passionate about the future? Are you a data engineer who specializes in wrangling a wide range of data into versatile, accessible sources of knowledge? If yes, PLEASE APPLY IMMEDATELY! More about Orange County's Credit Union: Workplace Excellence. Through our associates' opinions and voices, Orange County's Credit Union is proud to be recognized year over year as one of the best places to work in Orange County and is a recipient of the Peter Barron Stark Best of the Best Award for highest associate satisfaction in the workplace. As a leading financial service provider with over 80 years of experience serving 117,000+ members, Orange County's Credit Union is currently over $2 billion in assets & growing. Generous benefits include paid health insurance, time-off benefits, 401(k), and a professional, friendly work environment (with remote and hybrid options) focused on achieving goals, recognizing successes and excelling at member service. Putting People First: Connect, Discover, Deliver & Wow is Orange County’s Credit Union mantra. If you’re passionate about serving people, this role is rewarding, brings purpose, and the opportunity to make a difference! Overview: Are you our next Data Engineer? With your strong understanding of financial services operations (e.g., banking, asset management, insurance, mortgage, consumer & business lending) you will be part of an innovative team that will architect, design and implement data analytics solutions to further advance the organization’s strategic goals. In collaboration with Business and IT partners, you will support our efforts in re-engineering, optimizing and advancing the organization’s data platform with modern cloud-based analytics technologies that will address near-term and future business needs. Be part of our vision to advance data-driven insights and decision making for our mission driven organization. Essential Functions: Collaborate with delivery team and engage with organizational stakeholders (Business and IT) to design, develop and deliver end-to-end enterprise data analytics solutions to enable data-driven insights and decision making. Translate business requirements to technical solutions by applying technical knowledge and strong business acumen. Solve business problems and complex data requirements/challenges by incorporating standards and best practices into engineering solutions, and leveraging modern data science programming languages (e.g., SQL, Python, R, Scala, SAS) and Azure data and analytics services. Develop and implement database designs (logical and physical) and data models (normalized and dimensional) to support the new analytics platform. Design, implement and maintain data ingestion/integration and end-to-end data pipeline processes using Azure technologies for a wide variety of traditional and non-traditional data sources/formats (structured, unstructured, and semi-structured) through various protocols (e.g., REST, SOAP, SFTP, MQ, etc). Technical Must Haves for this Role: 5+ years of experience in Information Technology within a medium to large enterprise with complex business and IT environment. 3+ years of experience as a Data Engineer working with cross-functional teams (within IT and/or Business) on enterprise level business intelligence/data analytics implementations using Azure cloud-based analytics platforms/technologies (including hands-on experience with Microsoft/Azure stack, e.g., Synapse, Data Factory, Data Bricks, Data Lake, Data Catalog, SSIS, SQL, etc., and NoSQL databases). 3+ years of hands-on experience in designing, implementing and maintaining data ingestion /integration and end-to-end data pipeline using Azure technologies for a wide variety of traditional and non-traditional data sources/formats (structured, unstructured, and semi-structured) through various protocols (e.g., REST, SOAP, SFTP, MQ, etc.). 2+ years of experience working with and developing database designs (logical and physical) and data models (normalized and dimensional) for data warehouse, data marts and operational data stores. 2+ years of hands-on experience working with data science programming languages (e.g., SQL, Python, R, Scala, SAS). Experience working in an agile delivery environment with working knowledge of continuous integration/continuous delivery (CI/CD) and DevOps practices.  The targeted hourly range is $36.55 - $54.83. Final offer will be determined based on experience, education, training/certifications and specialized skills.  We perform thorough background checks and credit checks. EOE. Are you looking to join a dynamic, fast-paced team environment with a culture of collaboration and belonging? If so, let’s talk. Orange County's Credit Union is now seeking a talented and driven individual to accelerate our efforts and be a major part of our team and culture. Our team members are grounded in core values, have a strong capacity to learn, the energy to get things done, and bring real world experiences to help us think in new ways. Orange County's Credit Union actively invest in our team members to support their long-term growth so they can continue to advance our mission and achieve their highest potential. Are you passionate about the future? Are you a data engineer who specializes in wrangling a wide range of data into versatile, accessible sources of knowledge? If yes, PLEASE APPLY IMMEDATELY! More about Orange County's Credit Union: Workplace Excellence. Through our associates' opinions and voices, Orange County's Credit Union is proud to be recognized year over year as one of the best places to work in Orange County and is a recipient of the Peter Barron Stark Best of the Best Award for highest associate satisfaction in the workplace. As a leading financial service provider with over 80 years of experience serving 117,000+ members, Orange County's Credit Union is currently over $2 billion in assets & growing. Generous benefits include paid health insurance, time-off benefits, 401(k), and a professional, friendly work environment (with remote and hybrid options) focused on achieving goals, recognizing successes and excelling at member service. Putting People First: Connect, Discover, Deliver & Wow is Orange County’s Credit Union mantra. If you’re passionate about serving people, this role is rewarding, brings purpose, and the opportunity to make a difference! Overview: Are you our next Data Engineer? With your strong understanding of financial services operations (e.g., banking, asset management, insurance, mortgage, consumer & business lending) you will be part of an innovative team that will architect, design and implement data analytics solutions to further advance the organization’s strategic goals. In collaboration with Business and IT partners, you will support our efforts in re-engineering, optimizing and advancing the organization’s data platform with modern cloud-based analytics technologies that will address near-term and future business needs. Be part of our vision to advance data-driven insights and decision making for our mission driven organization. Essential Functions: Collaborate with delivery team and engage with organizational stakeholders (Business and IT) to design, develop and deliver end-to-end enterprise data analytics solutions to enable data-driven insights and decision making. Translate business requirements to technical solutions by applying technical knowledge and strong business acumen. Solve business problems and complex data requirements/challenges by incorporating standards and best practices into engineering solutions, and leveraging modern data science programming languages (e.g., SQL, Python, R, Scala, SAS) and Azure data and analytics services. Develop and implement database designs (logical and physical) and data models (normalized and dimensional) to support the new analytics platform. Design, implement and maintain data ingestion/integration and end-to-end data pipeline processes using Azure technologies for a wide variety of traditional and non-traditional data sources/formats (structured, unstructured, and semi-structured) through various protocols (e.g., REST, SOAP, SFTP, MQ, etc). Technical Must Haves for this Role: 5+ years of experience in Information Technology within a medium to large enterprise with complex business and IT environment. 3+ years of experience as a Data Engineer working with cross-functional teams (within IT and/or Business) on enterprise level business intelligence/data analytics implementations using Azure cloud-based analytics platforms/technologies (including hands-on experience with Microsoft/Azure stack, e.g., Synapse, Data Factory, Data Bricks, Data Lake, Data Catalog, SSIS, SQL, etc., and NoSQL databases). 3+ years of hands-on experience in designing, implementing and maintaining data ingestion /integration and end-to-end data pipeline using Azure technologies for a wide variety of traditional and non-traditional data sources/formats (structured, unstructured, and semi-structured) through various protocols (e.g., REST, SOAP, SFTP, MQ, etc.). 2+ years of experience working with and developing database designs (logical and physical) and data models (normalized and dimensional) for data warehouse, data marts and operational data stores. 2+ years of hands-on experience working with data science programming languages (e.g., SQL, Python, R, Scala, SAS). Experience working in an agile delivery environment with working knowledge of continuous integration/continuous delivery (CI/CD) and DevOps practices.  The targeted hourly range is $36.55 - $54.83. Final offer will be determined based on experience, education, training/certifications and specialized skills.  We perform thorough background checks and credit checks. EOE. Are you looking to join a dynamic, fast-paced team environment with a culture of collaboration and belonging? If so, let’s talk. Orange County's Credit Union is now seeking a talented and driven individual to accelerate our efforts and be a major part of our team and culture. Our team members are grounded in core values, have a strong capacity to learn, the energy to get things done, and bring real world experiences to help us think in new ways. Orange County's Credit Union actively invest in our team members to support their long-term growth so they can continue to advance our mission and achieve their highest potential. Are you passionate about the future? Are you a data engineer who specializes in wrangling a wide range of data into versatile, accessible sources of knowledge? If yes, PLEASE APPLY IMMEDATELY! More about Orange County's Credit Union: Workplace Excellence. Through our associates' opinions and voices, Orange County's Credit Union is proud to be recognized year over year as one of the best places to work in Orange County and is a recipient of the Peter Barron Stark Best of the Best Award for highest associate satisfaction in the workplace. As a leading financial service provider with over 80 years of experience serving 117,000+ members, Orange County's Credit Union is currently over $2 billion in assets & growing. Generous benefits include paid health insurance, time-off benefits, 401(k), and a professional, friendly work environment (with remote and hybrid options) focused on achieving goals, recognizing successes and excelling at member service. Putting People First: Connect, Discover, Deliver & Wow is Orange County’s Credit Union mantra. If you’re passionate about serving people, this role is rewarding, brings purpose, and the opportunity to make a difference! Overview: Are you our next Data Engineer? With your strong understanding of financial services operations (e.g., banking, asset management, insurance, mortgage, consumer & business lending) you will be part of an innovative team that will architect, design and implement data analytics solutions to further advance the organization’s strategic goals. In collaboration with Business and IT partners, you will support our efforts in re-engineering, optimizing and advancing the organization’s data platform with modern cloud-based analytics technologies that will address near-term and future business needs. Be part of our vision to advance data-driven insights and decision making for our mission driven organization. Essential Functions: Collaborate with delivery team and engage with organizational stakeholders (Business and IT) to design, develop and deliver end-to-end enterprise data analytics solutions to enable data-driven insights and decision making. Translate business requirements to technical solutions by applying technical knowledge and strong business acumen. Solve business problems and complex data requirements/challenges by incorporating standards and best practices into engineering solutions, and leveraging modern data science programming languages (e.g., SQL, Python, R, Scala, SAS) and Azure data and analytics services. Develop and implement database designs (logical and physical) and data models (normalized and dimensional) to support the new analytics platform. Design, implement and maintain data ingestion/integration and end-to-end data pipeline processes using Azure technologies for a wide variety of traditional and non-traditional data sources/formats (structured, unstructured, and semi-structured) through various protocols (e.g., REST, SOAP, SFTP, MQ, etc). Technical Must Haves for this Role: 5+ years of experience in Information Technology within a medium to large enterprise with complex business and IT environment. 3+ years of experience as a Data Engineer working with cross-functional teams (within IT and/or Business) on enterprise level business intelligence/data analytics implementations using Azure cloud-based analytics platforms/technologies (including hands-on experience with Microsoft/Azure stack, e.g., Synapse, Data Factory, Data Bricks, Data Lake, Data Catalog, SSIS, SQL, etc., and NoSQL databases). 3+ years of hands-on experience in designing, implementing and maintaining data ingestion /integration and end-to-end data pipeline using Azure technologies for a wide variety of traditional and non-traditional data sources/formats (structured, unstructured, and semi-structured) through various protocols (e.g., REST, SOAP, SFTP, MQ, etc.). 2+ years of experience working with and developing database designs (logical and physical) and data models (normalized and dimensional) for data warehouse, data marts and operational data stores. 2+ years of hands-on experience working with data science programming languages (e.g., SQL, Python, R, Scala, SAS). Experience working in an agile delivery environment with working knowledge of continuous integration/continuous delivery (CI/CD) and DevOps practices.  The targeted hourly range is $36.55 - $54.83. Final offer will be determined based on experience, education, training/certifications and specialized skills.  We perform thorough background checks and credit checks. EOE. Are you looking to join a dynamic, fast-paced team environment with a culture of collaboration and belonging? If so, let’s talk. Orange County's Credit Union is now seeking a talented and driven individual to accelerate our efforts and be a major part of our team and culture. Our team members are grounded in core values, have a strong capacity to learn, the energy to get things done, and bring real world experiences to help us think in new ways. Orange County's Credit Union actively invest in our team members to support their long-term growth so they can continue to advance our mission and achieve their highest potential. Are you passionate about the future? Are you a data engineer who specializes in wrangling a wide range of data into versatile, accessible sources of knowledge? If yes, PLEASE APPLY IMMEDATELY! More about Orange County's Credit Union: Workplace Excellence. Through our associates' opinions and voices, Orange County's Credit Union is proud to be recognized year over year as one of the best places to work in Orange County and is a recipient of the Peter Barron Stark Best of the Best Award for highest associate satisfaction in the workplace. As a leading financial service provider with over 80 years of experience serving 117,000+ members, Orange County's Credit Union is currently over $2 billion in assets & growing. Generous benefits include paid health insurance, time-off benefits, 401(k), and a professional, friendly work environment (with remote and hybrid options) focused on achieving goals, recognizing successes and excelling at member service. Putting People First: Connect, Discover, Deliver & Wow is Orange County’s Credit Union mantra. If you’re passionate about serving people, this role is rewarding, brings purpose, and the opportunity to make a difference! Overview: Are you our next Data Engineer? With your strong understanding of financial services operations (e.g., banking, asset management, insurance, mortgage, consumer & business lending) you will be part of an innovative team that will architect, design and implement data analytics solutions to further advance the organization’s strategic goals. In collaboration with Business and IT partners, you will support our efforts in re-engineering, optimizing and advancing the organization’s data platform with modern cloud-based analytics technologies that will address near-term and future business needs. Be part of our vision to advance data-driven insights and decision making for our mission driven organization. Essential Functions: Collaborate with delivery team and engage with organizational stakeholders (Business and IT) to design, develop and deliver end-to-end enterprise data analytics solutions to enable data-driven insights and decision making. Translate business requirements to technical solutions by applying technical knowledge and strong business acumen. Solve business problems and complex data requirements/challenges by incorporating standards and best practices into engineering solutions, and leveraging modern data science programming languages (e.g., SQL, Python, R, Scala, SAS) and Azure data and analytics services. Develop and implement database designs (logical and physical) and data models (normalized and dimensional) to support the new analytics platform. Design, implement and maintain data ingestion/integration and end-to-end data pipeline processes using Azure technologies for a wide variety of traditional and non-traditional data sources/formats (structured, unstructured, and semi-structured) through various protocols (e.g., REST, SOAP, SFTP, MQ, etc). Technical Must Haves for this Role: 5+ years of experience in Information Technology within a medium to large enterprise with complex business and IT environment. 3+ years of experience as a Data Engineer working with cross-functional teams (within IT and/or Business) on enterprise level business intelligence/data analytics implementations using Azure cloud-based analytics platforms/technologies (including hands-on experience with Microsoft/Azure stack, e.g., Synapse, Data Factory, Data Bricks, Data Lake, Data Catalog, SSIS, SQL, etc., and NoSQL databases). 3+ years of hands-on experience in designing, implementing and maintaining data ingestion /integration and end-to-end data pipeline using Azure technologies for a wide variety of traditional and non-traditional data sources/formats (structured, unstructured, and semi-structured) through various protocols (e.g., REST, SOAP, SFTP, MQ, etc.). 2+ years of experience working with and developing database designs (logical and physical) and data models (normalized and dimensional) for data warehouse, data marts and operational data stores. 2+ years of hands-on experience working with data science programming languages (e.g., SQL, Python, R, Scala, SAS). Experience working in an agile delivery environment with working knowledge of continuous integration/continuous delivery (CI/CD) and DevOps practices.  The targeted hourly range is $36.55 - $54.83. Final offer will be determined based on experience, education, training/certifications and specialized skills.  We perform thorough background checks and credit checks. EOE. Are you looking to join a dynamic, fast-paced team environment with a culture of collaboration and belonging? If so, let’s talk. Orange County's Credit Union is now seeking a talented and driven individual to accelerate our efforts and be a major part of our team and culture. Our team members are grounded in core values, have a strong capacity to learn, the energy to get things done, and bring real world experiences to help us think in new ways. Orange County's Credit Union actively invest in our team members to support their long-term growth so they can continue to advance our mission and achieve their highest potential. Are you passionate about the future? Are you a data engineer who specializes in wrangling a wide range of data into versatile, accessible sources of knowledge? If yes, PLEASE APPLY IMMEDATELY! More about Orange County's Credit Union: Workplace Excellence. Through our associates' opinions and voices, Orange County's Credit Union is proud to be recognized year over year as one of the best places to work in Orange County and is a recipient of the Peter Barron Stark Best of the Best Award for highest associate satisfaction in the workplace. As a leading financial service provider with over 80 years of experience serving 117,000+ members, Orange County's Credit Union is currently over $2 billion in assets & growing. Generous benefits include paid health insurance, time-off benefits, 401(k), and a professional, friendly work environment (with remote and hybrid options) focused on achieving goals, recognizing successes and excelling at member service. Putting People First: Connect, Discover, Deliver & Wow is Orange County’s Credit Union mantra. If you’re passionate about serving people, this role is rewarding, brings purpose, and the opportunity to make a difference! Overview: Are you our next Data Engineer? With your strong understanding of financial services operations (e.g., banking, asset management, insurance, mortgage, consumer & business lending) you will be part of an innovative team that will architect, design and implement data analytics solutions to further advance the organization’s strategic goals. In collaboration with Business and IT partners, you will support our efforts in re-engineering, optimizing and advancing the organization’s data platform with modern cloud-based analytics technologies that will address near-term and future business needs. Be part of our vision to advance data-driven insights and decision making for our mission driven organization. Essential Functions: Collaborate with delivery team and engage with organizational stakeholders (Business and IT) to design, develop and deliver end-to-end enterprise data analytics solutions to enable data-driven insights and decision making. Translate business requirements to technical solutions by applying technical knowledge and strong business acumen. Solve business problems and complex data requirements/challenges by incorporating standards and best practices into engineering solutions, and leveraging modern data science programming languages (e.g., SQL, Python, R, Scala, SAS) and Azure data and analytics services. Develop and implement database designs (logical and physical) and data models (normalized and dimensional) to support the new analytics platform. Design, implement and maintain data ingestion/integration and end-to-end data pipeline processes using Azure technologies for a wide variety of traditional and non-traditional data sources/formats (structured, unstructured, and semi-structured) through various protocols (e.g., REST, SOAP, SFTP, MQ, etc). Technical Must Haves for this Role: 5+ years of experience in Information Technology within a medium to large enterprise with complex business and IT environment. 3+ years of experience as a Data Engineer working with cross-functional teams (within IT and/or Business) on enterprise level business intelligence/data analytics implementations using Azure cloud-based analytics platforms/technologies (including hands-on experience with Microsoft/Azure stack, e.g., Synapse, Data Factory, Data Bricks, Data Lake, Data Catalog, SSIS, SQL, etc., and NoSQL databases). 3+ years of hands-on experience in designing, implementing and maintaining data ingestion /integration and end-to-end data pipeline using Azure technologies for a wide variety of traditional and non-traditional data sources/formats (structured, unstructured, and semi-structured) through various protocols (e.g., REST, SOAP, SFTP, MQ, etc.). 2+ years of experience working with and developing database designs (logical and physical) and data models (normalized and dimensional) for data warehouse, data marts and operational data stores. 2+ years of hands-on experience working with data science programming languages (e.g., SQL, Python, R, Scala, SAS). Experience working in an agile delivery environment with working knowledge of continuous integration/continuous delivery (CI/CD) and DevOps practices.  The targeted hourly range is $36.55 - $54.83. Final offer will be determined based on experience, education, training/certifications and specialized skills.  We perform thorough background checks and credit checks. EOE. Are you looking to join a dynamic, fast-paced team environment with a culture of collaboration and belonging? If so, let’s talk. Orange County's Credit Union is now seeking a talented and driven individual to accelerate our efforts and be a major part of our team and culture. Our team members are grounded in core values, have a strong capacity to learn, the energy to get things done, and bring real world experiences to help us think in new ways. Orange County's Credit Union actively invest in our team members to support their long-term growth so they can continue to advance our mission and achieve their highest potential. Are you passionate about the future? Are you a data engineer who specializes in wrangling a wide range of data into versatile, accessible sources of knowledge? If yes, PLEASE APPLY IMMEDATELY! More about Orange County's Credit Union: Workplace Excellence. Through our associates' opinions and voices, Orange County's Credit Union is proud to be recognized year over year as one of the best places to work in Orange County and is a recipient of the Peter Barron Stark Best of the Best Award for highest associate satisfaction in the workplace. As a leading financial service provider with over 80 years of experience serving 117,000+ members, Orange County's Credit Union is currently over $2 billion in assets & growing. Generous benefits include paid health insurance, time-off benefits, 401(k), and a professional, friendly work environment (with remote and hybrid options) focused on achieving goals, recognizing successes and excelling at member service. Putting People First: Connect, Discover, Deliver & Wow is Orange County’s Credit Union mantra. If you’re passionate about serving people, this role is rewarding, brings purpose, and the opportunity to make a difference! Overview: Are you our next Data Engineer? With your strong understanding of financial services operations (e.g., banking, asset management, insurance, mortgage, consumer & business lending) you will be part of an innovative team that will architect, design and implement data analytics solutions to further advance the organization’s strategic goals. In collaboration with Business and IT partners, you will support our efforts in re-engineering, optimizing and advancing the organization’s data platform with modern cloud-based analytics technologies that will address near-term and future business needs. Be part of our vision to advance data-driven insights and decision making for our mission driven organization. Essential Functions: Collaborate with delivery team and engage with organizational stakeholders (Business and IT) to design, develop and deliver end-to-end enterprise data analytics solutions to enable data-driven insights and decision making. Translate business requirements to technical solutions by applying technical knowledge and strong business acumen. Solve business problems and complex data requirements/challenges by incorporating standards and best practices into engineering solutions, and leveraging modern data science programming languages (e.g., SQL, Python, R, Scala, SAS) and Azure data and analytics services. Develop and implement database designs (logical and physical) and data models (normalized and dimensional) to support the new analytics platform. Design, implement and maintain data ingestion/integration and end-to-end data pipeline processes using Azure technologies for a wide variety of traditional and non-traditional data sources/formats (structured, unstructured, and semi-structured) through various protocols (e.g., REST, SOAP, SFTP, MQ, etc). Technical Must Haves for this Role: 5+ years of experience in Information Technology within a medium to large enterprise with complex business and IT environment. 3+ years of experience as a Data Engineer working with cross-functional teams (within IT and/or Business) on enterprise level business intelligence/data analytics implementations using Azure cloud-based analytics platforms/technologies (including hands-on experience with Microsoft/Azure stack, e.g., Synapse, Data Factory, Data Bricks, Data Lake, Data Catalog, SSIS, SQL, etc., and NoSQL databases). 3+ years of hands-on experience in designing, implementing and maintaining data ingestion /integration and end-to-end data pipeline using Azure technologies for a wide variety of traditional and non-traditional data sources/formats (structured, unstructured, and semi-structured) through various protocols (e.g., REST, SOAP, SFTP, MQ, etc.). 2+ years of experience working with and developing database designs (logical and physical) and data models (normalized and dimensional) for data warehouse, data marts and operational data stores. 2+ years of hands-on experience working with data science programming languages (e.g., SQL, Python, R, Scala, SAS). Experience working in an agile delivery environment with working knowledge of continuous integration/continuous delivery (CI/CD) and DevOps practices.  The targeted hourly range is $36.55 - $54.83. Final offer will be determined based on experience, education, training/certifications and specialized skills.  We perform thorough background checks and credit checks. EOE. Are you looking to join a dynamic, fast-paced team environment with a culture of collaboration and belonging? If so, let’s talk. Orange County's Credit Union is now seeking a talented and driven individual to accelerate our efforts and be a major part of our team and culture. Our team members are grounded in core values, have a strong capacity to learn, the energy to get things done, and bring real world experiences to help us think in new ways. Orange County's Credit Union actively invest in our team members to support their long-term growth so they can continue to advance our mission and achieve their highest potential. Are you passionate about the future? Are you a data engineer who specializes in wrangling a wide range of data into versatile, accessible sources of knowledge? If yes, PLEASE APPLY IMMEDATELY! More about Orange County's Credit Union: Workplace Excellence. Through our associates' opinions and voices, Orange County's Credit Union is proud to be recognized year over year as one of the best places to work in Orange County and is a recipient of the Peter Barron Stark Best of the Best Award for highest associate satisfaction in the workplace. As a leading financial service provider with over 80 years of experience serving 117,000+ members, Orange County's Credit Union is currently over $2 billion in assets & growing. Generous benefits include paid health insurance, time-off benefits, 401(k), and a professional, friendly work environment (with remote and hybrid options) focused on achieving goals, recognizing successes and excelling at member service. Putting People First: Connect, Discover, Deliver & Wow is Orange County’s Credit Union mantra. If you’re passionate about serving people, this role is rewarding, brings purpose, and the opportunity to make a difference! Overview: Are you our next Data Engineer? With your strong understanding of financial services operations (e.g., banking, asset management, insurance, mortgage, consumer & business lending) you will be part of an innovative team that will architect, design and implement data analytics solutions to further advance the organization’s strategic goals. In collaboration with Business and IT partners, you will support our efforts in re-engineering, optimizing and advancing the organization’s data platform with modern cloud-based analytics technologies that will address near-term and future business needs. Be part of our vision to advance data-driven insights and decision making for our mission driven organization. Essential Functions: Collaborate with delivery team and engage with organizational stakeholders (Business and IT) to design, develop and deliver end-to-end enterprise data analytics solutions to enable data-driven insights and decision making. Translate business requirements to technical solutions by applying technical knowledge and strong business acumen. Solve business problems and complex data requirements/challenges by incorporating standards and best practices into engineering solutions, and leveraging modern data science programming languages (e.g., SQL, Python, R, Scala, SAS) and Azure data and analytics services. Develop and implement database designs (logical and physical) and data models (normalized and dimensional) to support the new analytics platform. Design, implement and maintain data ingestion/integration and end-to-end data pipeline processes using Azure technologies for a wide variety of traditional and non-traditional data sources/formats (structured, unstructured, and semi-structured) through various protocols (e.g., REST, SOAP, SFTP, MQ, etc). Technical Must Haves for this Role: 5+ years of experience in Information Technology within a medium to large enterprise with complex business and IT environment. 3+ years of experience as a Data Engineer working with cross-functional teams (within IT and/or Business) on enterprise level business intelligence/data analytics implementations using Azure cloud-based analytics platforms/technologies (including hands-on experience with Microsoft/Azure stack, e.g., Synapse, Data Factory, Data Bricks, Data Lake, Data Catalog, SSIS, SQL, etc., and NoSQL databases). 3+ years of hands-on experience in designing, implementing and maintaining data ingestion /integration and end-to-end data pipeline using Azure technologies for a wide variety of traditional and non-traditional data sources/formats (structured, unstructured, and semi-structured) through various protocols (e.g., REST, SOAP, SFTP, MQ, etc.). 2+ years of experience working with and developing database designs (logical and physical) and data models (normalized and dimensional) for data warehouse, data marts and operational data stores. 2+ years of hands-on experience working with data science programming languages (e.g., SQL, Python, R, Scala, SAS). Experience working in an agile delivery environment with working knowledge of continuous integration/continuous delivery (CI/CD) and DevOps practices.  The targeted hourly range is $36.55 - $54.83. Final offer will be determined based on experience, education, training/certifications and specialized skills.  We perform thorough background checks and credit checks. EOE. Are you looking to join a dynamic, fast-paced team environment with a culture of collaboration and belonging? If so, let’s talk. Orange County's Credit Union is now seeking a talented and driven individual to accelerate our efforts and be a major part of our team and culture. Our team members are grounded in core values, have a strong capacity to learn, the energy to get things done, and bring real world experiences to help us think in new ways. Orange County's Credit Union actively invest in our team members to support their long-term growth so they can continue to advance our mission and achieve their highest potential. Are you passionate about the future? Are you a data engineer who specializes in wrangling a wide range of data into versatile, accessible sources of knowledge? If yes, PLEASE APPLY IMMEDATELY! More about Orange County's Credit Union: Workplace Excellence. Through our associates' opinions and voices, Orange County's Credit Union is proud to be recognized year over year as one of the best places to work in Orange County and is a recipient of the Peter Barron Stark Best of the Best Award for highest associate satisfaction in the workplace. As a leading financial service provider with over 80 years of experience serving 117,000+ members, Orange County's Credit Union is currently over $2 billion in assets & growing. Generous benefits include paid health insurance, time-off benefits, 401(k), and a professional, friendly work environment (with remote and hybrid options) focused on achieving goals, recognizing successes and excelling at member service. Putting People First: Connect, Discover, Deliver & Wow is Orange County’s Credit Union mantra. If you’re passionate about serving people, this role is rewarding, brings purpose, and the opportunity to make a difference! Overview: Are you our next Data Engineer? With your strong understanding of financial services operations (e.g., banking, asset management, insurance, mortgage, consumer & business lending) you will be part of an innovative team that will architect, design and implement data analytics solutions to further advance the organization’s strategic goals. In collaboration with Business and IT partners, you will support our efforts in re-engineering, optimizing and advancing the organization’s data platform with modern cloud-based analytics technologies that will address near-term and future business needs. Be part of our vision to advance data-driven insights and decision making for our mission driven organization. Essential Functions: Collaborate with delivery team and engage with organizational stakeholders (Business and IT) to design, develop and deliver end-to-end enterprise data analytics solutions to enable data-driven insights and decision making. Translate business requirements to technical solutions by applying technical knowledge and strong business acumen. Solve business problems and complex data requirements/challenges by incorporating standards and best practices into engineering solutions, and leveraging modern data science programming languages (e.g., SQL, Python, R, Scala, SAS) and Azure data and analytics services. Develop and implement database designs (logical and physical) and data models (normalized and dimensional) to support the new analytics platform. Design, implement and maintain data ingestion/integration and end-to-end data pipeline processes using Azure technologies for a wide variety of traditional and non-traditional data sources/formats (structured, unstructured, and semi-structured) through various protocols (e.g., REST, SOAP, SFTP, MQ, etc). Technical Must Haves for this Role: 5+ years of experience in Information Technology within a medium to large enterprise with complex business and IT environment. 3+ years of experience as a Data Engineer working with cross-functional teams (within IT and/or Business) on enterprise level business intelligence/data analytics implementations using Azure cloud-based analytics platforms/technologies (including hands-on experience with Microsoft/Azure stack, e.g., Synapse, Data Factory, Data Bricks, Data Lake, Data Catalog, SSIS, SQL, etc., and NoSQL databases). 3+ years of hands-on experience in designing, implementing and maintaining data ingestion /integration and end-to-end data pipeline using Azure technologies for a wide variety of traditional and non-traditional data sources/formats (structured, unstructured, and semi-structured) through various protocols (e.g., REST, SOAP, SFTP, MQ, etc.). 2+ years of experience working with and developing database designs (logical and physical) and data models (normalized and dimensional) for data warehouse, data marts and operational data stores. 2+ years of hands-on experience working with data science programming languages (e.g., SQL, Python, R, Scala, SAS). Experience working in an agile delivery environment with working knowledge of continuous integration/continuous delivery (CI/CD) and DevOps practices.  The targeted hourly range is $36.55 - $54.83. Final offer will be determined based on experience, education, training/certifications and specialized skills.  We perform thorough background checks and credit checks. EOE. Are you looking to join a dynamic, fast-paced team environment with a culture of collaboration and belonging? If so, let’s talk. Orange County's Credit Union is now seeking a talented and driven individual to accelerate our efforts and be a major part of our team and culture. Our team members are grounded in core values, have a strong capacity to learn, the energy to get things done, and bring real world experiences to help us think in new ways. Orange County's Credit Union actively invest in our team members to support their long-term growth so they can continue to advance our mission and achieve their highest potential. Are you passionate about the future? Are you a data engineer who specializes in wrangling a wide range of data into versatile, accessible sources of knowledge? If yes, PLEASE APPLY IMMEDATELY! More about Orange County's Credit Union: Workplace Excellence. Through our associates' opinions and voices, Orange County's Credit Union is proud to be recognized year over year as one of the best places to work in Orange County and is a recipient of the Peter Barron Stark Best of the Best Award for highest associate satisfaction in the workplace. As a leading financial service provider with over 80 years of experience serving 117,000+ members, Orange County's Credit Union is currently over $2 billion in assets & growing. Generous benefits include paid health insurance, time-off benefits, 401(k), and a professional, friendly work environment (with remote and hybrid options) focused on achieving goals, recognizing successes and excelling at member service. Putting People First: Connect, Discover, Deliver & Wow is Orange County’s Credit Union mantra. If you’re passionate about serving people, this role is rewarding, brings purpose, and the opportunity to make a difference! Overview: Are you our next Data Engineer? With your strong understanding of financial services operations (e.g., banking, asset management, insurance, mortgage, consumer & business lending) you will be part of an innovative team that will architect, design and implement data analytics solutions to further advance the organization’s strategic goals. In collaboration with Business and IT partners, you will support our efforts in re-engineering, optimizing and advancing the organization’s data platform with modern cloud-based analytics technologies that will address near-term and future business needs. Be part of our vision to advance data-driven insights and decision making for our mission driven organization. Essential Functions: Collaborate with delivery team and engage with organizational stakeholders (Business and IT) to design, develop and deliver end-to-end enterprise data analytics solutions to enable data-driven insights and decision making. Translate business requirements to technical solutions by applying technical knowledge and strong business acumen. Solve business problems and complex data requirements/challenges by incorporating standards and best practices into engineering solutions, and leveraging modern data science programming languages (e.g., SQL, Python, R, Scala, SAS) and Azure data and analytics services. Develop and implement database designs (logical and physical) and data models (normalized and dimensional) to support the new analytics platform. Design, implement and maintain data ingestion/integration and end-to-end data pipeline processes using Azure technologies for a wide variety of traditional and non-traditional data sources/formats (structured, unstructured, and semi-structured) through various protocols (e.g., REST, SOAP, SFTP, MQ, etc). Technical Must Haves for this Role: 5+ years of experience in Information Technology within a medium to large enterprise with complex business and IT environment. 3+ years of experience as a Data Engineer working with cross-functional teams (within IT and/or Business) on enterprise level business intelligence/data analytics implementations using Azure cloud-based analytics platforms/technologies (including hands-on experience with Microsoft/Azure stack, e.g., Synapse, Data Factory, Data Bricks, Data Lake, Data Catalog, SSIS, SQL, etc., and NoSQL databases). 3+ years of hands-on experience in designing, implementing and maintaining data ingestion /integration and end-to-end data pipeline using Azure technologies for a wide variety of traditional and non-traditional data sources/formats (structured, unstructured, and semi-structured) through various protocols (e.g., REST, SOAP, SFTP, MQ, etc.). 2+ years of experience working with and developing database designs (logical and physical) and data models (normalized and dimensional) for data warehouse, data marts and operational data stores. 2+ years of hands-on experience working with data science programming languages (e.g., SQL, Python, R, Scala, SAS). Experience working in an agile delivery environment with working knowledge of continuous integration/continuous delivery (CI/CD) and DevOps practices.  The targeted hourly range is $36.55 - $54.83. Final offer will be determined based on experience, education, training/certifications and specialized skills.  We perform thorough background checks and credit checks. EOE. Are you looking to join a dynamic, fast-paced team environment with a culture of collaboration and belonging? If so, let’s talk. Orange County's Credit Union is now seeking a talented and driven individual to accelerate our efforts and be a major part of our team and culture. Our team members are grounded in core values, have a strong capacity to learn, the energy to get things done, and bring real world experiences to help us think in new ways. Orange County's Credit Union actively invest in our team members to support their long-term growth so they can continue to advance our mission and achieve their highest potential. Are you passionate about the future? Are you a data engineer who specializes in wrangling a wide range of data into versatile, accessible sources of knowledge? If yes, PLEASE APPLY IMMEDATELY! More about Orange County's Credit Union: Workplace Excellence. Through our associates' opinions and voices, Orange County's Credit Union is proud to be recognized year over year as one of the best places to work in Orange County and is a recipient of the Peter Barron Stark Best of the Best Award for highest associate satisfaction in the workplace. As a leading financial service provider with over 80 years of experience serving 117,000+ members, Orange County's Credit Union is currently over $2 billion in assets & growing. Generous benefits include paid health insurance, time-off benefits, 401(k), and a professional, friendly work environment (with remote and hybrid options) focused on achieving goals, recognizing successes and excelling at member service. Putting People First: Connect, Discover, Deliver & Wow is Orange County’s Credit Union mantra. If you’re passionate about serving people, this role is rewarding, brings purpose, and the opportunity to make a difference! Overview: Are you our next Data Engineer? With your strong understanding of financial services operations (e.g., banking, asset management, insurance, mortgage, consumer & business lending) you will be part of an innovative team that will architect, design and implement data analytics solutions to further advance the organization’s strategic goals. In collaboration with Business and IT partners, you will support our efforts in re-engineering, optimizing and advancing the organization’s data platform with modern cloud-based analytics technologies that will address near-term and future business needs. Be part of our vision to advance data-driven insights and decision making for our mission driven organization. Essential Functions: Collaborate with delivery team and engage with organizational stakeholders (Business and IT) to design, develop and deliver end-to-end enterprise data analytics solutions to enable data-driven insights and decision making. Translate business requirements to technical solutions by applying technical knowledge and strong business acumen. Solve business problems and complex data requirements/challenges by incorporating standards and best practices into engineering solutions, and leveraging modern data science programming languages (e.g., SQL, Python, R, Scala, SAS) and Azure data and analytics services. Develop and implement database designs (logical and physical) and data models (normalized and dimensional) to support the new analytics platform. Design, implement and maintain data ingestion/integration and end-to-end data pipeline processes using Azure technologies for a wide variety of traditional and non-traditional data sources/formats (structured, unstructured, and semi-structured) through various protocols (e.g., REST, SOAP, SFTP, MQ, etc). Technical Must Haves for this Role: 5+ years of experience in Information Technology within a medium to large enterprise with complex business and IT environment. 3+ years of experience as a Data Engineer working with cross-functional teams (within IT and/or Business) on enterprise level business intelligence/data analytics implementations using Azure cloud-based analytics platforms/technologies (including hands-on experience with Microsoft/Azure stack, e.g., Synapse, Data Factory, Data Bricks, Data Lake, Data Catalog, SSIS, SQL, etc., and NoSQL databases). 3+ years of hands-on experience in designing, implementing and maintaining data ingestion /integration and end-to-end data pipeline using Azure technologies for a wide variety of traditional and non-traditional data sources/formats (structured, unstructured, and semi-structured) through various protocols (e.g., REST, SOAP, SFTP, MQ, etc.). 2+ years of experience working with and developing database designs (logical and physical) and data models (normalized and dimensional) for data warehouse, data marts and operational data stores. 2+ years of hands-on experience working with data science programming languages (e.g., SQL, Python, R, Scala, SAS). Experience working in an agile delivery environment with working knowledge of continuous integration/continuous delivery (CI/CD) and DevOps practices.  The targeted hourly range is $36.55 - $54.83. Final offer will be determined based on experience, education, training/certifications and specialized skills.  We perform thorough background checks and credit checks. EOE. Are you looking to join a dynamic, fast-paced team environment with a culture of collaboration and belonging? If so, let’s talk. Orange County's Credit Union is now seeking a talented and driven individual to accelerate our efforts and be a major part of our team and culture. Our team members are grounded in core values, have a strong capacity to learn, the energy to get things done, and bring real world experiences to help us think in new ways. Orange County's Credit Union actively invest in our team members to support their long-term growth so they can continue to advance our mission and achieve their highest potential. Are you passionate about the future? Are you a data engineer who specializes in wrangling a wide range of data into versatile, accessible sources of knowledge? If yes, PLEASE APPLY IMMEDATELY! More about Orange County's Credit Union: More about Orange County's Credit Union: Workplace Excellence. Through our associates' opinions and voices, Orange County's Credit Union is proud to be recognized year over year as one of the best places to work in Orange County and is a recipient of the Peter Barron Stark Best of the Best Award for highest associate satisfaction in the workplace. As a leading financial service provider with over 80 years of experience serving 117,000+ members, Orange County's Credit Union is currently over $2 billion in assets & growing. Generous benefits include paid health insurance, time-off benefits, 401(k), and a professional, friendly work environment (with remote and hybrid options) focused on achieving goals, recognizing successes and excelling at member service. Putting People First: Connect, Discover, Deliver & Wow is Orange County’s Credit Union mantra. If you’re passionate about serving people, this role is rewarding, brings purpose, and the opportunity to make a difference! Overview: Are you our next Data Engineer? With your strong understanding of financial services operations (e.g., banking, asset management, insurance, mortgage, consumer & business lending) you will be part of an innovative team that will architect, design and implement data analytics solutions to further advance the organization’s strategic goals. In collaboration with Business and IT partners, you will support our efforts in re-engineering, optimizing and advancing the organization’s data platform with modern cloud-based analytics technologies that will address near-term and future business needs. Be part of our vision to advance data-driven insights and decision making for our mission driven organization. Essential Functions: Collaborate with delivery team and engage with organizational stakeholders (Business and IT) to design, develop and deliver end-to-end enterprise data analytics solutions to enable data-driven insights and decision making. Translate business requirements to technical solutions by applying technical knowledge and strong business acumen. Solve business problems and complex data requirements/challenges by incorporating standards and best practices into engineering solutions, and leveraging modern data science programming languages (e.g., SQL, Python, R, Scala, SAS) and Azure data and analytics services. Develop and implement database designs (logical and physical) and data models (normalized and dimensional) to support the new analytics platform. Design, implement and maintain data ingestion/integration and end-to-end data pipeline processes using Azure technologies for a wide variety of traditional and non-traditional data sources/formats (structured, unstructured, and semi-structured) through various protocols (e.g., REST, SOAP, SFTP, MQ, etc). Collaborate with delivery team and engage with organizational stakeholders (Business and IT) to design, develop and deliver end-to-end enterprise data analytics solutions to enable data-driven insights and decision making. Translate business requirements to technical solutions by applying technical knowledge and strong business acumen. Solve business problems and complex data requirements/challenges by incorporating standards and best practices into engineering solutions, and leveraging modern data science programming languages (e.g., SQL, Python, R, Scala, SAS) and Azure data and analytics services. Develop and implement database designs (logical and physical) and data models (normalized and dimensional) to support the new analytics platform. Design, implement and maintain data ingestion/integration and end-to-end data pipeline processes using Azure technologies for a wide variety of traditional and non-traditional data sources/formats (structured, unstructured, and semi-structured) through various protocols (e.g., REST, SOAP, SFTP, MQ, etc). Technical Must Haves for this Role: 5+ years of experience in Information Technology within a medium to large enterprise with complex business and IT environment. 3+ years of experience as a Data Engineer working with cross-functional teams (within IT and/or Business) on enterprise level business intelligence/data analytics implementations using Azure cloud-based analytics platforms/technologies (including hands-on experience with Microsoft/Azure stack, e.g., Synapse, Data Factory, Data Bricks, Data Lake, Data Catalog, SSIS, SQL, etc., and NoSQL databases). 3+ years of hands-on experience in designing, implementing and maintaining data ingestion /integration and end-to-end data pipeline using Azure technologies for a wide variety of traditional and non-traditional data sources/formats (structured, unstructured, and semi-structured) through various protocols (e.g., REST, SOAP, SFTP, MQ, etc.). 2+ years of experience working with and developing database designs (logical and physical) and data models (normalized and dimensional) for data warehouse, data marts and operational data stores. 2+ years of hands-on experience working with data science programming languages (e.g., SQL, Python, R, Scala, SAS). Experience working in an agile delivery environment with working knowledge of continuous integration/continuous delivery (CI/CD) and DevOps practices. 5+ years of experience in Information Technology within a medium to large enterprise with complex business and IT environment. 3+ years of experience as a Data Engineer working with cross-functional teams (within IT and/or Business) on enterprise level business intelligence/data analytics implementations using Azure cloud-based analytics platforms/technologies (including hands-on experience with Microsoft/Azure stack, e.g., Synapse, Data Factory, Data Bricks, Data Lake, Data Catalog, SSIS, SQL, etc., and NoSQL databases). 3+ years of hands-on experience in designing, implementing and maintaining data ingestion /integration and end-to-end data pipeline using Azure technologies for a wide variety of traditional and non-traditional data sources/formats (structured, unstructured, and semi-structured) through various protocols (e.g., REST, SOAP, SFTP, MQ, etc.). 2+ years of experience working with and developing database designs (logical and physical) and data models (normalized and dimensional) for data warehouse, data marts and operational data stores. 2+ years of hands-on experience working with data science programming languages (e.g., SQL, Python, R, Scala, SAS). Experience working in an agile delivery environment with working knowledge of continuous integration/continuous delivery (CI/CD) and DevOps practices. The targeted hourly range is $36.55 - $54.83. Final offer will be determined based on experience, education, training/certifications and specialized skills. The targeted hourly range is $36.55 - $54.83. Final offer will be determined based on experience, education, training/certifications and specialized skills. We perform thorough background checks and credit checks. EOE. We perform thorough background checks and credit checks. EOE.",4.0,201 to 500 Employees,1938,Nonprofit Organization,Banking & Lending,Financial Services,$25 to $100 million (USD)
"Phoenix, AZ",Big Data Engineer,$50.00 - $60.00 Per Hour (Employer est.),VedaInfo Inc,"Hi, I hope this note finds you well I am reaching out to you regarding your career opportunity with us for “Big Data Engineer” for a “6+ Months Contract” @Phoenix, AZ Big Data Engineer 6+ Months Contract Phoenix, AZ Please find the below requirement details. Job Title: Big Data Engineer Job Title: Big Data Engineer Location: Phoenix, AZ (Onsite) Location: Phoenix, AZ (Onsite) Duration: 6+ months Duration: 6+ months Rate: $60/hr C2C Rate: $60/hr C2C Responsibilities: Responsibilities: Design, implement, and maintain big data systems handling large volumes of data. Utilize Hadoop, Hive, and Spark for efficient data processing and analysis. Collaborate with cross-functional teams to understand data requirements. Develop and optimize data ingestion, storage, and transformation processes. Build scalable data pipelines for seamless data processing and analysis. Monitor and troubleshoot data processing and performance issues. Stay updated with emerging big data technologies. Requirements: Requirements: Bachelor's/Master's degree in CS, Engineering, or related field 7+ years of experience as a Big Data Engineer Strong proficiency in Hadoop, Hive, and Spark Extensive experience with AWS or Azure Solid understanding of data ingestion, storage, and transformation Excellent problem-solving and communication skills Proactive and self-motivated This is a 6+ month onsite contract position in Phoenix, AZ, with the possibility of extension. Compensation is $60/hr on C2C basis. Please submit your updated resume highlighting relevant experience. Please submit your updated resume highlighting relevant experience. Thank you for your interest! Thanks & Regards Thanks & Regards Mohammed ZAIN Mohammed ZAIN Job Type: Contract Pay: $50.00 - $60.00 per hour Experience level: 7 years 7 years Ability to commute/relocate: Phoenix, AZ: Reliably commute or planning to relocate before starting work (Required) Phoenix, AZ: Reliably commute or planning to relocate before starting work (Required) Application Question(s): To improve your chances of getting Shortlisted, make sure to tailor your Profile to the Job Requirements, Highlight your Relevant Skills and Experience, and then upload your Resume. To improve your chances of getting Shortlisted, make sure to tailor your Profile to the Job Requirements, Highlight your Relevant Skills and Experience, and then upload your Resume. Experience: AWS (Amazon Web Services): 7 years (Required) Azure: 7 years (Required) Big data: 7 years (Required) Hadoop: 7 years (Required) Apache Hive: 7 years (Required) Spark: 7 years (Required) AWS (Amazon Web Services): 7 years (Required) Azure: 7 years (Required) Big data: 7 years (Required) Hadoop: 7 years (Required) Apache Hive: 7 years (Required) Spark: 7 years (Required) Work Location: In person",4.1,201 to 500 Employees,N/A,Company - Private,Information Technology Support Services,Information Technology,$1 to $5 million (USD)
"Plano, TX",Data Engineer,$60K - $110K (Employer est.),HealthComp,"Description  Healthcomp is hiring a SQL Database Developer who will be responsible for writing and editing complex SQL queries, designing tables, stored procedures, indexes, views, functions and performance tuning extracts. This role works closely with other members of the technical team to address business needs related to database administration, data warehousing, extraction, availability and delivery. If you are a data programmer with strong SQL experience, looking for high-impact role, this is the perfect opportunity for you!  Key Responsibilities  Collaborates with technical and business users to develop and maintain enterprise-wide solutions and standards to provide data required for metrics and analysis Creates test transactions, performs unit testing and confirms programs meet specifications Applies strong analytical and technical skills to understand, define, research, and resolve everyday application support issues Supports daily database administration and data warehouse processes, including checking production processes and/or jobs, data replication jobs, index maintenance, etc. Maintain and enhance the existing data warehouse and extracts Perform quality checks on reports and extracts to ensure exceptional quality Provides on-call support as needed Build relationships with business intelligence partners to understand data needs in order to execute with excellence on documented user requirements Create and maintain documentation for all projects  Skills, Knowledge & Expertise  Bachelor’s degree in computer science 3+ years of experience in SQL development, including developing SQL queries, designing tables, stored procedures, indexes, views, functions and performance tuning 3+ years of experience in software development, preferably in C# or C++ / .NET Desired experience in Visual Studio, SQL Server Management Studio, SSIS and SSRS Knowledgeable in DevOps (Azure & Git) and continuous integration/automated deployment Preferred experience in health insurance industry Ability to work independently and make decisions within department and company guidelines  Benefits  Competitive Salary with opportunity for advancement Flexible paid time off policy to support a healthy work-life environment Full offering of health and wellness benefits for you and your family Company paid life insurance and disability plan 401K plan with company matching  About HealthComp Operating since 1994, we’re a third-party administrator (TPA) committed to providing employers with all the services needed to administer their benefits efficiently resulting in better health outcomes for their employees and higher cost savings for them. We partner with a variety of health providers and technology vendors to ensure a robust offering of medical, dental, vision, COBRA, HIPAA, flexible spending accounts and reference-based pricing, so members can make the most out of their benefits. It’s comprehensive care without the confusion.  HealthComp is an Equal Opportunity Employer.  HealthComp recruiting correspondence will always come from a talent acquisition representative with an official @healthcomp e-mail address. In addition, our representatives will never ask for any form of payment from a new hire or candidate. Please report suspicious activity to hiringsecurity@Healthcomp.com. Description Description Healthcomp is hiring a SQL Database Developer who will be responsible for writing and editing complex SQL queries, designing tables, stored procedures, indexes, views, functions and performance tuning extracts. This role works closely with other members of the technical team to address business needs related to database administration, data warehousing, extraction, availability and delivery. If you are a data programmer with strong SQL experience, looking for high-impact role, this is the perfect opportunity for you! SQL Database Developer SQL Database Developer If you are a data programmer with strong SQL experience, looking for high-impact role, this is the perfect opportunity for you! If you are a data programmer with strong SQL experience, looking for high-impact role, this is the perfect opportunity for you! Key Responsibilities Key Responsibilities Collaborates with technical and business users to develop and maintain enterprise-wide solutions and standards to provide data required for metrics and analysis Creates test transactions, performs unit testing and confirms programs meet specifications Applies strong analytical and technical skills to understand, define, research, and resolve everyday application support issues Supports daily database administration and data warehouse processes, including checking production processes and/or jobs, data replication jobs, index maintenance, etc. Maintain and enhance the existing data warehouse and extracts Perform quality checks on reports and extracts to ensure exceptional quality Provides on-call support as needed Build relationships with business intelligence partners to understand data needs in order to execute with excellence on documented user requirements Create and maintain documentation for all projects Collaborates with technical and business users to develop and maintain enterprise-wide solutions and standards to provide data required for metrics and analysis Creates test transactions, performs unit testing and confirms programs meet specifications Applies strong analytical and technical skills to understand, define, research, and resolve everyday application support issues Supports daily database administration and data warehouse processes, including checking production processes and/or jobs, data replication jobs, index maintenance, etc. Maintain and enhance the existing data warehouse and extracts Perform quality checks on reports and extracts to ensure exceptional quality Provides on-call support as needed Build relationships with business intelligence partners to understand data needs in order to execute with excellence on documented user requirements Create and maintain documentation for all projects Skills, Knowledge & Expertise Skills, Knowledge & Expertise Bachelor’s degree in computer science 3+ years of experience in SQL development, including developing SQL queries, designing tables, stored procedures, indexes, views, functions and performance tuning 3+ years of experience in software development, preferably in C# or C++ / .NET Desired experience in Visual Studio, SQL Server Management Studio, SSIS and SSRS Knowledgeable in DevOps (Azure & Git) and continuous integration/automated deployment Preferred experience in health insurance industry Ability to work independently and make decisions within department and company guidelines Bachelor’s degree in computer science 3+ years of experience in SQL development, including developing SQL queries, designing tables, stored procedures, indexes, views, functions and performance tuning 3+ years of experience in software development, preferably in C# or C++ / .NET Desired experience in Visual Studio, SQL Server Management Studio, SSIS and SSRS Knowledgeable in DevOps (Azure & Git) and continuous integration/automated deployment Preferred experience in health insurance industry Ability to work independently and make decisions within department and company guidelines Benefits Benefits Competitive Salary with opportunity for advancement Flexible paid time off policy to support a healthy work-life environment Full offering of health and wellness benefits for you and your family Company paid life insurance and disability plan 401K plan with company matching Competitive Salary with opportunity for advancement Flexible paid time off policy to support a healthy work-life environment Full offering of health and wellness benefits for you and your family Company paid life insurance and disability plan 401K plan with company matching About HealthComp About HealthComp Operating since 1994, we’re a third-party administrator (TPA) committed to providing employers with all the services needed to administer their benefits efficiently resulting in better health outcomes for their employees and higher cost savings for them. We partner with a variety of health providers and technology vendors to ensure a robust offering of medical, dental, vision, COBRA, HIPAA, flexible spending accounts and reference-based pricing, so members can make the most out of their benefits. It’s comprehensive care without the confusion.  HealthComp is an Equal Opportunity Employer.  HealthComp recruiting correspondence will always come from a talent acquisition representative with an official @healthcomp e-mail address. In addition, our representatives will never ask for any form of payment from a new hire or candidate. Please report suspicious activity to hiringsecurity@Healthcomp.com.",2.7,501 to 1000 Employees,1994,Company - Private,Health Care Services & Hospitals,Healthcare,$25 to $100 million (USD)
"Chantilly, VA",Data Engineer,$155K (Employer est.),Momentum,"Welcome to the MOMENTUM Family! MOMENTUM is not just our company name; it is the highest value we deliver to our customers. We are a rapidly growing technology solutions company delivering innovative technology, engineering, and intelligence solutions across the DoD sector. The efforts of our high-capacity team ultimately strengthen our Nation and the warfighter.  Our team is dispersed throughout the US, which means we value the diversity and unique collaboration fostered throughout our team. We work incredibly hard for our customers and believe deeply in our core values. We're a high-energy, high-growth team and we love to win.  Data Engineer The Data Engineer provides engineering support to the data science and software engineering team members. Includes augmenting technical support in the area of data science, data engineering, and systems engineering, and reviewing and providing technical assessments. Provides current system architecture documentation, engineering/web development programming support for program/project requirements defined tasks, data science/data engineering related technical assessments, effective communication with users and managers, and server administration, including hardware and software support to existing servers.  In this role, you will: Design and optimize Data Pipelines using Spark, Hudi, EMR cloud services, and Kubernetes containers Make sure the pedigree and provenance of the data are maintained such that access to data is protected Clean and preprocess data to enable analytic access Collaborate with enterprise working groups to advance the state of data standards Collaborate with the engineering team, data stewards, and mission partners to aid in processes getting actionable value out of the data holdings architects' complex, repeatable ETL Provide Advanced Database Administration support in Oracle, MySQL, MariaDB, MongoDB, Elastic, and others Supports Experience with Targeting using Sponsor Tools, Reverse Engineering Support ad-hoc data analysis requirements defined by the client's Leadership. Reporting solutions will encompass multiple technology platforms used by the client and will drive business process re-design and raining/change management initiatives. Knowledge of business intelligence reporting tools and data visualization software including Tableau. Will work on Data cleaning and transformation efforts in the delivery of CDRLs. Experienced in extracting and aggregating structured and unstructured data. Experienced in data programming languages and tools such as Python and R. Experience with SQL or similar database language. Experience designing and implementing data models to enable, sustain and enhance the value of the information they contain. Strong analytical and critical thinking skills. Ability to work collaboratively and effectively in a team environment.  If you're suitable for this role, you have: Top Secret SCI with FULL SCOPE POLY REQUIRED   To learn more about us, check out our website at www.gomomentum.tech!  MOMENTUM is an EEO/M/F/Veteran/Disabled Employer: We are an equal opportunity employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability status, protected veteran status, or any other characteristic protected by law.  To perform this job successfully, an individual must be able to perform each essential duty satisfactorily. The qualifications listed above are representative of the knowledge, skill, and/or ability required. Reasonable accommodations may be made to enable individuals with disabilities to perform essential functions.  Accommodations: Consistent with the Americans with Disabilities Act (ADA) and Alabama civil rights law, it is the policy of Momentum to provide reasonable accommodation when requested by a qualified applicant or employee with a disability, unless such accommodation would cause an undue hardship. The policy regarding requests for reasonable accommodation applies to all aspects of employment, including the application process. If reasonable accommodation is needed, please include a request when applying. Welcome to the MOMENTUM Family! Welcome to the MOMENTUM Family! MOMENTUM is not just our company name; it is the highest value we deliver to our customers. We are a rapidly growing technology solutions company delivering innovative technology, engineering, and intelligence solutions across the DoD sector. The efforts of our high-capacity team ultimately strengthen our Nation and the warfighter. Our team is dispersed throughout the US, which means we value the diversity and unique collaboration fostered throughout our team. We work incredibly hard for our customers and believe deeply in our core values. We're a high-energy, high-growth team and we love to win. Data Engineer Data Engineer The Data Engineer provides engineering support to the data science and software engineering team members. Includes augmenting technical support in the area of data science, data engineering, and systems engineering, and reviewing and providing technical assessments. Provides current system architecture documentation, engineering/web development programming support for program/project requirements defined tasks, data science/data engineering related technical assessments, effective communication with users and managers, and server administration, including hardware and software support to existing servers. In this role, you will: In this role, you will: Design and optimize Data Pipelines using Spark, Hudi, EMR cloud services, and Kubernetes containers Make sure the pedigree and provenance of the data are maintained such that access to data is protected Clean and preprocess data to enable analytic access Collaborate with enterprise working groups to advance the state of data standards Collaborate with the engineering team, data stewards, and mission partners to aid in processes getting actionable value out of the data holdings architects' complex, repeatable ETL Provide Advanced Database Administration support in Oracle, MySQL, MariaDB, MongoDB, Elastic, and others Supports Experience with Targeting using Sponsor Tools, Reverse Engineering Support ad-hoc data analysis requirements defined by the client's Leadership. Reporting solutions will encompass multiple technology platforms used by the client and will drive business process re-design and raining/change management initiatives. Knowledge of business intelligence reporting tools and data visualization software including Tableau. Will work on Data cleaning and transformation efforts in the delivery of CDRLs. Experienced in extracting and aggregating structured and unstructured data. Experienced in data programming languages and tools such as Python and R. Experience with SQL or similar database language. Experience designing and implementing data models to enable, sustain and enhance the value of the information they contain. Strong analytical and critical thinking skills. Ability to work collaboratively and effectively in a team environment. Design and optimize Data Pipelines using Spark, Hudi, EMR cloud services, and Kubernetes containers Make sure the pedigree and provenance of the data are maintained such that access to data is protected Clean and preprocess data to enable analytic access Collaborate with enterprise working groups to advance the state of data standards Collaborate with the engineering team, data stewards, and mission partners to aid in processes getting actionable value out of the data holdings architects' complex, repeatable ETL Provide Advanced Database Administration support in Oracle, MySQL, MariaDB, MongoDB, Elastic, and others Supports Experience with Targeting using Sponsor Tools, Reverse Engineering Support ad-hoc data analysis requirements defined by the client's Leadership. Reporting solutions will encompass multiple technology platforms used by the client and will drive business process re-design and raining/change management initiatives. Knowledge of business intelligence reporting tools and data visualization software including Tableau. Will work on Data cleaning and transformation efforts in the delivery of CDRLs. Experienced in extracting and aggregating structured and unstructured data. Experienced in data programming languages and tools such as Python and R. Experience with SQL or similar database language. Experience designing and implementing data models to enable, sustain and enhance the value of the information they contain. Strong analytical and critical thinking skills. Ability to work collaboratively and effectively in a team environment. If you're suitable for this role, you have: If you're suitable for this role, you have: Top Secret SCI with FULL SCOPE POLY REQUIRED Top Secret SCI with FULL SCOPE POLY REQUIRED To learn more about us, check out our website at www.gomomentum.tech! To learn more about us, check out our website at www.gomomentum.tech! MOMENTUM is an EEO/M/F/Veteran/Disabled Employer: MOMENTUM is an EEO/M/F/Veteran/Disabled Employer: We are an equal opportunity employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability status, protected veteran status, or any other characteristic protected by law. To perform this job successfully, an individual must be able to perform each essential duty satisfactorily. The qualifications listed above are representative of the knowledge, skill, and/or ability required. Reasonable accommodations may be made to enable individuals with disabilities to perform essential functions. Accommodations: Accommodations: Consistent with the Americans with Disabilities Act (ADA) and Alabama civil rights law, it is the policy of Momentum to provide reasonable accommodation when requested by a qualified applicant or employee with a disability, unless such accommodation would cause an undue hardship. The policy regarding requests for reasonable accommodation applies to all aspects of employment, including the application process. If reasonable accommodation is needed, please include a request when applying.",3.6,501 to 1000 Employees,1987,Company - Public,Advertising & Public Relations,Media & Communication,$100 to $500 million (USD)
"Arlington, VA",Senior Data Engineer (Hybrid),$104K - $148K (Glassdoor est.),PBS Distribution,"POSITION TITLE: Senior Data Engineer (Hybrid) DEPARTMENT: Analytics and Technology LOCATION: Arlington, VA STATUS: Full-time, Exempt MANAGER: Director, Data Engineering PBS Distribution (PBSd)* is a leading distributor of public media content around the world, entertaining audiences across platforms and formats. The company, a joint venture of PBS and GBH Boston, provides premium content through multiple digital channels and video services. PBS Distribution operates six subscription streaming channels — PBS Masterpiece (US and CA), PBS KIDS, PBS Living, PBS Documentaries, and PBS America (U.K.) as well as numerous Free Ad-supported Streaming TV (FAST) Channels in the U.S and U.K. In addition, the company reaches viewers through transactional video-on-demand (TVOD), subscription video-on-demand (SVOD) licensing, Advertising-based Video on Demand (AVOD), DVD and Blu-ray, theatrical releasing, educational platforms, non-theatrical and inflight sales, and serves broadcasters and producers providing program sales and co-production financing. At PBS Distribution, we want to best represent the communities that we serve. The global community is diverse, and we are our best when we have a team that reflects that. PBS Distribution is proud to be an equal opportunity workplace, celebrating, supporting, and thriving when our team is able to show up fully and authentically.  POSITION SUMMARY: The Senior Data Engineer is responsible for the department's data value add work, including developing data models, maintaining a data warehouse and analytics environment, and writing scripts for data integration and analysis. This role will work closely and collaboratively with members of the Analytics and Technology teams to define requirements, mine and analyze data, integrate data from a variety of sources, and deploy high quality data pipelines in support of the analytics needs across the enterprise. They will also support the comprehensive Enterprise Data Catalog and manage other proprietary systems.  RESPONSIBILITIES: Build, operate, and maintain ETL processes that gather data from video partners. Model data pipelines in DBT to prepare data for analysis. Write internal tools to assist with data cleaning and labeling. Adhere to high standards for code quality and testing. Clarify data requirements and priorities with other project teams. WORK EXPERIENCE: 5+ years of experience building backend software or services 3+ years of experience working with large scale data. (1+ TBs, 100M+ rows) EDUCATION/TRAINING: Bachelor’s degree in computer science, Engineering, or related field SKILLS: 5+ years of experience building complex server-side Python applications, processing data with Pandas, optimizing complex SQL queries at scale, and consuming REST APIs 3+ years of experience modeling data in relational and columnar databases, building data pipelines in DBT, managing cloud infrastructure with Terraform, and building internal tools using Django, Streamlit, or an equivalent framework Able to work independently with minimal supervision while adhering to company policies, legal requirements, and customer specifications Strong verbal, written, and interpersonal communication skills PBS Distribution (PBSd) is an equal opportunity employer that is committed to diversity and inclusion in the workplace. We prohibit discrimination and harassment of any kind. PBS Distribution is a for-profit organization with a mission to drive profitable growth by building key partnerships and distributing inspiring content. PBS Distribution is headquartered in Arlington, VA with an additional office In Boston, MA. POSITION TITLE: Senior Data Engineer (Hybrid) POSITION TITLE: DEPARTMENT: Analytics and Technology DEPARTMENT: LOCATION: Arlington, VA LOCATION: STATUS: Full-time, Exempt STATUS: MANAGER: Director, Data Engineering MANAGER: PBS Distribution (PBSd)* is a leading distributor of public media content around the world, entertaining audiences across platforms and formats. The company, a joint venture of PBS and GBH Boston, provides premium content through multiple digital channels and video services. PBS Distribution operates six subscription streaming channels — PBS Masterpiece (US and CA), PBS KIDS, PBS Living, PBS Documentaries, and PBS America (U.K.) as well as numerous Free Ad-supported Streaming TV (FAST) Channels in the U.S and U.K. In addition, the company reaches viewers through transactional video-on-demand (TVOD), subscription video-on-demand (SVOD) licensing, Advertising-based Video on Demand (AVOD), DVD and Blu-ray, theatrical releasing, educational platforms, non-theatrical and inflight sales, and serves broadcasters and producers providing program sales and co-production financing. * At PBS Distribution, we want to best represent the communities that we serve. The global community is diverse, and we are our best when we have a team that reflects that. PBS Distribution is proud to be an equal opportunity workplace, celebrating, supporting, and thriving when our team is able to show up fully and authentically. POSITION SUMMARY: POSITION SUMMARY: The Senior Data Engineer is responsible for the department's data value add work, including developing data models, maintaining a data warehouse and analytics environment, and writing scripts for data integration and analysis. This role will work closely and collaboratively with members of the Analytics and Technology teams to define requirements, mine and analyze data, integrate data from a variety of sources, and deploy high quality data pipelines in support of the analytics needs across the enterprise. They will also support the comprehensive Enterprise Data Catalog and manage other proprietary systems. RESPONSIBILITIES: RESPONSIBILITIES: Build, operate, and maintain ETL processes that gather data from video partners. Model data pipelines in DBT to prepare data for analysis. Write internal tools to assist with data cleaning and labeling. Adhere to high standards for code quality and testing. Clarify data requirements and priorities with other project teams. Build, operate, and maintain ETL processes that gather data from video partners. Model data pipelines in DBT to prepare data for analysis. Write internal tools to assist with data cleaning and labeling. Adhere to high standards for code quality and testing. Clarify data requirements and priorities with other project teams. WORK EXPERIENCE: WORK EXPERIENCE: 5+ years of experience building backend software or services 3+ years of experience working with large scale data. (1+ TBs, 100M+ rows) 5+ years of experience building backend software or services 3+ years of experience working with large scale data. (1+ TBs, 100M+ rows) EDUCATION/TRAINING: EDUCATION/TRAINING: Bachelor’s degree in computer science, Engineering, or related field Bachelor’s degree in computer science, Engineering, or related field SKILLS: SKILLS: 5+ years of experience building complex server-side Python applications, processing data with Pandas, optimizing complex SQL queries at scale, and consuming REST APIs 3+ years of experience modeling data in relational and columnar databases, building data pipelines in DBT, managing cloud infrastructure with Terraform, and building internal tools using Django, Streamlit, or an equivalent framework Able to work independently with minimal supervision while adhering to company policies, legal requirements, and customer specifications Strong verbal, written, and interpersonal communication skills 5+ years of experience building complex server-side Python applications, processing data with Pandas, optimizing complex SQL queries at scale, and consuming REST APIs 3+ years of experience modeling data in relational and columnar databases, building data pipelines in DBT, managing cloud infrastructure with Terraform, and building internal tools using Django, Streamlit, or an equivalent framework Able to work independently with minimal supervision while adhering to company policies, legal requirements, and customer specifications Strong verbal, written, and interpersonal communication skills PBS Distribution (PBSd) is an equal opportunity employer that is committed to diversity and inclusion in the workplace. We prohibit discrimination and harassment of any kind. PBS Distribution is a for-profit organization with a mission to drive profitable growth by building key partnerships and distributing inspiring content. PBS Distribution is headquartered in Arlington, VA with an additional office In Boston, MA. PBS Distribution is a for-profit organization with a mission to drive profitable growth by building key partnerships and distributing inspiring content. PBS Distribution is headquartered in Arlington, VA with an additional office In Boston, MA. PBS Distribution is a for-profit organization with a mission to drive profitable growth by building key partnerships and distributing inspiring content. PBS Distribution is a for-profit organization with a mission to drive profitable growth by building key partnerships and distributing inspiring content. PBS Distribution is headquartered in Arlington, VA with an additional office In Boston, MA. PBS Distribution is headquartered in Arlington, VA with an additional office In Boston, MA.",4.6,51 to 200 Employees,2009,Unknown,Broadcast Media,Media & Communication,Unknown / Non-Applicable
"Fort Gordon, GA",Data Engineer,$74K - $108K (Glassdoor est.),Advanced Technology Leaders Inc,"Job Title: Senior Computer Programmer/Software Engineer (Data Engineer) This position is in support of the Cyber Battle Lab on Fort Gordon, GA. Education and Experience: Required: 1) Experience: a minimum of five (5) years’ of hands-on experience in: TCP/IP, Ethernet, wireless transmission devices, network security, ethical hacking, network and computer system fundamentals, web servers and applications, SQL injection, hacking Wi-Fi and Bluetooth, mobile device security, evasion, cloud technologies and security, and virtualization technologies. 2) A minimum of five (5) years’ of hands-on-experience in utilizing many of the following software applications and simulation environments: HTML, JavaScript, C, C++, Java, Python, PHP, ASP, NETMySQL; Delphi, EXata, QUALNET, Joint Network Emulator (JNE), OneSAF, C++ and C, MS Visual Studio, Visual Basic, SQL Server (current and all previous versions), Transact SQL, Rational Purify, Perl, Systems Tool Kit (STK), Active Directory, DNS, DHCP, Mail Server, Print Server, LDAP, Kerberos, Systems Planning, Engineering, and Evaluation Device (SPEED), Microsoft SharePoint, CO Enhanced Network and Training Simulators (CENTS) and the Naval Research Laboratories Builder model. 3) Proficiencies: possess the knowledge, skills and abilities to build an emulation environment to conduct ethical hacking using IP Schemas, the OSI Model, LANs/WANs, switch, router, firewall, access point, SNMP, ARP, NAT, HTTP, FTP, Telnet, RDP, SSH, POP, SMTP, SSL, and NetBIOS. 4) Be computer literate with emphasis on proficiency with MS Office products (i.e. MS Word, PowerPoint, Excel, and Access). 5) Demonstrated effectiveness in collecting information and accurately representing/visualizing it to technical and non-technical third parties and executive leadership. Desired: 1) Experience and familiarity with Big Data architecture platforms, tools, and operational constructs to include Flink, Spark, Hadoop, Kafka, Power Bi and the Lower Echelon Analytic Platform Tactical (LTAC). 2) Knowledge of containerized data deployment and structure, including Docker, Kubernetes, and OpenShift. 3) Experience and familiarity with High Level Architecture (HLA) system interfaces. 4) Experience and familiarity with cloud analytics suites, including Gabriel Nimbus, Amazon Web Services (AWS), Oracle Cloud Infrastructure Data, or Azure Analytics Services. 5) Knowledge of parallel processing libraries, such as Dask for Python 8) Applied mathematics (e.g. probability and statistics, formal modeling, computational, social sciences). 6) Experience with statistical data analysis (e.g. linear models, multivariate analysis, stochastic, models, sampling methods). Certifications: Certification & Training, Baseline, day 1: IAT Level II or III, IAW DoDM 8570 standards maintained at https://cyber.mil Certification & Training, CE, day 1: None Certification & Training, CE, within 6 months: (1) Microsoft 365 Modern Desktop Administrator Associate, Microsoft 365 Azure Administrator Associate, Microsoft Certified Solutions Associate (MCSA) Windows Server, CompTIA Server+, or equivalent; or (2) Linux: either earn Linux+ CE certification; or complete Linux training. Supervisory Responsibility: None Work Environment: This job operates in a professional classroom and office environment. This role routinely uses standard office equipment. Physical Demands: Stand, bend, and stoop for long periods of time while delivering training. Be able to maneuver in and out of strategic and tactical equipment, and capable of lifting and moving electronic equipment in access of 40lbs in and out of classrooms and labs. The physical demands described here are representative of those that must be met by an employee to successfully perform the essential functions of this job. Position Type and Expected Hours of Work: This is a full-time position. Days and hours of work are Monday through Friday, 8:00 a.m. to 4:00 p.m. Workdays do not include working or conducting business on Federal holidays or when the Government facility is closed due to local or national emergencies, administrative closings, or similar Government directed closings. Travel: No travel is expected for this position. Additional Eligibility Qualifications: N/A Work Authorization/Security Clearance: May be asked to provide proof of US citizenship Must be able to maintain a US Government issued TS clearance with SCI eligibility throughout the life of the contract. AAP/EEO Statement: ATL provides equal employment opportunities to all employees and applicants without regard to race, color, religion, sex/gender, sexual orientation, national origin, age, disability, marital status, genetic information and/or predisposing genetic characteristics, victim of domestic violence status, veteran status, or other protected class status. This policy applies to all terms and conditions of employment, including, but not limited to, hiring, placement, promotion, termination, layoff, recall, transfer, leave of absence, compensation, and training. The Company also prohibits retaliation against any employee who exercises his or her rights under applicable anti-discrimination laws. Veterans with expertise in these areas are highly encouraged to apply. Other Duties: The above job description is not intended to be an all-inclusive list of duties and standards of the position. Incumbents will follow any other instructions, and perform any other related duties, as assigned by their supervisor. Duties, responsibilities and activities may change at any time with or without notice. Benefits: Health benefits to include medical and dental Paid Personal and Vacation Paid Holidays 401(k) Retirement Plan Job Title: Senior Computer Programmer/Software Engineer (Data Engineer) This position is in support of the Cyber Battle Lab on Fort Gordon, GA. Education and Experience: Required: 1) Experience: a minimum of five (5) years’ of hands-on experience in: TCP/IP, Ethernet, wireless transmission devices, network security, ethical hacking, network and computer system fundamentals, web servers and applications, SQL injection, hacking Wi-Fi and Bluetooth, mobile device security, evasion, cloud technologies and security, and virtualization technologies. 2) A minimum of five (5) years’ of hands-on-experience in utilizing many of the following software applications and simulation environments: HTML, JavaScript, C, C++, Java, Python, PHP, ASP, NETMySQL; Delphi, EXata, QUALNET, Joint Network Emulator (JNE), OneSAF, C++ and C, MS Visual Studio, Visual Basic, SQL Server (current and all previous versions), Transact SQL, Rational Purify, Perl, Systems Tool Kit (STK), Active Directory, DNS, DHCP, Mail Server, Print Server, LDAP, Kerberos, Systems Planning, Engineering, and Evaluation Device (SPEED), Microsoft SharePoint, CO Enhanced Network and Training Simulators (CENTS) and the Naval Research Laboratories Builder model. 3) Proficiencies: possess the knowledge, skills and abilities to build an emulation environment to conduct ethical hacking using IP Schemas, the OSI Model, LANs/WANs, switch, router, firewall, access point, SNMP, ARP, NAT, HTTP, FTP, Telnet, RDP, SSH, POP, SMTP, SSL, and NetBIOS. 4) Be computer literate with emphasis on proficiency with MS Office products (i.e. MS Word, PowerPoint, Excel, and Access). 5) Demonstrated effectiveness in collecting information and accurately representing/visualizing it to technical and non-technical third parties and executive leadership. Desired: 1) Experience and familiarity with Big Data architecture platforms, tools, and operational constructs to include Flink, Spark, Hadoop, Kafka, Power Bi and the Lower Echelon Analytic Platform Tactical (LTAC). 2) Knowledge of containerized data deployment and structure, including Docker, Kubernetes, and OpenShift. 3) Experience and familiarity with High Level Architecture (HLA) system interfaces. 4) Experience and familiarity with cloud analytics suites, including Gabriel Nimbus, Amazon Web Services (AWS), Oracle Cloud Infrastructure Data, or Azure Analytics Services. 5) Knowledge of parallel processing libraries, such as Dask for Python 8) Applied mathematics (e.g. probability and statistics, formal modeling, computational, social sciences). 6) Experience with statistical data analysis (e.g. linear models, multivariate analysis, stochastic, models, sampling methods). Certifications: Certification & Training, Baseline, day 1: IAT Level II or III, IAW DoDM 8570 standards maintained at https://cyber.mil Certification & Training, CE, day 1: None Certification & Training, CE, within 6 months: (1) Microsoft 365 Modern Desktop Administrator Associate, Microsoft 365 Azure Administrator Associate, Microsoft Certified Solutions Associate (MCSA) Windows Server, CompTIA Server+, or equivalent; or (2) Linux: either earn Linux+ CE certification; or complete Linux training. Supervisory Responsibility: None Work Environment: This job operates in a professional classroom and office environment. This role routinely uses standard office equipment. Physical Demands: Stand, bend, and stoop for long periods of time while delivering training. Be able to maneuver in and out of strategic and tactical equipment, and capable of lifting and moving electronic equipment in access of 40lbs in and out of classrooms and labs. The physical demands described here are representative of those that must be met by an employee to successfully perform the essential functions of this job. Position Type and Expected Hours of Work: This is a full-time position. Days and hours of work are Monday through Friday, 8:00 a.m. to 4:00 p.m. Workdays do not include working or conducting business on Federal holidays or when the Government facility is closed due to local or national emergencies, administrative closings, or similar Government directed closings. Travel: No travel is expected for this position. Additional Eligibility Qualifications: N/A Work Authorization/Security Clearance: May be asked to provide proof of US citizenship Must be able to maintain a US Government issued TS clearance with SCI eligibility throughout the life of the contract. AAP/EEO Statement: ATL provides equal employment opportunities to all employees and applicants without regard to race, color, religion, sex/gender, sexual orientation, national origin, age, disability, marital status, genetic information and/or predisposing genetic characteristics, victim of domestic violence status, veteran status, or other protected class status. This policy applies to all terms and conditions of employment, including, but not limited to, hiring, placement, promotion, termination, layoff, recall, transfer, leave of absence, compensation, and training. The Company also prohibits retaliation against any employee who exercises his or her rights under applicable anti-discrimination laws. Veterans with expertise in these areas are highly encouraged to apply. Other Duties: The above job description is not intended to be an all-inclusive list of duties and standards of the position. Incumbents will follow any other instructions, and perform any other related duties, as assigned by their supervisor. Duties, responsibilities and activities may change at any time with or without notice. Benefits: Health benefits to include medical and dental Paid Personal and Vacation Paid Holidays 401(k) Retirement Plan Job Title: Senior Computer Programmer/Software Engineer (Data Engineer) This position is in support of the Cyber Battle Lab on Fort Gordon, GA. Education and Experience: Required: 1) Experience: a minimum of five (5) years’ of hands-on experience in: TCP/IP, Ethernet, wireless transmission devices, network security, ethical hacking, network and computer system fundamentals, web servers and applications, SQL injection, hacking Wi-Fi and Bluetooth, mobile device security, evasion, cloud technologies and security, and virtualization technologies. 2) A minimum of five (5) years’ of hands-on-experience in utilizing many of the following software applications and simulation environments: HTML, JavaScript, C, C++, Java, Python, PHP, ASP, NETMySQL; Delphi, EXata, QUALNET, Joint Network Emulator (JNE), OneSAF, C++ and C, MS Visual Studio, Visual Basic, SQL Server (current and all previous versions), Transact SQL, Rational Purify, Perl, Systems Tool Kit (STK), Active Directory, DNS, DHCP, Mail Server, Print Server, LDAP, Kerberos, Systems Planning, Engineering, and Evaluation Device (SPEED), Microsoft SharePoint, CO Enhanced Network and Training Simulators (CENTS) and the Naval Research Laboratories Builder model. 3) Proficiencies: possess the knowledge, skills and abilities to build an emulation environment to conduct ethical hacking using IP Schemas, the OSI Model, LANs/WANs, switch, router, firewall, access point, SNMP, ARP, NAT, HTTP, FTP, Telnet, RDP, SSH, POP, SMTP, SSL, and NetBIOS. 4) Be computer literate with emphasis on proficiency with MS Office products (i.e. MS Word, PowerPoint, Excel, and Access). 5) Demonstrated effectiveness in collecting information and accurately representing/visualizing it to technical and non-technical third parties and executive leadership. Desired: 1) Experience and familiarity with Big Data architecture platforms, tools, and operational constructs to include Flink, Spark, Hadoop, Kafka, Power Bi and the Lower Echelon Analytic Platform Tactical (LTAC). 2) Knowledge of containerized data deployment and structure, including Docker, Kubernetes, and OpenShift. 3) Experience and familiarity with High Level Architecture (HLA) system interfaces. 4) Experience and familiarity with cloud analytics suites, including Gabriel Nimbus, Amazon Web Services (AWS), Oracle Cloud Infrastructure Data, or Azure Analytics Services. 5) Knowledge of parallel processing libraries, such as Dask for Python 8) Applied mathematics (e.g. probability and statistics, formal modeling, computational, social sciences). 6) Experience with statistical data analysis (e.g. linear models, multivariate analysis, stochastic, models, sampling methods). Certifications: Certification & Training, Baseline, day 1: IAT Level II or III, IAW DoDM 8570 standards maintained at https://cyber.mil Certification & Training, CE, day 1: None Certification & Training, CE, within 6 months: (1) Microsoft 365 Modern Desktop Administrator Associate, Microsoft 365 Azure Administrator Associate, Microsoft Certified Solutions Associate (MCSA) Windows Server, CompTIA Server+, or equivalent; or (2) Linux: either earn Linux+ CE certification; or complete Linux training. Supervisory Responsibility: None Work Environment: This job operates in a professional classroom and office environment. This role routinely uses standard office equipment. Physical Demands: Stand, bend, and stoop for long periods of time while delivering training. Be able to maneuver in and out of strategic and tactical equipment, and capable of lifting and moving electronic equipment in access of 40lbs in and out of classrooms and labs. The physical demands described here are representative of those that must be met by an employee to successfully perform the essential functions of this job. Position Type and Expected Hours of Work: This is a full-time position. Days and hours of work are Monday through Friday, 8:00 a.m. to 4:00 p.m. Workdays do not include working or conducting business on Federal holidays or when the Government facility is closed due to local or national emergencies, administrative closings, or similar Government directed closings. Travel: No travel is expected for this position. Additional Eligibility Qualifications: N/A Work Authorization/Security Clearance: May be asked to provide proof of US citizenship Must be able to maintain a US Government issued TS clearance with SCI eligibility throughout the life of the contract. AAP/EEO Statement: ATL provides equal employment opportunities to all employees and applicants without regard to race, color, religion, sex/gender, sexual orientation, national origin, age, disability, marital status, genetic information and/or predisposing genetic characteristics, victim of domestic violence status, veteran status, or other protected class status. This policy applies to all terms and conditions of employment, including, but not limited to, hiring, placement, promotion, termination, layoff, recall, transfer, leave of absence, compensation, and training. The Company also prohibits retaliation against any employee who exercises his or her rights under applicable anti-discrimination laws. Veterans with expertise in these areas are highly encouraged to apply. Other Duties: The above job description is not intended to be an all-inclusive list of duties and standards of the position. Incumbents will follow any other instructions, and perform any other related duties, as assigned by their supervisor. Duties, responsibilities and activities may change at any time with or without notice. Benefits: Health benefits to include medical and dental Paid Personal and Vacation Paid Holidays 401(k) Retirement Plan Job Title: Senior Computer Programmer/Software Engineer (Data Engineer) This position is in support of the Cyber Battle Lab on Fort Gordon, GA. Education and Experience: Required: 1) Experience: a minimum of five (5) years’ of hands-on experience in: TCP/IP, Ethernet, wireless transmission devices, network security, ethical hacking, network and computer system fundamentals, web servers and applications, SQL injection, hacking Wi-Fi and Bluetooth, mobile device security, evasion, cloud technologies and security, and virtualization technologies. 2) A minimum of five (5) years’ of hands-on-experience in utilizing many of the following software applications and simulation environments: HTML, JavaScript, C, C++, Java, Python, PHP, ASP, NETMySQL; Delphi, EXata, QUALNET, Joint Network Emulator (JNE), OneSAF, C++ and C, MS Visual Studio, Visual Basic, SQL Server (current and all previous versions), Transact SQL, Rational Purify, Perl, Systems Tool Kit (STK), Active Directory, DNS, DHCP, Mail Server, Print Server, LDAP, Kerberos, Systems Planning, Engineering, and Evaluation Device (SPEED), Microsoft SharePoint, CO Enhanced Network and Training Simulators (CENTS) and the Naval Research Laboratories Builder model. 3) Proficiencies: possess the knowledge, skills and abilities to build an emulation environment to conduct ethical hacking using IP Schemas, the OSI Model, LANs/WANs, switch, router, firewall, access point, SNMP, ARP, NAT, HTTP, FTP, Telnet, RDP, SSH, POP, SMTP, SSL, and NetBIOS. 4) Be computer literate with emphasis on proficiency with MS Office products (i.e. MS Word, PowerPoint, Excel, and Access). 5) Demonstrated effectiveness in collecting information and accurately representing/visualizing it to technical and non-technical third parties and executive leadership. Desired: 1) Experience and familiarity with Big Data architecture platforms, tools, and operational constructs to include Flink, Spark, Hadoop, Kafka, Power Bi and the Lower Echelon Analytic Platform Tactical (LTAC). 2) Knowledge of containerized data deployment and structure, including Docker, Kubernetes, and OpenShift. 3) Experience and familiarity with High Level Architecture (HLA) system interfaces. 4) Experience and familiarity with cloud analytics suites, including Gabriel Nimbus, Amazon Web Services (AWS), Oracle Cloud Infrastructure Data, or Azure Analytics Services. 5) Knowledge of parallel processing libraries, such as Dask for Python 8) Applied mathematics (e.g. probability and statistics, formal modeling, computational, social sciences). 6) Experience with statistical data analysis (e.g. linear models, multivariate analysis, stochastic, models, sampling methods). Certifications: Certification & Training, Baseline, day 1: IAT Level II or III, IAW DoDM 8570 standards maintained at https://cyber.mil Certification & Training, CE, day 1: None Certification & Training, CE, within 6 months: (1) Microsoft 365 Modern Desktop Administrator Associate, Microsoft 365 Azure Administrator Associate, Microsoft Certified Solutions Associate (MCSA) Windows Server, CompTIA Server+, or equivalent; or (2) Linux: either earn Linux+ CE certification; or complete Linux training. Supervisory Responsibility: None Work Environment: This job operates in a professional classroom and office environment. This role routinely uses standard office equipment. Physical Demands: Stand, bend, and stoop for long periods of time while delivering training. Be able to maneuver in and out of strategic and tactical equipment, and capable of lifting and moving electronic equipment in access of 40lbs in and out of classrooms and labs. The physical demands described here are representative of those that must be met by an employee to successfully perform the essential functions of this job. Position Type and Expected Hours of Work: This is a full-time position. Days and hours of work are Monday through Friday, 8:00 a.m. to 4:00 p.m. Workdays do not include working or conducting business on Federal holidays or when the Government facility is closed due to local or national emergencies, administrative closings, or similar Government directed closings. Travel: No travel is expected for this position. Additional Eligibility Qualifications: N/A Work Authorization/Security Clearance: May be asked to provide proof of US citizenship Must be able to maintain a US Government issued TS clearance with SCI eligibility throughout the life of the contract. AAP/EEO Statement: ATL provides equal employment opportunities to all employees and applicants without regard to race, color, religion, sex/gender, sexual orientation, national origin, age, disability, marital status, genetic information and/or predisposing genetic characteristics, victim of domestic violence status, veteran status, or other protected class status. This policy applies to all terms and conditions of employment, including, but not limited to, hiring, placement, promotion, termination, layoff, recall, transfer, leave of absence, compensation, and training. The Company also prohibits retaliation against any employee who exercises his or her rights under applicable anti-discrimination laws. Veterans with expertise in these areas are highly encouraged to apply. Other Duties: The above job description is not intended to be an all-inclusive list of duties and standards of the position. Incumbents will follow any other instructions, and perform any other related duties, as assigned by their supervisor. Duties, responsibilities and activities may change at any time with or without notice. Benefits: Health benefits to include medical and dental Paid Personal and Vacation Paid Holidays 401(k) Retirement Plan Job Title: Senior Computer Programmer/Software Engineer (Data Engineer) This position is in support of the Cyber Battle Lab on Fort Gordon, GA. Education and Experience: Required: 1) Experience: a minimum of five (5) years’ of hands-on experience in: TCP/IP, Ethernet, wireless transmission devices, network security, ethical hacking, network and computer system fundamentals, web servers and applications, SQL injection, hacking Wi-Fi and Bluetooth, mobile device security, evasion, cloud technologies and security, and virtualization technologies. 2) A minimum of five (5) years’ of hands-on-experience in utilizing many of the following software applications and simulation environments: HTML, JavaScript, C, C++, Java, Python, PHP, ASP, NETMySQL; Delphi, EXata, QUALNET, Joint Network Emulator (JNE), OneSAF, C++ and C, MS Visual Studio, Visual Basic, SQL Server (current and all previous versions), Transact SQL, Rational Purify, Perl, Systems Tool Kit (STK), Active Directory, DNS, DHCP, Mail Server, Print Server, LDAP, Kerberos, Systems Planning, Engineering, and Evaluation Device (SPEED), Microsoft SharePoint, CO Enhanced Network and Training Simulators (CENTS) and the Naval Research Laboratories Builder model. 3) Proficiencies: possess the knowledge, skills and abilities to build an emulation environment to conduct ethical hacking using IP Schemas, the OSI Model, LANs/WANs, switch, router, firewall, access point, SNMP, ARP, NAT, HTTP, FTP, Telnet, RDP, SSH, POP, SMTP, SSL, and NetBIOS. 4) Be computer literate with emphasis on proficiency with MS Office products (i.e. MS Word, PowerPoint, Excel, and Access). 5) Demonstrated effectiveness in collecting information and accurately representing/visualizing it to technical and non-technical third parties and executive leadership. Desired: 1) Experience and familiarity with Big Data architecture platforms, tools, and operational constructs to include Flink, Spark, Hadoop, Kafka, Power Bi and the Lower Echelon Analytic Platform Tactical (LTAC). 2) Knowledge of containerized data deployment and structure, including Docker, Kubernetes, and OpenShift. 3) Experience and familiarity with High Level Architecture (HLA) system interfaces. 4) Experience and familiarity with cloud analytics suites, including Gabriel Nimbus, Amazon Web Services (AWS), Oracle Cloud Infrastructure Data, or Azure Analytics Services. 5) Knowledge of parallel processing libraries, such as Dask for Python 8) Applied mathematics (e.g. probability and statistics, formal modeling, computational, social sciences). 6) Experience with statistical data analysis (e.g. linear models, multivariate analysis, stochastic, models, sampling methods). Certifications: Certification & Training, Baseline, day 1: IAT Level II or III, IAW DoDM 8570 standards maintained at https://cyber.mil Certification & Training, CE, day 1: None Certification & Training, CE, within 6 months: (1) Microsoft 365 Modern Desktop Administrator Associate, Microsoft 365 Azure Administrator Associate, Microsoft Certified Solutions Associate (MCSA) Windows Server, CompTIA Server+, or equivalent; or (2) Linux: either earn Linux+ CE certification; or complete Linux training. Supervisory Responsibility: None Work Environment: This job operates in a professional classroom and office environment. This role routinely uses standard office equipment. Physical Demands: Stand, bend, and stoop for long periods of time while delivering training. Be able to maneuver in and out of strategic and tactical equipment, and capable of lifting and moving electronic equipment in access of 40lbs in and out of classrooms and labs. The physical demands described here are representative of those that must be met by an employee to successfully perform the essential functions of this job. Position Type and Expected Hours of Work: This is a full-time position. Days and hours of work are Monday through Friday, 8:00 a.m. to 4:00 p.m. Workdays do not include working or conducting business on Federal holidays or when the Government facility is closed due to local or national emergencies, administrative closings, or similar Government directed closings. Travel: No travel is expected for this position. Additional Eligibility Qualifications: N/A Work Authorization/Security Clearance: May be asked to provide proof of US citizenship Must be able to maintain a US Government issued TS clearance with SCI eligibility throughout the life of the contract. AAP/EEO Statement: ATL provides equal employment opportunities to all employees and applicants without regard to race, color, religion, sex/gender, sexual orientation, national origin, age, disability, marital status, genetic information and/or predisposing genetic characteristics, victim of domestic violence status, veteran status, or other protected class status. This policy applies to all terms and conditions of employment, including, but not limited to, hiring, placement, promotion, termination, layoff, recall, transfer, leave of absence, compensation, and training. The Company also prohibits retaliation against any employee who exercises his or her rights under applicable anti-discrimination laws. Veterans with expertise in these areas are highly encouraged to apply. Other Duties: The above job description is not intended to be an all-inclusive list of duties and standards of the position. Incumbents will follow any other instructions, and perform any other related duties, as assigned by their supervisor. Duties, responsibilities and activities may change at any time with or without notice. Benefits: Health benefits to include medical and dental Paid Personal and Vacation Paid Holidays 401(k) Retirement Plan Job Title: Senior Computer Programmer/Software Engineer (Data Engineer) This position is in support of the Cyber Battle Lab on Fort Gordon, GA. Education and Experience: Required: 1) Experience: a minimum of five (5) years’ of hands-on experience in: TCP/IP, Ethernet, wireless transmission devices, network security, ethical hacking, network and computer system fundamentals, web servers and applications, SQL injection, hacking Wi-Fi and Bluetooth, mobile device security, evasion, cloud technologies and security, and virtualization technologies. 2) A minimum of five (5) years’ of hands-on-experience in utilizing many of the following software applications and simulation environments: HTML, JavaScript, C, C++, Java, Python, PHP, ASP, NETMySQL; Delphi, EXata, QUALNET, Joint Network Emulator (JNE), OneSAF, C++ and C, MS Visual Studio, Visual Basic, SQL Server (current and all previous versions), Transact SQL, Rational Purify, Perl, Systems Tool Kit (STK), Active Directory, DNS, DHCP, Mail Server, Print Server, LDAP, Kerberos, Systems Planning, Engineering, and Evaluation Device (SPEED), Microsoft SharePoint, CO Enhanced Network and Training Simulators (CENTS) and the Naval Research Laboratories Builder model. 3) Proficiencies: possess the knowledge, skills and abilities to build an emulation environment to conduct ethical hacking using IP Schemas, the OSI Model, LANs/WANs, switch, router, firewall, access point, SNMP, ARP, NAT, HTTP, FTP, Telnet, RDP, SSH, POP, SMTP, SSL, and NetBIOS. 4) Be computer literate with emphasis on proficiency with MS Office products (i.e. MS Word, PowerPoint, Excel, and Access). 5) Demonstrated effectiveness in collecting information and accurately representing/visualizing it to technical and non-technical third parties and executive leadership. Desired: 1) Experience and familiarity with Big Data architecture platforms, tools, and operational constructs to include Flink, Spark, Hadoop, Kafka, Power Bi and the Lower Echelon Analytic Platform Tactical (LTAC). 2) Knowledge of containerized data deployment and structure, including Docker, Kubernetes, and OpenShift. 3) Experience and familiarity with High Level Architecture (HLA) system interfaces. 4) Experience and familiarity with cloud analytics suites, including Gabriel Nimbus, Amazon Web Services (AWS), Oracle Cloud Infrastructure Data, or Azure Analytics Services. 5) Knowledge of parallel processing libraries, such as Dask for Python 8) Applied mathematics (e.g. probability and statistics, formal modeling, computational, social sciences). 6) Experience with statistical data analysis (e.g. linear models, multivariate analysis, stochastic, models, sampling methods). Certifications: Certification & Training, Baseline, day 1: IAT Level II or III, IAW DoDM 8570 standards maintained at https://cyber.mil Certification & Training, CE, day 1: None Certification & Training, CE, within 6 months: (1) Microsoft 365 Modern Desktop Administrator Associate, Microsoft 365 Azure Administrator Associate, Microsoft Certified Solutions Associate (MCSA) Windows Server, CompTIA Server+, or equivalent; or (2) Linux: either earn Linux+ CE certification; or complete Linux training. Supervisory Responsibility: None Work Environment: This job operates in a professional classroom and office environment. This role routinely uses standard office equipment. Physical Demands: Stand, bend, and stoop for long periods of time while delivering training. Be able to maneuver in and out of strategic and tactical equipment, and capable of lifting and moving electronic equipment in access of 40lbs in and out of classrooms and labs. The physical demands described here are representative of those that must be met by an employee to successfully perform the essential functions of this job. Position Type and Expected Hours of Work: This is a full-time position. Days and hours of work are Monday through Friday, 8:00 a.m. to 4:00 p.m. Workdays do not include working or conducting business on Federal holidays or when the Government facility is closed due to local or national emergencies, administrative closings, or similar Government directed closings. Travel: No travel is expected for this position. Additional Eligibility Qualifications: N/A Work Authorization/Security Clearance: May be asked to provide proof of US citizenship Must be able to maintain a US Government issued TS clearance with SCI eligibility throughout the life of the contract. AAP/EEO Statement: ATL provides equal employment opportunities to all employees and applicants without regard to race, color, religion, sex/gender, sexual orientation, national origin, age, disability, marital status, genetic information and/or predisposing genetic characteristics, victim of domestic violence status, veteran status, or other protected class status. This policy applies to all terms and conditions of employment, including, but not limited to, hiring, placement, promotion, termination, layoff, recall, transfer, leave of absence, compensation, and training. The Company also prohibits retaliation against any employee who exercises his or her rights under applicable anti-discrimination laws. Veterans with expertise in these areas are highly encouraged to apply. Other Duties: The above job description is not intended to be an all-inclusive list of duties and standards of the position. Incumbents will follow any other instructions, and perform any other related duties, as assigned by their supervisor. Duties, responsibilities and activities may change at any time with or without notice. Benefits: Health benefits to include medical and dental Paid Personal and Vacation Paid Holidays 401(k) Retirement Plan Job Title: Senior Computer Programmer/Software Engineer (Data Engineer) Job Title: This position is in support of the Cyber Battle Lab on Fort Gordon, GA. Education and Experience: Education and Experience: Required: Required: 1) Experience: a minimum of five (5) years’ of hands-on experience in: TCP/IP, Ethernet, wireless transmission devices, network security, ethical hacking, network and computer system fundamentals, web servers and applications, SQL injection, hacking Wi-Fi and Bluetooth, mobile device security, evasion, cloud technologies and security, and virtualization technologies. 2) A minimum of five (5) years’ of hands-on-experience in utilizing many of the following software applications and simulation environments: HTML, JavaScript, C, C++, Java, Python, PHP, ASP, NETMySQL; Delphi, EXata, QUALNET, Joint Network Emulator (JNE), OneSAF, C++ and C, MS Visual Studio, Visual Basic, SQL Server (current and all previous versions), Transact SQL, Rational Purify, Perl, Systems Tool Kit (STK), Active Directory, DNS, DHCP, Mail Server, Print Server, LDAP, Kerberos, Systems Planning, Engineering, and Evaluation Device (SPEED), Microsoft SharePoint, CO Enhanced Network and Training Simulators (CENTS) and the Naval Research Laboratories Builder model. 3) Proficiencies: possess the knowledge, skills and abilities to build an emulation environment to conduct ethical hacking using IP Schemas, the OSI Model, LANs/WANs, switch, router, firewall, access point, SNMP, ARP, NAT, HTTP, FTP, Telnet, RDP, SSH, POP, SMTP, SSL, and NetBIOS. 4) Be computer literate with emphasis on proficiency with MS Office products (i.e. MS Word, PowerPoint, Excel, and Access). 5) Demonstrated effectiveness in collecting information and accurately representing/visualizing it to technical and non-technical third parties and executive leadership. Desired: Desired: 1) Experience and familiarity with Big Data architecture platforms, tools, and operational constructs to include Flink, Spark, Hadoop, Kafka, Power Bi and the Lower Echelon Analytic Platform Tactical (LTAC). 2) Knowledge of containerized data deployment and structure, including Docker, Kubernetes, and OpenShift. 3) Experience and familiarity with High Level Architecture (HLA) system interfaces. 4) Experience and familiarity with cloud analytics suites, including Gabriel Nimbus, Amazon Web Services (AWS), Oracle Cloud Infrastructure Data, or Azure Analytics Services. 5) Knowledge of parallel processing libraries, such as Dask for Python 8) Applied mathematics (e.g. probability and statistics, formal modeling, computational, social sciences). 6) Experience with statistical data analysis (e.g. linear models, multivariate analysis, stochastic, models, sampling methods). Certifications: Certifications: Certification & Training, Baseline, day 1: IAT Level II or III, IAW DoDM 8570 standards maintained at https://cyber.mil Certification & Training, CE, day 1: None Certification & Training, CE, within 6 months: (1) Microsoft 365 Modern Desktop Administrator Associate, Microsoft 365 Azure Administrator Associate, Microsoft Certified Solutions Associate (MCSA) Windows Server, CompTIA Server+, or equivalent; or (2) Linux: either earn Linux+ CE certification; or complete Linux training. Certification & Training, Baseline, day 1: IAT Level II or III, IAW DoDM 8570 standards maintained at https://cyber.mil Certification & Training, CE, day 1: None Certification & Training, CE, within 6 months: (1) Microsoft 365 Modern Desktop Administrator Associate, Microsoft 365 Azure Administrator Associate, Microsoft Certified Solutions Associate (MCSA) Windows Server, CompTIA Server+, or equivalent; or (2) Linux: either earn Linux+ CE certification; or complete Linux training. Supervisory Responsibility: None Supervisory Responsibility: Work Environment: This job operates in a professional classroom and office environment. This role routinely uses standard office equipment. Work Environment: Physical Demands: Physical Demands: Stand, bend, and stoop for long periods of time while delivering training. Be able to maneuver in and out of strategic and tactical equipment, and capable of lifting and moving electronic equipment in access of 40lbs in and out of classrooms and labs. Stand, bend, and stoop for long periods of time while delivering training. Be able to maneuver in and out of strategic and tactical equipment, and capable of lifting and moving electronic equipment in access of 40lbs in and out of classrooms and labs. The physical demands described here are representative of those that must be met by an employee to successfully perform the essential functions of this job. The physical demands described here are representative of those that must be met by an employee to successfully perform the essential functions of this job. Position Type and Expected Hours of Work: Position Type and Expected Hours of Work: This is a full-time position. Days and hours of work are Monday through Friday, 8:00 a.m. to 4:00 p.m. Workdays do not include working or conducting business on Federal holidays or when the Government facility is closed due to local or national emergencies, administrative closings, or similar Government directed closings. Travel: No travel is expected for this position. Travel: Additional Eligibility Qualifications: N/A Additional Eligibility Qualifications: Work Authorization/Security Clearance: Work Authorization/Security Clearance: May be asked to provide proof of US citizenship Must be able to maintain a US Government issued TS clearance with SCI eligibility throughout the life of the contract. May be asked to provide proof of US citizenship Must be able to maintain a US Government issued TS clearance with SCI eligibility throughout the life of the contract. AAP/EEO Statement: AAP/EEO Statement: ATL provides equal employment opportunities to all employees and applicants without regard to race, color, religion, sex/gender, sexual orientation, national origin, age, disability, marital status, genetic information and/or predisposing genetic characteristics, victim of domestic violence status, veteran status, or other protected class status. This policy applies to all terms and conditions of employment, including, but not limited to, hiring, placement, promotion, termination, layoff, recall, transfer, leave of absence, compensation, and training. The Company also prohibits retaliation against any employee who exercises his or her rights under applicable anti-discrimination laws. Veterans with expertise in these areas are highly encouraged to apply. Other Duties: Other Duties: The above job description is not intended to be an all-inclusive list of duties and standards of the position. The above job description is not intended to be an all-inclusive list of duties and standards of the position. Incumbents will follow any other instructions, and perform any other related duties, as assigned by their supervisor. Duties, responsibilities and activities may change at any time with or without notice. Incumbents will follow any other instructions, and perform any other related duties, as assigned by their supervisor. Duties, responsibilities and activities may change at any time with or without notice. Benefits: Benefits: Health benefits to include medical and dental Paid Personal and Vacation Paid Holidays 401(k) Retirement Plan Health benefits to include medical and dental Paid Personal and Vacation Paid Holidays 401(k) Retirement Plan",4.8,1 to 50 Employees,N/A,Company - Private,N/A,N/A,Unknown / Non-Applicable
Remote,Data Engineer 2,N/A,MyFitnessPal,"At MyFitnessPal, our vision is to be the global catalyst for every ""body"" to achieve their healthy. We believe good health starts with what you eat. We provide the tools and resources to reach your fitness goals. We are looking for a Data Engineer to join the MyFitnessPal Data Engineering team. Our users rely on MyFitnessPal to power their health and fitness journeys every day. As a member of our MyFitnessPal Engineering team, you'll have the opportunity to positively impact those users with your expertise in the backend systems that drive the MyFitnessPal ecosystem. In addition to technical expertise, you'll find that your teammates value collaboration, mentorship, and inclusive environments. What you'll be doing: Build and maintain pipelines that provide data critical to operational reporting and data-driven initiatives Integrate with services and systems across the MyFitnessPal engineering teams Evaluate and improve existing pipelines, data models, and processes to provide more robust solutions that allow others to move quickly and efficiently Support resolution of production issues across the Data Platform stack Engage in code-reviews, working with teammates to learn and grow Live our core values in all you do: Be Kind and Care Live Good Health Be Data-Inspired Champion Change Leave it Better than You Found It Make It Happen Qualifications to be successful in this role: 2-4 years of data engineering experience working with large datasets and complex data environments, processes, and associated solutions Experience with a variety of data stores (e.g. Snowflake, MySQL, MongoDB, DynamoDB, etc.) Experience with data orchestration tooling (e.g. Airflow, Data Factory, etc.) Experience with development languages (e.g. Python, SQL, Scala, etc.) Understanding of data modeling for analysis by business intelligence or data science teams Experience with a variety of API design patterns, such as REST Experience developing validation and data integrity frameworks Triaging and debugging production data issues Experience building high volume data pipelines for downstream analysis supporting operational indicators Experience with data at scale Worked alongside client teams to support integration efforts Familiarity with AWS and/or other cloud computing platforms  Please consider applying even if you don't meet 100% of the qualifications. Research shows you can still be considered for a position if you meet some of the requirements. At MyFitnessPal, we're building a fitness product for everyone and believe our team should reflect that. We encourage people of different backgrounds, experiences, abilities, and perspectives to apply. Exciting Full-Time Employee Benefits, Perks and Culture Embrace the Freedom: Be a digital nomad, work from anywhere we have operations within the continental U.S. Office Vibes: If you prefer working in an office, we've got you covered, our HQ is in vibrant Austin, TX. Face-to-Face Connections: We value personal connections. Enjoy opportunities to meet and connect with your team members in person to help forge meaningful relationships that extend beyond the virtual realm. Teams meet as often as needed and all of MyFitnessPal gathers annually. Flexibility At Its Best: Achieve the work-life balance you deserve. Enjoy a flexible time-off policy and work on your own terms with our Responsible Time Off benefit. Give Back: Use your volunteer days off to support what matters most to you. Each full time teammate receives 2 days per calendar year to give back to their community through service. Mentorship Program: Take control of your career through our mentorship program where, if you'd like, you will be matched with a teammate who can help you scale your skills and propel your growth. Family-Friendly Support: Embrace the journey with confidence and care. Enjoy our paid maternity and paternity leave, to provide time to balance family responsibilities with your career and take the time needed to strengthen family relationships. We understand the complexities of starting or expanding a family, which is why we provide best-in-class comprehensive assistance for fertility-related matters. Wellness Comes First: Live Good Health is one of our core values. Receive a monthly Wellness Allowance, empowering you to focus on your physical and mental well-being by choosing from a range of wellness initiatives, including dedicated mental health days. Celebrate Greatness: Your hard work deserves recognition! Our reward and recognition platform empowers peers to acknowledge and reward each other for the exceptional contributions they make. Elevate Your Health & Fitness: Get access to MyFitnessPal Premium, allowing you to take your fitness, health and wellness journey to new heights. Unlock Your Potential: Access our virtual learning and development library, and participate in training opportunities to continuously grow and enhance your skills. Championing Inclusion: Our dedicated DEI Committee actively fosters a diverse and inclusive workplace by setting actionable goals and evaluating progress across the organization. Healthcare Matters: Your well-being is our priority. Take advantage of our competitive medical, dental, and vision benefits that cater to your holistic healthcare needs. Feel secure and supported on your wellness journey. Secure Your Future: Benefit from our retirement savings program, giving you peace of mind for your financial goals. Reach them sooner with MyFitnessPal's competitive employer match.  At MyFitnessPal, our mission is to enable people to make healthy choices. And it wouldn't be possible without our team. We celebrate the unique POV that each person brings to the table and believe in a collaborative and inclusive environment. As an equal opportunity employer, we prohibit any unlawful discrimination on the basis of race, religion, military or veteran status, sex, gender, marital status, gender identity or expression, sexual orientation, national origin, age, or disability. These are our guiding ideologies and apply across all aspects of employment.  MyFitnessPal participates in E-Verify. At MyFitnessPal, our vision is to be the global catalyst for every ""body"" to achieve their healthy. We believe good health starts with what you eat. We provide the tools and resources to reach your fitness goals. We are looking for a Data Engineer to join the MyFitnessPal Data Engineering team. Our users rely on MyFitnessPal to power their health and fitness journeys every day. As a member of our MyFitnessPal Engineering team, you'll have the opportunity to positively impact those users with your expertise in the backend systems that drive the MyFitnessPal ecosystem. In addition to technical expertise, you'll find that your teammates value collaboration, mentorship, and inclusive environments. What you'll be doing: What you'll be doing: Build and maintain pipelines that provide data critical to operational reporting and data-driven initiatives Integrate with services and systems across the MyFitnessPal engineering teams Evaluate and improve existing pipelines, data models, and processes to provide more robust solutions that allow others to move quickly and efficiently Support resolution of production issues across the Data Platform stack Engage in code-reviews, working with teammates to learn and grow Live our core values in all you do: Be Kind and Care Live Good Health Be Data-Inspired Champion Change Leave it Better than You Found It Make It Happen Build and maintain pipelines that provide data critical to operational reporting and data-driven initiatives Integrate with services and systems across the MyFitnessPal engineering teams Evaluate and improve existing pipelines, data models, and processes to provide more robust solutions that allow others to move quickly and efficiently Support resolution of production issues across the Data Platform stack Engage in code-reviews, working with teammates to learn and grow Live our core values in all you do: Be Kind and Care Live Good Health Be Data-Inspired Champion Change Leave it Better than You Found It Make It Happen Be Kind and Care Live Good Health Be Data-Inspired Champion Change Leave it Better than You Found It Make It Happen Qualifications to be successful in this role: Qualifications to be successful in this role: 2-4 years of data engineering experience working with large datasets and complex data environments, processes, and associated solutions Experience with a variety of data stores (e.g. Snowflake, MySQL, MongoDB, DynamoDB, etc.) Experience with data orchestration tooling (e.g. Airflow, Data Factory, etc.) Experience with development languages (e.g. Python, SQL, Scala, etc.) Understanding of data modeling for analysis by business intelligence or data science teams Experience with a variety of API design patterns, such as REST Experience developing validation and data integrity frameworks Triaging and debugging production data issues Experience building high volume data pipelines for downstream analysis supporting operational indicators Experience with data at scale Worked alongside client teams to support integration efforts Familiarity with AWS and/or other cloud computing platforms 2-4 years of data engineering experience working with large datasets and complex data environments, processes, and associated solutions Experience with a variety of data stores (e.g. Snowflake, MySQL, MongoDB, DynamoDB, etc.) Experience with data orchestration tooling (e.g. Airflow, Data Factory, etc.) Experience with development languages (e.g. Python, SQL, Scala, etc.) Understanding of data modeling for analysis by business intelligence or data science teams Experience with a variety of API design patterns, such as REST Experience developing validation and data integrity frameworks Triaging and debugging production data issues Experience building high volume data pipelines for downstream analysis supporting operational indicators Experience with data at scale Worked alongside client teams to support integration efforts Familiarity with AWS and/or other cloud computing platforms Please consider applying even if you don't meet 100% of the qualifications. Research shows you can still be considered for a position if you meet some of the requirements. At MyFitnessPal, we're building a fitness product for everyone and believe our team should reflect that. We encourage people of different backgrounds, experiences, abilities, and perspectives to apply. Exciting Full-Time Employee Benefits, Perks and Culture Embrace the Freedom: Be a digital nomad, work from anywhere we have operations within the continental U.S. Office Vibes: If you prefer working in an office, we've got you covered, our HQ is in vibrant Austin, TX. Face-to-Face Connections: We value personal connections. Enjoy opportunities to meet and connect with your team members in person to help forge meaningful relationships that extend beyond the virtual realm. Teams meet as often as needed and all of MyFitnessPal gathers annually. Flexibility At Its Best: Achieve the work-life balance you deserve. Enjoy a flexible time-off policy and work on your own terms with our Responsible Time Off benefit. Give Back: Use your volunteer days off to support what matters most to you. Each full time teammate receives 2 days per calendar year to give back to their community through service. Mentorship Program: Take control of your career through our mentorship program where, if you'd like, you will be matched with a teammate who can help you scale your skills and propel your growth. Family-Friendly Support: Embrace the journey with confidence and care. Enjoy our paid maternity and paternity leave, to provide time to balance family responsibilities with your career and take the time needed to strengthen family relationships. We understand the complexities of starting or expanding a family, which is why we provide best-in-class comprehensive assistance for fertility-related matters. Wellness Comes First: Live Good Health is one of our core values. Receive a monthly Wellness Allowance, empowering you to focus on your physical and mental well-being by choosing from a range of wellness initiatives, including dedicated mental health days. Celebrate Greatness: Your hard work deserves recognition! Our reward and recognition platform empowers peers to acknowledge and reward each other for the exceptional contributions they make. Elevate Your Health & Fitness: Get access to MyFitnessPal Premium, allowing you to take your fitness, health and wellness journey to new heights. Unlock Your Potential: Access our virtual learning and development library, and participate in training opportunities to continuously grow and enhance your skills. Championing Inclusion: Our dedicated DEI Committee actively fosters a diverse and inclusive workplace by setting actionable goals and evaluating progress across the organization. Healthcare Matters: Your well-being is our priority. Take advantage of our competitive medical, dental, and vision benefits that cater to your holistic healthcare needs. Feel secure and supported on your wellness journey. Secure Your Future: Benefit from our retirement savings program, giving you peace of mind for your financial goals. Reach them sooner with MyFitnessPal's competitive employer match.  At MyFitnessPal, our mission is to enable people to make healthy choices. And it wouldn't be possible without our team. We celebrate the unique POV that each person brings to the table and believe in a collaborative and inclusive environment. As an equal opportunity employer, we prohibit any unlawful discrimination on the basis of race, religion, military or veteran status, sex, gender, marital status, gender identity or expression, sexual orientation, national origin, age, or disability. These are our guiding ideologies and apply across all aspects of employment.  MyFitnessPal participates in E-Verify. Exciting Full-Time Employee Benefits, Perks and Culture Exciting Full-Time Employee Benefits, Perks and Culture Embrace the Freedom: Be a digital nomad, work from anywhere we have operations within the continental U.S. Embrace the Freedom: Office Vibes: If you prefer working in an office, we've got you covered, our HQ is in vibrant Austin, TX. Office Vibes: Face-to-Face Connections: We value personal connections. Enjoy opportunities to meet and connect with your team members in person to help forge meaningful relationships that extend beyond the virtual realm. Teams meet as often as needed and all of MyFitnessPal gathers annually. Face-to-Face Connections : Flexibility At Its Best: Achieve the work-life balance you deserve. Enjoy a flexible time-off policy and work on your own terms with our Responsible Time Off benefit. Flexibility At Its Best: Responsible Time Off Give Back: Use your volunteer days off to support what matters most to you. Each full time teammate receives 2 days per calendar year to give back to their community through service. Give Back: Mentorship Program: Take control of your career through our mentorship program where, if you'd like, you will be matched with a teammate who can help you scale your skills and propel your growth. Mentorship Program: Family-Friendly Support: Embrace the journey with confidence and care. Enjoy our paid maternity and paternity leave, to provide time to balance family responsibilities with your career and take the time needed to strengthen family relationships. We understand the complexities of starting or expanding a family, which is why we provide best-in-class comprehensive assistance for fertility-related matters. Family-Friendly Support : Wellness Comes First: Live Good Health is one of our core values. Receive a monthly Wellness Allowance, empowering you to focus on your physical and mental well-being by choosing from a range of wellness initiatives, including dedicated mental health days. Wellness Comes First : : Celebrate Greatness: Your hard work deserves recognition! Our reward and recognition platform empowers peers to acknowledge and reward each other for the exceptional contributions they make. Celebrate Greatness: Elevate Your Health & Fitness: Get access to MyFitnessPal Premium, allowing you to take your fitness, health and wellness journey to new heights. Elevate Your Health & Fitness: Unlock Your Potential: Access our virtual learning and development library, and participate in training opportunities to continuously grow and enhance your skills. Unlock Your Potential: Championing Inclusion: Our dedicated DEI Committee actively fosters a diverse and inclusive workplace by setting actionable goals and evaluating progress across the organization. Championing Inclusion Healthcare Matters: Your well-being is our priority. Take advantage of our competitive medical, dental, and vision benefits that cater to your holistic healthcare needs. Feel secure and supported on your wellness journey. Healthcare Matters: Secure Your Future: Benefit from our retirement savings program, giving you peace of mind for your financial goals. Reach them sooner with MyFitnessPal's competitive employer match. Secure Your Future: At MyFitnessPal, our mission is to enable people to make healthy choices. And it wouldn't be possible without our team. We celebrate the unique POV that each person brings to the table and believe in a collaborative and inclusive environment. As an equal opportunity employer, we prohibit any unlawful discrimination on the basis of race, religion, military or veteran status, sex, gender, marital status, gender identity or expression, sexual orientation, national origin, age, or disability. These are our guiding ideologies and apply across all aspects of employment. MyFitnessPal participates in E-Verify.",4.2,51 to 200 Employees,2005,Company - Private,Internet & Web Services,Information Technology,$100 to $500 million (USD)
Remote,Data Engineer,$60.00 - $70.00 Per Hour (Employer est.),zettalogix.Inc,"Title: Data Engineer Title: Data Engineer Experience in Retail merchandising analytics is a must, Retail merchandising analytics is a must, Duration: 12 months Duration: 12 months Location: Remote Location: Remote Interview Process: Interview Process: 1*_st_ round – Hirevue Video Call* 1*_ _ 2*_nd_ round – Hiring Manager* 2*_ _ Skills Required: Data engineering, analytics, and data modeling experience Python, Spark, Databricks, Azure, Power BI Experience in retail merchandising analytics is a must Data engineering, analytics, and data modeling experience Data engineering, analytics, and data modeling experience Python, Spark, Databricks, Azure, Power BI Python, Spark, Databricks, Azure, Power BI Experience in retail merchandising analytics is a must Experience in retail merchandising analytics is a must Job Type: Contract Pay: $60.00 - $70.00 per hour Experience level: 8 years 8 years Schedule: Monday to Friday Monday to Friday Experience: retail merchandising analytics: 8 years (Required) Python: 8 years (Required) Power BI: 8 years (Required) retail merchandising analytics: 8 years (Required) Python: 8 years (Required) Power BI: 8 years (Required) Work Location: Remote",N/A,1 to 50 Employees,N/A,Company - Private,N/A,N/A,Unknown / Non-Applicable
Remote,Data Engineer,$84K - $120K (Employer est.),Radcube LLC,"The basic skills and experience needed for the role. Data Management Experience in AWS – AWS S3, Glue, Redshift Data modelling, engineering to design/build conformed data models Power BI for data visualizations Data analysis and insights discovery Health care product analytics Job Types: Full-time, Contract Pay: $84,263.57 - $120,000.00 per year Benefits: 401(k) Dental insurance Health insurance 401(k) Dental insurance Health insurance Experience: AWS: 3 years (Preferred) Data modelling: 3 years (Preferred) Power BI: 3 years (Preferred) Data analysis: 3 years (Preferred) Health care products: 3 years (Preferred) AWS: 3 years (Preferred) Data modelling: 3 years (Preferred) Power BI: 3 years (Preferred) Data analysis: 3 years (Preferred) Health care products: 3 years (Preferred) Work Location: Remote",3.6,51 to 200 Employees,N/A,Company - Private,Information Technology Support Services,Information Technology,Unknown / Non-Applicable
"New York, NY",Data Science/ Machine Learning Engineer,$98K - $151K (Glassdoor est.),Perpetual,"New York office Perpetual is an award winning product design and software development agency in NYC. We work on a variety of innovative and exciting projects, ranging from creative interactive front-end experiences to advanced scalable back-end and data systems. Our clients include several successful startups and innovative large corporations like Reuters, Elle Magazine, Car & Driver, Sonder, Unpakt, The Inside, MIT, University of California and Reliance Jio. Our team has prior experience at companies like McKinsey, Verizon, Reuters, Code & Theory and Fantasy Interactive. We follow best practices in Product Management, UX Design and Software Engineering. We are looking for a Data Scientist who will support our client product development initiatives with insights gained from analyzing product data. The ideal candidate is adept at using large data sets to find opportunities for product and process optimization and using models to test the effectiveness of different courses of action. They must have strong experience using a variety of data mining/data analysis methods, using a variety of data tools, building and implementing models, using/creating algorithms and creating/running simulations. They must have a proven ability to drive business results with their data-based insights. They must be comfortable working with a wide range of stakeholders and functional teams. The right candidate will have a passion for discovering solutions hidden in large data sets and working with stakeholders to improve business outcomes. Key Skills and Requirements: 5+ Years of experience in Data Science or Data Engineering Experience using statistical computer languages (R, Python, SQL, etc.) to manipulate data and draw insights from large data sets. Experience creating and using advanced machine learning algorithms and statistics: regression, simulation, scenario analysis, modeling, clustering, decision trees, neural networks, etc. Experience working with and creating data architectures. Experience with distributed data/computing tools: Map/Reduce, Hadoop, Hive, Spark, MySQL, etc. Proven ability to develop custom data models and algorithms to apply to data sets Proven ability to develop company A/B testing framework and test model quality. Proven ability to develop processes and tools to monitor and analyze model performance and data accuracy. Should be able to use predictive modeling to increase and optimize customer experiences, revenue generation, ad targeting and other business outcomes. Should be able to mine and analyze data from company databases to drive optimization and improvement of product development, marketing techniques and business strategies. Benefits and Perks Medical / Dental / Vision Insurance Visa Sponsorship/H1/OPT Friendly 401K Savings and Retirement Plan Emphasis on Work Life Balance : Set working hours and no weekend work Fitness Membership Discount Commuter Benefits Health Savings Account Primary Healthcare (24/7 Primary Care) Health Advocacy (24/7 Employee Assistance Program to support mental health and a balanced lifestyle) Loft Space in Chelsea (NYC) with a startup vibe Collaborative, learning and fun culture Free office snacks and drinks Comped team outings and other celebrations To apply for this position Send an email with a resume and cover letter to careers@perpetualny.com New York office New York office New York office New York office New York office New York office New York office Perpetual is an award winning product design and software development agency in NYC. We work on a variety of innovative and exciting projects, ranging from creative interactive front-end experiences to advanced scalable back-end and data systems. Our clients include several successful startups and innovative large corporations like Reuters, Elle Magazine, Car & Driver, Sonder, Unpakt, The Inside, MIT, University of California and Reliance Jio. Our team has prior experience at companies like McKinsey, Verizon, Reuters, Code & Theory and Fantasy Interactive. We follow best practices in Product Management, UX Design and Software Engineering. We are looking for a Data Scientist who will support our client product development initiatives with insights gained from analyzing product data. The ideal candidate is adept at using large data sets to find opportunities for product and process optimization and using models to test the effectiveness of different courses of action. They must have strong experience using a variety of data mining/data analysis methods, using a variety of data tools, building and implementing models, using/creating algorithms and creating/running simulations. They must have a proven ability to drive business results with their data-based insights. They must be comfortable working with a wide range of stakeholders and functional teams. The right candidate will have a passion for discovering solutions hidden in large data sets and working with stakeholders to improve business outcomes. Key Skills and Requirements: 5+ Years of experience in Data Science or Data Engineering Experience using statistical computer languages (R, Python, SQL, etc.) to manipulate data and draw insights from large data sets. Experience creating and using advanced machine learning algorithms and statistics: regression, simulation, scenario analysis, modeling, clustering, decision trees, neural networks, etc. Experience working with and creating data architectures. Experience with distributed data/computing tools: Map/Reduce, Hadoop, Hive, Spark, MySQL, etc. Proven ability to develop custom data models and algorithms to apply to data sets Proven ability to develop company A/B testing framework and test model quality. Proven ability to develop processes and tools to monitor and analyze model performance and data accuracy. Should be able to use predictive modeling to increase and optimize customer experiences, revenue generation, ad targeting and other business outcomes. Should be able to mine and analyze data from company databases to drive optimization and improvement of product development, marketing techniques and business strategies. Benefits and Perks Medical / Dental / Vision Insurance Visa Sponsorship/H1/OPT Friendly 401K Savings and Retirement Plan Emphasis on Work Life Balance : Set working hours and no weekend work Fitness Membership Discount Commuter Benefits Health Savings Account Primary Healthcare (24/7 Primary Care) Health Advocacy (24/7 Employee Assistance Program to support mental health and a balanced lifestyle) Loft Space in Chelsea (NYC) with a startup vibe Collaborative, learning and fun culture Free office snacks and drinks Comped team outings and other celebrations To apply for this position Send an email with a resume and cover letter to careers@perpetualny.com Perpetual is an award winning product design and software development agency in NYC. We work on a variety of innovative and exciting projects, ranging from creative interactive front-end experiences to advanced scalable back-end and data systems. Our clients include several successful startups and innovative large corporations like Reuters, Elle Magazine, Car & Driver, Sonder, Unpakt, The Inside, MIT, University of California and Reliance Jio. Our team has prior experience at companies like McKinsey, Verizon, Reuters, Code & Theory and Fantasy Interactive. We follow best practices in Product Management, UX Design and Software Engineering. We are looking for a Data Scientist who will support our client product development initiatives with insights gained from analyzing product data. The ideal candidate is adept at using large data sets to find opportunities for product and process optimization and using models to test the effectiveness of different courses of action. They must have strong experience using a variety of data mining/data analysis methods, using a variety of data tools, building and implementing models, using/creating algorithms and creating/running simulations. They must have a proven ability to drive business results with their data-based insights. They must be comfortable working with a wide range of stakeholders and functional teams. The right candidate will have a passion for discovering solutions hidden in large data sets and working with stakeholders to improve business outcomes. Key Skills and Requirements: 5+ Years of experience in Data Science or Data Engineering Experience using statistical computer languages (R, Python, SQL, etc.) to manipulate data and draw insights from large data sets. Experience creating and using advanced machine learning algorithms and statistics: regression, simulation, scenario analysis, modeling, clustering, decision trees, neural networks, etc. Experience working with and creating data architectures. Experience with distributed data/computing tools: Map/Reduce, Hadoop, Hive, Spark, MySQL, etc. Proven ability to develop custom data models and algorithms to apply to data sets Proven ability to develop company A/B testing framework and test model quality. Proven ability to develop processes and tools to monitor and analyze model performance and data accuracy. Should be able to use predictive modeling to increase and optimize customer experiences, revenue generation, ad targeting and other business outcomes. Should be able to mine and analyze data from company databases to drive optimization and improvement of product development, marketing techniques and business strategies. Benefits and Perks Medical / Dental / Vision Insurance Visa Sponsorship/H1/OPT Friendly 401K Savings and Retirement Plan Emphasis on Work Life Balance : Set working hours and no weekend work Fitness Membership Discount Commuter Benefits Health Savings Account Primary Healthcare (24/7 Primary Care) Health Advocacy (24/7 Employee Assistance Program to support mental health and a balanced lifestyle) Loft Space in Chelsea (NYC) with a startup vibe Collaborative, learning and fun culture Free office snacks and drinks Comped team outings and other celebrations To apply for this position Send an email with a resume and cover letter to careers@perpetualny.com Perpetual is an award winning product design and software development agency in NYC. We work on a variety of innovative and exciting projects, ranging from creative interactive front-end experiences to advanced scalable back-end and data systems. Our clients include several successful startups and innovative large corporations like Reuters, Elle Magazine, Car & Driver, Sonder, Unpakt, The Inside, MIT, University of California and Reliance Jio. Our team has prior experience at companies like McKinsey, Verizon, Reuters, Code & Theory and Fantasy Interactive. We follow best practices in Product Management, UX Design and Software Engineering. We are looking for a Data Scientist who will support our client product development initiatives with insights gained from analyzing product data. The ideal candidate is adept at using large data sets to find opportunities for product and process optimization and using models to test the effectiveness of different courses of action. They must have strong experience using a variety of data mining/data analysis methods, using a variety of data tools, building and implementing models, using/creating algorithms and creating/running simulations. They must have a proven ability to drive business results with their data-based insights. They must be comfortable working with a wide range of stakeholders and functional teams. The right candidate will have a passion for discovering solutions hidden in large data sets and working with stakeholders to improve business outcomes. Key Skills and Requirements: 5+ Years of experience in Data Science or Data Engineering Experience using statistical computer languages (R, Python, SQL, etc.) to manipulate data and draw insights from large data sets. Experience creating and using advanced machine learning algorithms and statistics: regression, simulation, scenario analysis, modeling, clustering, decision trees, neural networks, etc. Experience working with and creating data architectures. Experience with distributed data/computing tools: Map/Reduce, Hadoop, Hive, Spark, MySQL, etc. Proven ability to develop custom data models and algorithms to apply to data sets Proven ability to develop company A/B testing framework and test model quality. Proven ability to develop processes and tools to monitor and analyze model performance and data accuracy. Should be able to use predictive modeling to increase and optimize customer experiences, revenue generation, ad targeting and other business outcomes. Should be able to mine and analyze data from company databases to drive optimization and improvement of product development, marketing techniques and business strategies. Benefits and Perks Medical / Dental / Vision Insurance Visa Sponsorship/H1/OPT Friendly 401K Savings and Retirement Plan Emphasis on Work Life Balance : Set working hours and no weekend work Fitness Membership Discount Commuter Benefits Health Savings Account Primary Healthcare (24/7 Primary Care) Health Advocacy (24/7 Employee Assistance Program to support mental health and a balanced lifestyle) Loft Space in Chelsea (NYC) with a startup vibe Collaborative, learning and fun culture Free office snacks and drinks Comped team outings and other celebrations To apply for this position Send an email with a resume and cover letter to careers@perpetualny.com Perpetual is an award winning product design and software development agency in NYC. We work on a variety of innovative and exciting projects, ranging from creative interactive front-end experiences to advanced scalable back-end and data systems. Our clients include several successful startups and innovative large corporations like Reuters, Elle Magazine, Car & Driver, Sonder, Unpakt, The Inside, MIT, University of California and Reliance Jio. Our team has prior experience at companies like McKinsey, Verizon, Reuters, Code & Theory and Fantasy Interactive. We follow best practices in Product Management, UX Design and Software Engineering. We are looking for a Data Scientist who will support our client product development initiatives with insights gained from analyzing product data. The ideal candidate is adept at using large data sets to find opportunities for product and process optimization and using models to test the effectiveness of different courses of action. They must have strong experience using a variety of data mining/data analysis methods, using a variety of data tools, building and implementing models, using/creating algorithms and creating/running simulations. They must have a proven ability to drive business results with their data-based insights. They must be comfortable working with a wide range of stakeholders and functional teams. The right candidate will have a passion for discovering solutions hidden in large data sets and working with stakeholders to improve business outcomes. Key Skills and Requirements: 5+ Years of experience in Data Science or Data Engineering Experience using statistical computer languages (R, Python, SQL, etc.) to manipulate data and draw insights from large data sets. Experience creating and using advanced machine learning algorithms and statistics: regression, simulation, scenario analysis, modeling, clustering, decision trees, neural networks, etc. Experience working with and creating data architectures. Experience with distributed data/computing tools: Map/Reduce, Hadoop, Hive, Spark, MySQL, etc. Proven ability to develop custom data models and algorithms to apply to data sets Proven ability to develop company A/B testing framework and test model quality. Proven ability to develop processes and tools to monitor and analyze model performance and data accuracy. Should be able to use predictive modeling to increase and optimize customer experiences, revenue generation, ad targeting and other business outcomes. Should be able to mine and analyze data from company databases to drive optimization and improvement of product development, marketing techniques and business strategies. Benefits and Perks Medical / Dental / Vision Insurance Visa Sponsorship/H1/OPT Friendly 401K Savings and Retirement Plan Emphasis on Work Life Balance : Set working hours and no weekend work Fitness Membership Discount Commuter Benefits Health Savings Account Primary Healthcare (24/7 Primary Care) Health Advocacy (24/7 Employee Assistance Program to support mental health and a balanced lifestyle) Loft Space in Chelsea (NYC) with a startup vibe Collaborative, learning and fun culture Free office snacks and drinks Comped team outings and other celebrations To apply for this position Send an email with a resume and cover letter to careers@perpetualny.com Perpetual is an award winning product design and software development agency in NYC. We work on a variety of innovative and exciting projects, ranging from creative interactive front-end experiences to advanced scalable back-end and data systems. Our clients include several successful startups and innovative large corporations like Reuters, Elle Magazine, Car & Driver, Sonder, Unpakt, The Inside, MIT, University of California and Reliance Jio. Our team has prior experience at companies like McKinsey, Verizon, Reuters, Code & Theory and Fantasy Interactive. We follow best practices in Product Management, UX Design and Software Engineering. We are looking for a Data Scientist who will support our client product development initiatives with insights gained from analyzing product data. The ideal candidate is adept at using large data sets to find opportunities for product and process optimization and using models to test the effectiveness of different courses of action. They must have strong experience using a variety of data mining/data analysis methods, using a variety of data tools, building and implementing models, using/creating algorithms and creating/running simulations. They must have a proven ability to drive business results with their data-based insights. They must be comfortable working with a wide range of stakeholders and functional teams. The right candidate will have a passion for discovering solutions hidden in large data sets and working with stakeholders to improve business outcomes. Perpetual is an award winning product design and software development agency in NYC. We work on a variety of innovative and exciting projects, ranging from creative interactive front-end experiences to advanced scalable back-end and data systems. Our clients include several successful startups and innovative large corporations like Reuters, Elle Magazine, Car & Driver, Sonder, Unpakt, The Inside, MIT, University of California and Reliance Jio. Our team has prior experience at companies like McKinsey, Verizon, Reuters, Code & Theory and Fantasy Interactive. We follow best practices in Product Management, UX Design and Software Engineering. We are looking for a Data Scientist who will support our client product development initiatives with insights gained from analyzing product data. The ideal candidate is adept at using large data sets to find opportunities for product and process optimization and using models to test the effectiveness of different courses of action. They must have strong experience using a variety of data mining/data analysis methods, using a variety of data tools, building and implementing models, using/creating algorithms and creating/running simulations. They must have a proven ability to drive business results with their data-based insights. They must be comfortable working with a wide range of stakeholders and functional teams. The right candidate will have a passion for discovering solutions hidden in large data sets and working with stakeholders to improve business outcomes. Perpetual is an award winning product design and software development agency in NYC. We work on a variety of innovative and exciting projects, ranging from creative interactive front-end experiences to advanced scalable back-end and data systems. Our clients include several successful startups and innovative large corporations like Reuters, Elle Magazine, Car & Driver, Sonder, Unpakt, The Inside, MIT, University of California and Reliance Jio. Our team has prior experience at companies like McKinsey, Verizon, Reuters, Code & Theory and Fantasy Interactive. We follow best practices in Product Management, UX Design and Software Engineering. Perpetual is an award winning product design and software development agency in NYC. We work on a variety of innovative and exciting projects, ranging from creative interactive front-end experiences to advanced scalable back-end and data systems. Our clients include several successful startups and innovative large corporations like Reuters, Elle Magazine, Car & Driver, Sonder, Unpakt, The Inside, MIT, University of California and Reliance Jio. Our team has prior experience at companies like McKinsey, Verizon, Reuters, Code & Theory and Fantasy Interactive. We follow best practices in Product Management, UX Design and Software Engineering. We are looking for a Data Scientist who will support our client product development initiatives with insights gained from analyzing product data. The ideal candidate is adept at using large data sets to find opportunities for product and process optimization and using models to test the effectiveness of different courses of action. They must have strong experience using a variety of data mining/data analysis methods, using a variety of data tools, building and implementing models, using/creating algorithms and creating/running simulations. They must have a proven ability to drive business results with their data-based insights. They must be comfortable working with a wide range of stakeholders and functional teams. The right candidate will have a passion for discovering solutions hidden in large data sets and working with stakeholders to improve business outcomes. We are looking for a Data Scientist who will support our client product development initiatives with insights gained from analyzing product data. The ideal candidate is adept at using large data sets to find opportunities for product and process optimization and using models to test the effectiveness of different courses of action. They must have strong experience using a variety of data mining/data analysis methods, using a variety of data tools, building and implementing models, using/creating algorithms and creating/running simulations. They must have a proven ability to drive business results with their data-based insights. They must be comfortable working with a wide range of stakeholders and functional teams. The right candidate will have a passion for discovering solutions hidden in large data sets and working with stakeholders to improve business outcomes. Key Skills and Requirements: 5+ Years of experience in Data Science or Data Engineering Experience using statistical computer languages (R, Python, SQL, etc.) to manipulate data and draw insights from large data sets. Experience creating and using advanced machine learning algorithms and statistics: regression, simulation, scenario analysis, modeling, clustering, decision trees, neural networks, etc. Experience working with and creating data architectures. Experience with distributed data/computing tools: Map/Reduce, Hadoop, Hive, Spark, MySQL, etc. Proven ability to develop custom data models and algorithms to apply to data sets Proven ability to develop company A/B testing framework and test model quality. Proven ability to develop processes and tools to monitor and analyze model performance and data accuracy. Should be able to use predictive modeling to increase and optimize customer experiences, revenue generation, ad targeting and other business outcomes. Should be able to mine and analyze data from company databases to drive optimization and improvement of product development, marketing techniques and business strategies. Key Skills and Requirements: Key Skills and Requirements: Key Skills and Requirements: 5+ Years of experience in Data Science or Data Engineering Experience using statistical computer languages (R, Python, SQL, etc.) to manipulate data and draw insights from large data sets. Experience creating and using advanced machine learning algorithms and statistics: regression, simulation, scenario analysis, modeling, clustering, decision trees, neural networks, etc. Experience working with and creating data architectures. Experience with distributed data/computing tools: Map/Reduce, Hadoop, Hive, Spark, MySQL, etc. Proven ability to develop custom data models and algorithms to apply to data sets Proven ability to develop company A/B testing framework and test model quality. Proven ability to develop processes and tools to monitor and analyze model performance and data accuracy. Should be able to use predictive modeling to increase and optimize customer experiences, revenue generation, ad targeting and other business outcomes. Should be able to mine and analyze data from company databases to drive optimization and improvement of product development, marketing techniques and business strategies. 5+ Years of experience in Data Science or Data Engineering Experience using statistical computer languages (R, Python, SQL, etc.) to manipulate data and draw insights from large data sets. Experience creating and using advanced machine learning algorithms and statistics: regression, simulation, scenario analysis, modeling, clustering, decision trees, neural networks, etc. Experience working with and creating data architectures. Experience with distributed data/computing tools: Map/Reduce, Hadoop, Hive, Spark, MySQL, etc. Proven ability to develop custom data models and algorithms to apply to data sets Proven ability to develop company A/B testing framework and test model quality. Proven ability to develop processes and tools to monitor and analyze model performance and data accuracy. Should be able to use predictive modeling to increase and optimize customer experiences, revenue generation, ad targeting and other business outcomes. Should be able to mine and analyze data from company databases to drive optimization and improvement of product development, marketing techniques and business strategies. 5+ Years of experience in Data Science or Data Engineering Experience using statistical computer languages (R, Python, SQL, etc.) to manipulate data and draw insights from large data sets. Experience creating and using advanced machine learning algorithms and statistics: regression, simulation, scenario analysis, modeling, clustering, decision trees, neural networks, etc. Experience working with and creating data architectures. Experience with distributed data/computing tools: Map/Reduce, Hadoop, Hive, Spark, MySQL, etc. Proven ability to develop custom data models and algorithms to apply to data sets Proven ability to develop company A/B testing framework and test model quality. Proven ability to develop processes and tools to monitor and analyze model performance and data accuracy. Should be able to use predictive modeling to increase and optimize customer experiences, revenue generation, ad targeting and other business outcomes. Should be able to mine and analyze data from company databases to drive optimization and improvement of product development, marketing techniques and business strategies. 5+ Years of experience in Data Science or Data Engineering Experience using statistical computer languages (R, Python, SQL, etc.) to manipulate data and draw insights from large data sets. Experience creating and using advanced machine learning algorithms and statistics: regression, simulation, scenario analysis, modeling, clustering, decision trees, neural networks, etc. Experience working with and creating data architectures. Experience with distributed data/computing tools: Map/Reduce, Hadoop, Hive, Spark, MySQL, etc. Proven ability to develop custom data models and algorithms to apply to data sets Proven ability to develop company A/B testing framework and test model quality. Proven ability to develop processes and tools to monitor and analyze model performance and data accuracy. Should be able to use predictive modeling to increase and optimize customer experiences, revenue generation, ad targeting and other business outcomes. Should be able to mine and analyze data from company databases to drive optimization and improvement of product development, marketing techniques and business strategies. Benefits and Perks Medical / Dental / Vision Insurance Visa Sponsorship/H1/OPT Friendly 401K Savings and Retirement Plan Emphasis on Work Life Balance : Set working hours and no weekend work Fitness Membership Discount Commuter Benefits Health Savings Account Primary Healthcare (24/7 Primary Care) Health Advocacy (24/7 Employee Assistance Program to support mental health and a balanced lifestyle) Loft Space in Chelsea (NYC) with a startup vibe Collaborative, learning and fun culture Free office snacks and drinks Comped team outings and other celebrations Benefits and Perks Benefits and Perks Benefits and Perks Medical / Dental / Vision Insurance Visa Sponsorship/H1/OPT Friendly 401K Savings and Retirement Plan Emphasis on Work Life Balance : Set working hours and no weekend work Fitness Membership Discount Commuter Benefits Health Savings Account Primary Healthcare (24/7 Primary Care) Health Advocacy (24/7 Employee Assistance Program to support mental health and a balanced lifestyle) Loft Space in Chelsea (NYC) with a startup vibe Collaborative, learning and fun culture Free office snacks and drinks Comped team outings and other celebrations Medical / Dental / Vision Insurance Visa Sponsorship/H1/OPT Friendly 401K Savings and Retirement Plan Emphasis on Work Life Balance : Set working hours and no weekend work Fitness Membership Discount Commuter Benefits Health Savings Account Primary Healthcare (24/7 Primary Care) Health Advocacy (24/7 Employee Assistance Program to support mental health and a balanced lifestyle) Loft Space in Chelsea (NYC) with a startup vibe Collaborative, learning and fun culture Free office snacks and drinks Comped team outings and other celebrations Medical / Dental / Vision Insurance Visa Sponsorship/H1/OPT Friendly 401K Savings and Retirement Plan Emphasis on Work Life Balance : Set working hours and no weekend work Fitness Membership Discount Commuter Benefits Health Savings Account Primary Healthcare (24/7 Primary Care) Health Advocacy (24/7 Employee Assistance Program to support mental health and a balanced lifestyle) Loft Space in Chelsea (NYC) with a startup vibe Collaborative, learning and fun culture Free office snacks and drinks Comped team outings and other celebrations Medical / Dental / Vision Insurance Visa Sponsorship/H1/OPT Friendly 401K Savings and Retirement Plan Emphasis on Work Life Balance : Set working hours and no weekend work Fitness Membership Discount Commuter Benefits Health Savings Account Primary Healthcare (24/7 Primary Care) Health Advocacy (24/7 Employee Assistance Program to support mental health and a balanced lifestyle) Loft Space in Chelsea (NYC) with a startup vibe Collaborative, learning and fun culture Free office snacks and drinks Comped team outings and other celebrations To apply for this position Send an email with a resume and cover letter to careers@perpetualny.com To apply for this position Send an email with a resume and cover letter to careers@perpetualny.com To apply for this position To apply for this position To apply for this position Send an email with a resume and cover letter to Send an email with a resume and cover letter to careers@perpetualny.com careers@perpetualny.com careers@perpetualny.com careers@perpetualny.com",3.8,501 to 1000 Employees,1886,Company - Public,Investment & Asset Management,Financial Services,$100 to $500 million (USD)
United States,Data Engineer - 1483,$95K - $133K (Employer est.),MeridianLink,"JOB SUMMARY We are looking for an accomplished Data Engineer to join our quickly growing Analytics team. The hire will be responsible for expanding and improving our data and data pipeline architecture, as well as optimizing data flow and MDM for cross functional teams. The ideal candidate is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up. The Data Engineer will support database architects, data analysts and data scientists on data initiatives and will ensure optimal data delivery throughout ongoing projects. They must be self-directed and comfortable supporting the data needs of multiple teams, systems, and products. The right candidate will be excited by the prospect of optimizing or even re-designing our company’s data pipelines to support our next generation of products and data initiatives. RESPONSIBILITIES Design, develop, and operate large scale data pipelines to support internal and external consumers Improve and automate internal processes Integrate data sources to meet business requirements Write robust, maintainable, well documented code QUALIFICATIONS 2-4 years professional Data Engineering and Data warehousing experience Extremely strong implementation experience in Python, Spark, Azure Databricks, Delta Lake, and Databricks Data Warehouse. SQL development knowledge – Stored procedures, triggers, jobs, indexes, partitioning etc. Be able to write/debug complex SQL queries Azure Data factory or Azure Synapse Analytics ETL/ELT and Data-warehousing techniques and best practices Experience with MS-SQL server and Databricks Data warehouse. Experience building, maintaining, and scaling ETL/ELT processes and infrastructure Knowledge of being able to work with a variety of Ingestion patterns such as API/SQL servers etc. Experience with cloud infrastructure (Azure strongly preferred) Knowledge of Master Data Management Implementation experience with various data modelling techniques Implementation experience working with a BI visualization tool (Sisense is a plus) Experience with CI/CD tools (Preferred Gitlab, Jenkins) Pluses for experience with oUI development frameworks such as java script, Django, REACT etc. Experience working in a fast-paced product environment, with an attitude of getting the job done with the least amount of tech debt Prior Financial industry experience a plus. Be able to navigate ambiguity and pivot based on business priorities with ease. Strong communication, negotiating and estimating skills. Be a team player and should be able to collaborate well. Enables entrepreneurs and consumers to achieve the American dream by creating technological solutions that fuel the engine for financial growth. Our top-notch solutions create the premier customer experience every time. We believe in the principles of empowerment, collaboration, individual achievement, and innovation. At MeridianLink, we work together to design, implement, test, and deliver state-of-the-art web applications for the financial services industry, using the latest technologies including cloud computing, mobile development, responsive design, ASP.NET, JavaScript, C#, VB.NET, and SQL Server. OUR CULTURE Our low turnover is a testament to our wonderful culture where people value the work they do and appreciate each other for their contributions. MeridianLink develops our employees so they can grow professionally by preferring to promote from within. We have an open door policy with direct access to executives; we want to hear your ideas and what you think. Our company believes that to be productive in the long term, we must have a genuine work-life balance. We understand that employees have families and full lives outside of the office. To that end, we honor their personal commitments. MeridianLink is an Equal Opportunity Employer. We do not discriminate on the basis of race, religion, color, sex, age, national origin, disability or any other characteristic protected by applicable law. MeridianLink runs a comprehensive background check, credit check and drug test as part of our offer process.  MeridianLink has a wonderful culture where people value the work they do and appreciate each other for their contributions. We develop our employees so they can grow professionally by preferring to promote from within. We have an open-door policy with direct access to executives; we want to hear your ideas and what you think. Our company believes that to be productive in the long term, we must have a genuine work-life balance. We understand that employees have families and full lives outside of the office. To that end, we honor their personal commitments. MeridianLink is an Equal Opportunity Employer. We do not discriminate based on race, religion, color, sex, age, national origin, disability, or any other characteristic protected by applicable law. MeridianLink runs a comprehensive background check, credit check, and drug test as part of our offer process. Salary range of $94,500-$133,400. [It is not typical for offers to be made at or near the top of the range.] The actual salary will be determined based on experience and other job-related factors permitted by law. Meridianlink offers:  Potential For Equity-Based Awards Insurance coverage (medical, dental, vision, life, and disability) Flexible paid time off Paid holidays 401(k) plan with company match Remote work All compensation and benefits are subject to the terms and conditions of the underlying plans or programs, as applicable and as may be amended, terminated, or superseded from time to time. #LI-REMOTE JOB SUMMARY We are looking for an accomplished Data Engineer to join our quickly growing Analytics team. The hire will be responsible for expanding and improving our data and data pipeline architecture, as well as optimizing data flow and MDM for cross functional teams. The ideal candidate is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up. The Data Engineer will support database architects, data analysts and data scientists on data initiatives and will ensure optimal data delivery throughout ongoing projects. They must be self-directed and comfortable supporting the data needs of multiple teams, systems, and products. The right candidate will be excited by the prospect of optimizing or even re-designing our company’s data pipelines to support our next generation of products and data initiatives. RESPONSIBILITIES Design, develop, and operate large scale data pipelines to support internal and external consumers Improve and automate internal processes Integrate data sources to meet business requirements Write robust, maintainable, well documented code Design, develop, and operate large scale data pipelines to support internal and external consumers Improve and automate internal processes Integrate data sources to meet business requirements Write robust, maintainable, well documented code QUALIFICATIONS 2-4 years professional Data Engineering and Data warehousing experience Extremely strong implementation experience in 2-4 years professional Data Engineering and Data warehousing experience Extremely strong implementation experience in Python, Spark, Azure Databricks, Delta Lake, and Databricks Data Warehouse. SQL development knowledge – Stored procedures, triggers, jobs, indexes, partitioning etc. Be able to write/debug complex SQL queries Azure Data factory or Azure Synapse Analytics ETL/ELT and Data-warehousing techniques and best practices Experience with MS-SQL server and Databricks Data warehouse. Experience building, maintaining, and scaling ETL/ELT processes and infrastructure Knowledge of being able to work with a variety of Ingestion patterns such as API/SQL servers etc. Experience with cloud infrastructure (Azure strongly preferred) Knowledge of Master Data Management Implementation experience with various data modelling techniques Implementation experience working with a BI visualization tool (Sisense is a plus) Experience with CI/CD tools (Preferred Gitlab, Jenkins) Pluses for experience with Experience with MS-SQL server and Databricks Data warehouse. Experience building, maintaining, and scaling ETL/ELT processes and infrastructure Knowledge of being able to work with a variety of Ingestion patterns such as API/SQL servers etc. Experience with cloud infrastructure (Azure strongly preferred) Knowledge of Master Data Management Implementation experience with various data modelling techniques Implementation experience working with a BI visualization tool (Sisense is a plus) Experience with CI/CD tools (Preferred Gitlab, Jenkins) Pluses for experience with oUI development frameworks such as java script, Django, REACT etc. Experience working in a fast-paced product environment, with an attitude of getting the job done with the least amount of tech debt Prior Financial industry experience a plus. Be able to navigate ambiguity and pivot based on business priorities with ease. Strong communication, negotiating and estimating skills. Be a team player and should be able to collaborate well. Experience working in a fast-paced product environment, with an attitude of getting the job done with the least amount of tech debt Prior Financial industry experience a plus. Be able to navigate ambiguity and pivot based on business priorities with ease. Strong communication, negotiating and estimating skills. Be a team player and should be able to collaborate well. Enables entrepreneurs and consumers to achieve the American dream by creating technological solutions that fuel the engine for financial growth. Our top-notch solutions create the premier customer experience every time. We believe in the principles of empowerment, collaboration, individual achievement, and innovation. At MeridianLink, we work together to design, implement, test, and deliver state-of-the-art web applications for the financial services industry, using the latest technologies including cloud computing, mobile development, responsive design, ASP.NET, JavaScript, C#, VB.NET, and SQL Server. OUR CULTURE Our low turnover is a testament to our wonderful culture where people value the work they do and appreciate each other for their contributions. MeridianLink develops our employees so they can grow professionally by preferring to promote from within. We have an open door policy with direct access to executives; we want to hear your ideas and what you think. Our company believes that to be productive in the long term, we must have a genuine work-life balance. We understand that employees have families and full lives outside of the office. To that end, we honor their personal commitments. MeridianLink is an Equal Opportunity Employer. We do not discriminate on the basis of race, religion, color, sex, age, national origin, disability or any other characteristic protected by applicable law. MeridianLink runs a comprehensive background check, credit check and drug test as part of our offer process. MeridianLink has a wonderful culture where people value the work they do and appreciate each other for their contributions. We develop our employees so they can grow professionally by preferring to promote from within. We have an open-door policy with direct access to executives; we want to hear your ideas and what you think. Our company believes that to be productive in the long term, we must have a genuine work-life balance. We understand that employees have families and full lives outside of the office. To that end, we honor their personal commitments. MeridianLink is an Equal Opportunity Employer. We do not discriminate based on race, religion, color, sex, age, national origin, disability, or any other characteristic protected by applicable law. MeridianLink runs a comprehensive background check, credit check, and drug test as part of our offer process. Salary range of $94,500-$133,400. [It is not typical for offers to be made at or near the top of the range.] The actual salary will be determined based on experience and other job-related factors permitted by law. Meridianlink offers: Potential For Equity-Based Awards Insurance coverage (medical, dental, vision, life, and disability) Flexible paid time off Paid holidays 401(k) plan with company match Remote work All compensation and benefits are subject to the terms and conditions of the underlying plans or programs, as applicable and as may be amended, terminated, or superseded from time to time. #LI-REMOTE",3.9,501 to 1000 Employees,1998,Company - Public,Computer Hardware Development,Information Technology,Unknown / Non-Applicable
"Seattle, WA",Data Engineer,$95K - $130K (Employer est.),Stanley | tms,"Data Engineer  About Us Stanley, a HAVI company, is defined by Creativity, Building and Invention. We are makers of the legendary bottle and box. Driven by purpose, passion and performance. Obsessed with making a difference. And keeping our promises. Proud of our yesterday. And focused on building the team of tomorrow.  Position Overview Stanley, a HAVI company, is looking for a Data Engineer to support our current and future data platform. This role will be responsible for ensuring data is available and consumable by business analysts and other key business users. This includes responsibility for the ingestion of source data through CI/CD pipelines. As a member of the Enterprise Data Platform team, you will contribute to projects and initiatives across the enterprise.  Key Responsibilities Develop end-to-end solutions for ingestion, storage, prepping, and modeling of data Test, audit, maintain and tune existing solutions Implement data structures using data modeling, ELT/ETL processes, and SQL technologies Monitor daily data loads and remedy issues within SLA Update and extend system documentation Regularly interact across functional areas to ensure objectives are met Exercise independent judgment in methods, techniques, and evaluation criteria for obtaining results  Who You Are Daily interaction with cloud-based data platforms and services Solid understanding of data architecture principles and practices Working knowledge of data warehousing concepts supporting performant access to critical business data Familiarity with tabular modeling to define data relationships and navigation taxonomies Experience with coding languages like SQL, Python and R  Education and Experience Bachelor’s degree in Computer Engineering, Computer Science, Mathematics or equivalent related IT-specific experience At least 5 years of Data Engineer experience is required, preferably in a Cloud environment Experience with database query and analysis languages (e.g. T-SQL, PL-SQL, R, SAS, Python) and data visualization tools (e.g. PowerBI, Tableau, D3) Experience working with various data sources (e.g. SQL, Oracle database, flat files, Web API, XML) Experience with cloud warehouse and analytics required; Azure Data Storage and Analytics (e.g. Data Lake, Data Factory, Synapse and Analysis Services) Experience with Oracle EBS preferred Experience with Synapse ML preferred Experience with multi-tenant infrastructure preferred Understanding of manufacturing, supply chain, inventory management and sales operations data and systems preferred Demonstrable ability to communicate, partner and deliver solutions to business customers Puget Sound based only. This is a hybrid role in our Seattle office. We are unable to provide sponsorship for this role.  About HAVI: HAVI is a global, privately owned company that connects people with ideas, data with insights, supply with demand, restaurants with deliveries and ultimately, people with the products they love. Whether we are sourcing, storing or delivering products, we bring unmatched category expertise and unrivaled operational excellence, combined with powerful digital analytics and insights. Founded in 1974, HAVI employs more than 10,000 people and serves customers in more than 100 countries. HAVI’s business units include Supply Chain, tms and Stanley. Our portfolio of businesses offers best-in-class sourcing and supply chain capabilities, brand-defining marketing and promotion services and innovative consumer products. For more information, please visit HAVI.com, tmsw.com and stanley1913.com. Data Engineer Data Engineer About Us About Us Stanley, a HAVI company, is defined by Creativity, Building and Invention. We are makers of the legendary bottle and box. Driven by purpose, passion and performance. Obsessed with making a difference. And keeping our promises. Proud of our yesterday. And focused on building the team of tomorrow. Position Overview Position Overview Stanley, a HAVI company, is looking for a Data Engineer to support our current and future data platform. This role will be responsible for ensuring data is available and consumable by business analysts and other key business users. This includes responsibility for the ingestion of source data through CI/CD pipelines. As a member of the Enterprise Data Platform team, you will contribute to projects and initiatives across the enterprise. Key Responsibilities Key Responsibilities Develop end-to-end solutions for ingestion, storage, prepping, and modeling of data Develop end-to-end solutions for ingestion, storage, prepping, and modeling of data Test, audit, maintain and tune existing solutions Test, audit, maintain and tune existing solutions Implement data structures using data modeling, ELT/ETL processes, and SQL technologies Implement data structures using data modeling, ELT/ETL processes, and SQL technologies Monitor daily data loads and remedy issues within SLA Monitor daily data loads and remedy issues within SLA Update and extend system documentation Update and extend system documentation Regularly interact across functional areas to ensure objectives are met Regularly interact across functional areas to ensure objectives are met Exercise independent judgment in methods, techniques, and evaluation criteria for obtaining results Exercise independent judgment in methods, techniques, and evaluation criteria for obtaining results Who You Are Who You Are Daily interaction with cloud-based data platforms and services Daily interaction with cloud-based data platforms and services Solid understanding of data architecture principles and practices Solid understanding of data architecture principles and practices Working knowledge of data warehousing concepts supporting performant access to critical business data Working knowledge of data warehousing concepts supporting performant access to critical business data Familiarity with tabular modeling to define data relationships and navigation taxonomies Familiarity with tabular modeling to define data relationships and navigation taxonomies Experience with coding languages like SQL, Python and R Experience with coding languages like SQL, Python and R Education and Experience Education and Experience Bachelor’s degree in Computer Engineering, Computer Science, Mathematics or equivalent related IT-specific experience Bachelor’s degree in Computer Engineering, Computer Science, Mathematics or equivalent related IT-specific experience At least 5 years of Data Engineer experience is required, preferably in a Cloud environment At least 5 years of Data Engineer experience is required, preferably in a Cloud environment Experience with database query and analysis languages (e.g. T-SQL, PL-SQL, R, SAS, Python) and data visualization tools (e.g. PowerBI, Tableau, D3) Experience with database query and analysis languages (e.g. T-SQL, PL-SQL, R, SAS, Python) and data visualization tools (e.g. PowerBI, Tableau, D3) Experience working with various data sources (e.g. SQL, Oracle database, flat files, Web API, XML) Experience working with various data sources (e.g. SQL, Oracle database, flat files, Web API, XML) Experience with cloud warehouse and analytics required; Azure Data Storage and Analytics (e.g. Data Lake, Data Factory, Synapse and Analysis Services) Experience with cloud warehouse and analytics required; Azure Data Storage and Analytics (e.g. Data Lake, Data Factory, Synapse and Analysis Services) Experience with Oracle EBS preferred Experience with Oracle EBS preferred Experience with Synapse ML preferred Experience with Synapse ML preferred Experience with multi-tenant infrastructure preferred Experience with multi-tenant infrastructure preferred Understanding of manufacturing, supply chain, inventory management and sales operations data and systems preferred Understanding of manufacturing, supply chain, inventory management and sales operations data and systems preferred Demonstrable ability to communicate, partner and deliver solutions to business customers Demonstrable ability to communicate, partner and deliver solutions to business customers Puget Sound based only. This is a hybrid role in our Seattle office. Puget Sound based only. This is a hybrid role in our Seattle office. We are unable to provide sponsorship for this role. We are unable to provide sponsorship for this role. About HAVI: About HAVI: HAVI is a global, privately owned company that connects people with ideas, data with insights, supply with demand, restaurants with deliveries and ultimately, people with the products they love. Whether we are sourcing, storing or delivering products, we bring unmatched category expertise and unrivaled operational excellence, combined with powerful digital analytics and insights. Founded in 1974, HAVI employs more than 10,000 people and serves customers in more than 100 countries. HAVI’s business units include Supply Chain, tms and Stanley. Our portfolio of businesses offers best-in-class sourcing and supply chain capabilities, brand-defining marketing and promotion services and innovative consumer products. For more information, please visit HAVI.com, tmsw.com and stanley1913.com.",N/A,,,,,,
"Edison, NJ",BIG DATA ENGINEER,$88K - $126K (Glassdoor est.),ADVANTECS GROUP,"Job Description: Develop big data scalable solutions using Hadoop, Hive, Spark, Map-Reduce, Java, Python. Design schema and data molding for NoSQL Database & Data Warehouse. Develop ETL data flow and Cloud Integration to build reporting solutions. Design and implement software solutions on Cloud platforms like AWS/Azure with capabilities of processing terabytes of data. Master's degree in Science, Technology, or Engineering (any) is required. Work location: Edison, NJ and various unanticipated locations throughout the U.S. Send resume to HR Dept., Advantecs Group, Inc., 22 Meridian Road, Suite 4, Edison, NJ 08820. Should the candidate accept employment with Advantecs Group, Inc., the referring employee will be eligible to receive an award of $1,000.00 for the successful referral. Job Description: Job Description: Develop big data scalable solutions using Hadoop, Hive, Spark, Map-Reduce, Java, Python. Design schema and data molding for NoSQL Database & Data Warehouse. Develop ETL data flow and Cloud Integration to build reporting solutions. Design and implement software solutions on Cloud platforms like AWS/Azure with capabilities of processing terabytes of data. Develop big data scalable solutions using Hadoop, Hive, Spark, Map-Reduce, Java, Python. Design schema and data molding for NoSQL Database & Data Warehouse. Develop ETL data flow and Cloud Integration to build reporting solutions. Design and implement software solutions on Cloud platforms like AWS/Azure with capabilities of processing terabytes of data. Master's degree in Science, Technology, or Engineering (any) is required. Work location: Edison, NJ and various unanticipated locations throughout the U.S. Work location: Send resume to HR Dept., Advantecs Group, Inc., 22 Meridian Road, Suite 4, Edison, NJ 08820. Send resume to Should the candidate accept employment with Advantecs Group, Inc., the referring employee will be eligible to receive an award of $1,000.00 for the successful referral. Should the candidate accept employment with Advantecs Group, Inc., the referring employee will be eligible to receive an award of $1,000.00 for the successful referral. Please send your resumes to careers@advantecs.com Please send your resumes to careers@advantecs.com careers@advantecs.com",N/A,1 to 50 Employees,N/A,Company - Private,Enterprise Software & Network Solutions,Information Technology,Less than $1 million (USD)
"Palo Alto, CA",Data Engineer,$100K - $143K (Glassdoor est.),Luminar,"Luminar is a global automotive technology company ushering in a new era of vehicle safety and autonomy. For the past decade, Luminar has built an advanced hardware and software platform to enable its more than 50 industry partners, including the majority of global automotive OEMs. From Volvo Cars and Mercedes-Benz for consumer vehicles and Daimler Trucks for commercial trucks, to tech partners NVIDIA and Intel's Mobileye, Luminar is poised to be the first automotive technology company to enable next-generation safety and autonomous capabilities for production vehicles. For more information please visit www.luminartech.com. Team Overview The Data Engineering team builds the apps and infrastructure to leverage data from our vehicles and operations. We are building the next generation data lake and pipelines as we scale our offerings. You will develop and implement the systems that power Luminar's data-driven improvement and growth. Responsibilities Designing, building, and maintaining the infrastructure that transforms data at scale into insights: Ingestion of real-world vehicle data. Automated labeling and data enrichment. Generation of ground truth information. Analysis of data quality. Partnering with engineering and ML teams to define data consumption patterns and establish best practices. Establishing robust data integrity and systems monitoring. Minimum Qualifications 4+ years of relevant industry experience: Building backend / data services at scale. Hands-on Big Data software and infrastructure development skills. Experience with NoSQL databases and/or Relational databases. Accomplished in at least one backend language, preferably Python. Strong communication skills, being able to collaborate with the wider software organization. Bachelor's Degree (or higher) in Computer Science. Preferred Qualifications Proficiency in Python. Experience with MongoDB. Experience with pipeline / workflow frameworks: Argo Workflows or Airflow. Hands-on experience with cloud computing: AWS / GCP / Azure / Alibaba Cloud. Experience with automation of data infrastructure: Docker / Kubernetes / Terraform / Linux. Experience with SRE practices - running services reliably in production environments: Incident management and root cause analysis. Performance Engineering. Previous experience in the Automotive / Robotics industry. Nice To Have Proficiency in front-end development, preferably using React. Experience with processing frameworks like Spark / Hadoop. Experience with big data / data warehouse tools like Redshift, Presto/Athena, BigQuery and Snowflake. Experience with streaming and message queueing technologies: Kafka / Kinesis / RabbitMQ. Hands-on experience with data storage and transfer schemes like Avro, Parquet and Protobuf. Experience with rosbags. Benefits and Perks Competitive Pay, 401k Matching and meaningful Equity from a publicly traded company Great Medical Benefits Tuition Assistance, Meals served in the office No limit on PTO Flexible Work Schedule with some Work From Home flexibility as well  Industry: Autonomous vehicles, automotive, product demonstration Employment Type: Full-time Job Functions: Engineering  Luminar is an equal opportunity employer. All applicants will be considered for employment without regard to race, color, ancestry, national origin, sex, gender, sexual orientation, marital status, religion, age, disability, gender identity, results of genetic testing, service in the military, or any other characteristic protected by applicable federal, state or local laws. We will make a reasonable accommodation for any qualified applicant with a disability, provided that the individual is otherwise qualified to safely perform the essential functions of the job with or without accommodation and that the accommodation would not impose an undue hardship on the operation of our business. Please let us know if you believe you require reasonable accommodation, or if you would like assistance to complete an application or to participate in an interview at the company.  At Luminar, your base pay is one part of your total compensation package. This role pays a base between $142,000 and $199,000* per year. Within this range, individual pay is determined by work location and additional factors, including job-related skills, experience, and relevant education or training. You will also be eligible to receive other benefits including: equity in the form of restricted stock unit awards, comprehensive medical and dental coverage, 401k plan, life and disability benefits, flexible time off, paid parental leave, and tuition reimbursement for formal education related to advancing your career at Luminar. The specific programs and options available to an employee may vary depending on date of hire and schedule type. Note that the pay range listed for this position is a good faith and reasonable estimate of the range of possible base compensation at the time of posting. Luminar is a global automotive technology company ushering in a new era of vehicle safety and autonomy. For the past decade, Luminar has built an advanced hardware and software platform to enable its more than 50 industry partners, including the majority of global automotive OEMs. From Volvo Cars and Mercedes-Benz for consumer vehicles and Daimler Trucks for commercial trucks, to tech partners NVIDIA and Intel's Mobileye, Luminar is poised to be the first automotive technology company to enable next-generation safety and autonomous capabilities for production vehicles. For more information please visit www.luminartech.com. Team Overview Team Overview The Data Engineering team builds the apps and infrastructure to leverage data from our vehicles and operations. We are building the next generation data lake and pipelines as we scale our offerings. You will develop and implement the systems that power Luminar's data-driven improvement and growth. Responsibilities Responsibilities Designing, building, and maintaining the infrastructure that transforms data at scale into insights: Ingestion of real-world vehicle data. Automated labeling and data enrichment. Generation of ground truth information. Analysis of data quality. Partnering with engineering and ML teams to define data consumption patterns and establish best practices. Establishing robust data integrity and systems monitoring. Designing, building, and maintaining the infrastructure that transforms data at scale into insights: Ingestion of real-world vehicle data. Automated labeling and data enrichment. Generation of ground truth information. Analysis of data quality. Ingestion of real-world vehicle data. Automated labeling and data enrichment. Generation of ground truth information. Analysis of data quality. Partnering with engineering and ML teams to define data consumption patterns and establish best practices. Establishing robust data integrity and systems monitoring. Minimum Qualifications Minimum Qualifications 4+ years of relevant industry experience: Building backend / data services at scale. Hands-on Big Data software and infrastructure development skills. Experience with NoSQL databases and/or Relational databases. Accomplished in at least one backend language, preferably Python. Strong communication skills, being able to collaborate with the wider software organization. Bachelor's Degree (or higher) in Computer Science. 4+ years of relevant industry experience: Building backend / data services at scale. Hands-on Big Data software and infrastructure development skills. Experience with NoSQL databases and/or Relational databases. Building backend / data services at scale. Hands-on Big Data software and infrastructure development skills. Experience with NoSQL databases and/or Relational databases. Accomplished in at least one backend language, preferably Python. Strong communication skills, being able to collaborate with the wider software organization. Bachelor's Degree (or higher) in Computer Science. Preferred Qualifications Preferred Qualifications Proficiency in Python. Experience with MongoDB. Experience with pipeline / workflow frameworks: Argo Workflows or Airflow. Hands-on experience with cloud computing: AWS / GCP / Azure / Alibaba Cloud. Experience with automation of data infrastructure: Docker / Kubernetes / Terraform / Linux. Experience with SRE practices - running services reliably in production environments: Incident management and root cause analysis. Performance Engineering. Previous experience in the Automotive / Robotics industry. Proficiency in Python. Experience with MongoDB. Experience with pipeline / workflow frameworks: Argo Workflows or Airflow. Hands-on experience with cloud computing: AWS / GCP / Azure / Alibaba Cloud. Experience with automation of data infrastructure: Docker / Kubernetes / Terraform / Linux. Experience with SRE practices - running services reliably in production environments: Incident management and root cause analysis. Performance Engineering. Incident management and root cause analysis. Performance Engineering. Previous experience in the Automotive / Robotics industry. Nice To Have Nice To Have Proficiency in front-end development, preferably using React. Experience with processing frameworks like Spark / Hadoop. Experience with big data / data warehouse tools like Redshift, Presto/Athena, BigQuery and Snowflake. Experience with streaming and message queueing technologies: Kafka / Kinesis / RabbitMQ. Hands-on experience with data storage and transfer schemes like Avro, Parquet and Protobuf. Experience with rosbags. Proficiency in front-end development, preferably using React. Experience with processing frameworks like Spark / Hadoop. Experience with big data / data warehouse tools like Redshift, Presto/Athena, BigQuery and Snowflake. Experience with streaming and message queueing technologies: Kafka / Kinesis / RabbitMQ. Hands-on experience with data storage and transfer schemes like Avro, Parquet and Protobuf. Experience with rosbags. Benefits and Perks Competitive Pay, 401k Matching and meaningful Equity from a publicly traded company Great Medical Benefits Tuition Assistance, Meals served in the office No limit on PTO Flexible Work Schedule with some Work From Home flexibility as well  Industry: Autonomous vehicles, automotive, product demonstration Employment Type: Full-time Job Functions: Engineering  Luminar is an equal opportunity employer. All applicants will be considered for employment without regard to race, color, ancestry, national origin, sex, gender, sexual orientation, marital status, religion, age, disability, gender identity, results of genetic testing, service in the military, or any other characteristic protected by applicable federal, state or local laws. We will make a reasonable accommodation for any qualified applicant with a disability, provided that the individual is otherwise qualified to safely perform the essential functions of the job with or without accommodation and that the accommodation would not impose an undue hardship on the operation of our business. Please let us know if you believe you require reasonable accommodation, or if you would like assistance to complete an application or to participate in an interview at the company.  At Luminar, your base pay is one part of your total compensation package. This role pays a base between $142,000 and $199,000* per year. Within this range, individual pay is determined by work location and additional factors, including job-related skills, experience, and relevant education or training. You will also be eligible to receive other benefits including: equity in the form of restricted stock unit awards, comprehensive medical and dental coverage, 401k plan, life and disability benefits, flexible time off, paid parental leave, and tuition reimbursement for formal education related to advancing your career at Luminar. The specific programs and options available to an employee may vary depending on date of hire and schedule type. Note that the pay range listed for this position is a good faith and reasonable estimate of the range of possible base compensation at the time of posting. Benefits and Perks Benefits and Perks Competitive Pay, 401k Matching and meaningful Equity from a publicly traded company Great Medical Benefits Tuition Assistance, Meals served in the office No limit on PTO Flexible Work Schedule with some Work From Home flexibility as well Competitive Pay, 401k Matching and meaningful Equity from a publicly traded company Great Medical Benefits Tuition Assistance, Meals served in the office No limit on PTO Flexible Work Schedule with some Work From Home flexibility as well Industry: Autonomous vehicles, automotive, product demonstration Industry Employment Type: Full-time Employment Type Job Functions: Engineering Job Functions Luminar is an equal opportunity employer. All applicants will be considered for employment without regard to race, color, ancestry, national origin, sex, gender, sexual orientation, marital status, religion, age, disability, gender identity, results of genetic testing, service in the military, or any other characteristic protected by applicable federal, state or local laws. We will make a reasonable accommodation for any qualified applicant with a disability, provided that the individual is otherwise qualified to safely perform the essential functions of the job with or without accommodation and that the accommodation would not impose an undue hardship on the operation of our business. Please let us know if you believe you require reasonable accommodation, or if you would like assistance to complete an application or to participate in an interview at the company. Luminar is an equal opportunity employer. All applicants will be considered for employment without regard to race, color, ancestry, national origin, sex, gender, sexual orientation, marital status, religion, age, disability, gender identity, results of genetic testing, service in the military, or any other characteristic protected by applicable federal, state or local laws. We will make a reasonable accommodation for any qualified applicant with a disability, provided that the individual is otherwise qualified to safely perform the essential functions of the job with or without accommodation and that the accommodation would not impose an undue hardship on the operation of our business. Please let us know if you believe you require reasonable accommodation, or if you would like assistance to complete an application or to participate in an interview at the company. At Luminar, your base pay is one part of your total compensation package. This role pays a base between $142,000 and $199,000* per year. Within this range, individual pay is determined by work location and additional factors, including job-related skills, experience, and relevant education or training. You will also be eligible to receive other benefits including: equity in the form of restricted stock unit awards, comprehensive medical and dental coverage, 401k plan, life and disability benefits, flexible time off, paid parental leave, and tuition reimbursement for formal education related to advancing your career at Luminar. The specific programs and options available to an employee may vary depending on date of hire and schedule type. At Luminar, your base pay is one part of your total compensation package. This role pays a base between $142,000 and $199,000* per year. Within this range, individual pay is determined by work location and additional factors, including job-related skills, experience, and relevant education or training. You will also be eligible to receive other benefits including: equity in the form of restricted stock unit awards, comprehensive medical and dental coverage, 401k plan, life and disability benefits, flexible time off, paid parental leave, and tuition reimbursement for formal education related to advancing your career at Luminar. The specific programs and options available to an employee may vary depending on date of hire and schedule type. Note that the pay range listed for this position is a good faith and reasonable estimate of the range of possible base compensation at the time of posting. Note that the pay range listed for this position is a good faith and reasonable estimate of the range of possible base compensation at the time of posting. Note that the pay range listed for this position is a good faith and reasonable estimate of the range of possible base compensation at the time of posting.",3.8,201 to 500 Employees,2012,Company - Public,Computer Hardware Development,Information Technology,Unknown / Non-Applicable
"Dearborn, MI",Data Engineer,$75K - $100K (Glassdoor est.),Optimal Inc.,"Position Description: Create data tools for data scientist team members that assist them in building and optimizing our products Product And Requirements Management: Participate in and/or lead the development of requirements, features, user stories, use cases, and test cases. Participate in stand-up operations meetings. Analyze and verify requirements for completeness, consistency, comprehensibility, feasibility, and conformity to standards. Operations: Generate Metrics, Perform User Access Authorization, Perform Password Maintenance for Data Catalog team jobs, and Build Deployment Pipelines. Create detailed business analysis, outline challenges, opportunities, and solutions for the team and applications Incident, Problem, And Change/Service Requests: Participate and/or lead incident, problem, change and service request-related activities. Includes root cause analysis (RCA). Includes proactive problem management/defect prevention activities. Design/Develop/Test/Deploy: Work with the Business Customer, Product Owner, Architects, Product Designer, Software Engineers, and Security Controls Champion on solution design, development, and deployment Plan and coordinate user acceptance testing of new application feature. Skills and Experience Required: 3+ years of Qliksense/Qlikview experience 3+ years of Java and API experience 3+ years' experience with GIT 3+ years with SQL language and database management 3+ years' experience developing process documentation and reports. Big data platform experience (Google Cloud Platform) Strong critical and analytical thinking skills Knowledge of automotive industry business processes Programming in Python with data visualization experience, Java Proficient Data management and processing technology skills including relational (PostgreSQL, SQL Server) and non-relational database technologies (Hive, Spark) Education Required: Bachelor's or Master's degree in computer science, STEM or related fields Position Description: Position Description: Create data tools for data scientist team members that assist them in building and optimizing our products Product And Requirements Management: Participate in and/or lead the development of requirements, features, user stories, use cases, and test cases. Participate in stand-up operations meetings. Analyze and verify requirements for completeness, consistency, comprehensibility, feasibility, and conformity to standards. Operations: Generate Metrics, Perform User Access Authorization, Perform Password Maintenance for Data Catalog team jobs, and Build Deployment Pipelines. Create detailed business analysis, outline challenges, opportunities, and solutions for the team and applications Incident, Problem, And Change/Service Requests: Participate and/or lead incident, problem, change and service request-related activities. Includes root cause analysis (RCA). Includes proactive problem management/defect prevention activities. Design/Develop/Test/Deploy: Work with the Business Customer, Product Owner, Architects, Product Designer, Software Engineers, and Security Controls Champion on solution design, development, and deployment Plan and coordinate user acceptance testing of new application feature. Create data tools for data scientist team members that assist them in building and optimizing our products Product And Requirements Management: Participate in and/or lead the development of requirements, features, user stories, use cases, and test cases. Participate in stand-up operations meetings. Analyze and verify requirements for completeness, consistency, comprehensibility, feasibility, and conformity to standards. Operations: Generate Metrics, Perform User Access Authorization, Perform Password Maintenance for Data Catalog team jobs, and Build Deployment Pipelines. Create detailed business analysis, outline challenges, opportunities, and solutions for the team and applications Incident, Problem, And Change/Service Requests: Participate and/or lead incident, problem, change and service request-related activities. Includes root cause analysis (RCA). Includes proactive problem management/defect prevention activities. Design/Develop/Test/Deploy: Work with the Business Customer, Product Owner, Architects, Product Designer, Software Engineers, and Security Controls Champion on solution design, development, and deployment Plan and coordinate user acceptance testing of new application feature. Skills and Experience Required: Skills and Experience Required: 3+ years of Qliksense/Qlikview experience 3+ years of Java and API experience 3+ years' experience with GIT 3+ years with SQL language and database management 3+ years' experience developing process documentation and reports. Big data platform experience (Google Cloud Platform) Strong critical and analytical thinking skills Knowledge of automotive industry business processes Programming in Python with data visualization experience, Java Proficient Data management and processing technology skills including relational (PostgreSQL, SQL Server) and non-relational database technologies (Hive, Spark) 3+ years of Qliksense/Qlikview experience 3+ years of Java and API experience 3+ years' experience with GIT 3+ years with SQL language and database management 3+ years' experience developing process documentation and reports. Big data platform experience (Google Cloud Platform) Strong critical and analytical thinking skills Knowledge of automotive industry business processes Programming in Python with data visualization experience, Java Proficient Data management and processing technology skills including relational (PostgreSQL, SQL Server) and non-relational database technologies (Hive, Spark) Education Required: Education Required: Bachelor's or Master's degree in computer science, STEM or related fields Bachelor's or Master's degree in computer science, STEM or related fields",3.6,1 to 50 Employees,2004,Nonprofit Organization,Education & Training Services,Education,Unknown / Non-Applicable
"Edison, NJ",Senior Data Engineer,$43.96 - $70.65 Per Hour (Employer est.),DiamondPick,"Hi , Greetings from Diamond pick inc. We are currently looking for the below position for one of our clients... Please let me know your interest along with your updated resume ASAP.. Role:Data engineer Role:Data engineer Location: Berkley Heights, NJ(Onsite)(locals only) Location: Berkley Heights, NJ(Onsite)(locals only) 9+ years of experience is must 9+ years of experience is must Description Skills: strong Java,Azure,Spark & sql Skills: strong Java,Azure,Spark & sql Company Description: Apexon is a digital-first technology services firm backed by Goldman Sachs Asset Management and Everstone Capital. We specialize in accelerating business transformation and delivering human-centric digital experiences. For over 17 years, Apexon has been meeting customers wherever they are in the digital lifecycle and helping them outperform their competition through speed and innovation. RESPONSIBILITIES Basic Qualifications for consideration: 5+ Overall industry experience 3+ years' experience with building large scale big data applications development Bachelors in Computer Science or related field Provide technical leadership in developing data solutions and building frameworks Experience building Data Lake, data applications and micro-services using Azure, AWS and Hands-on experience in cloud Data stack (preference is Azure) Hands-on experience with Springboot framework, MVC architecture, Hibernate, API creation, Restful services, SOAP using Java Java experience with OOPS concepts, multithreading Experience deploying code on containers Hands on experience with leveraging CI/CD to rapidly build & test application code Conduct code reviews and strive for improvement in software engineering quality Review data architecture and develop detailed implementation design Hands-on experience in production rollout and infrastructure configuration Demonstrable experience of successfully delivering big data projects using Kafka, Spark Experience working on NoSQL Databases such as Cassandra, HBase, DynamoDB, and Elastic Search Experience working with PCI Data and working with data scientists is a plus In depth knowledge of design principles and patterns Able to tune big data solutions to improve performance 5+ Overall industry experience 3+ years' experience with building large scale big data applications development Bachelors in Computer Science or related field Provide technical leadership in developing data solutions and building frameworks Experience building Data Lake, data applications and micro-services using Azure, AWS and Hands-on experience in cloud Data stack (preference is Azure) Hands-on experience with Springboot framework, MVC architecture, Hibernate, API creation, Restful services, SOAP using Java Java experience with OOPS concepts, multithreading Experience deploying code on containers Hands on experience with leveraging CI/CD to rapidly build & test application code Conduct code reviews and strive for improvement in software engineering quality Review data architecture and develop detailed implementation design Hands-on experience in production rollout and infrastructure configuration Demonstrable experience of successfully delivering big data projects using Kafka, Spark Experience working on NoSQL Databases such as Cassandra, HBase, DynamoDB, and Elastic Search Experience working with PCI Data and working with data scientists is a plus In depth knowledge of design principles and patterns Able to tune big data solutions to improve performance QualificationsBachelor's Degree in Computer Science or Computer Engineering is required Job Type: Contract Salary: $43.96 - $70.65 per hour Ability to commute/relocate: Edison, NJ 08817: Reliably commute or planning to relocate before starting work (Required) Edison, NJ 08817: Reliably commute or planning to relocate before starting work (Required) Experience: SQL (Required) Java (Required) Azure (Required) SQL (Required) Java (Required) Azure (Required) Work Location: In person",4.5,1001 to 5000 Employees,2018,Company - Private,HR Consulting,Human Resources & Staffing,Unknown / Non-Applicable
"Jersey City, NJ",Sr. Data Engineer,$104K - $146K (Glassdoor est.),InterSources Inc,"Role: Senior Data Engineer Role: Senior Data Engineer Location: Jersey City, NJ (onsite) Location: Jersey City, NJ (onsite) Duration: Full-time Duration: Full-time Job summary: Job summary: Developer with strong technical ability with 10+ years of experience in Java/J2EE design and development Experienced in working on medium to large enterprise projects, preferably in financial services Should have knowledge on Apache Spark framework. Must have knowledge on HBase Should have basic knowledge on Bigdata Cluster and operations Person should have worked in Agile/DevOps Environment Good communication skills Developer with strong technical ability with 10+ years of experience in Java/J2EE design and development Experienced in working on medium to large enterprise projects, preferably in financial services Should have knowledge on Apache Spark framework. Must have knowledge on HBase Should have basic knowledge on Bigdata Cluster and operations Person should have worked in Agile/DevOps Environment Good communication skills Key Responsibilities: Key Responsibilities: Experience with developing software that processes, persists and distributes data via relational and non-relational technologies Employ standards, frameworks and patterns while designing and developing components Develop high quality code employing software engineering and testing best practices Converse with various data provider and consumer applications in their languages/terminologies Partner with database developers to implement ingestion, orchestration, quality/reconciliation and distribution services Experience with developing software that processes, persists and distributes data via relational and non-relational technologies Employ standards, frameworks and patterns while designing and developing components Develop high quality code employing software engineering and testing best practices Converse with various data provider and consumer applications in their languages/terminologies Partner with database developers to implement ingestion, orchestration, quality/reconciliation and distribution services Skills Required: Skills Required: Experience with developing software that processes, persists and distributes data via relational and non-relational technologies: Strong pySpark/Java Skills Experience in design and development of batch/real time Spark processing pipelines. Knowledge of Spark framework – Core Spark, Spark Data Frames, Spark streaming, pyspark Knowledge of Bigdata Cluster and operations. Experience with developing software that processes, persists and distributes data via relational and non-relational technologies: Strong pySpark/Java Skills Experience in design and development of batch/real time Spark processing pipelines. Knowledge of Spark framework – Core Spark, Spark Data Frames, Spark streaming, pyspark Knowledge of Bigdata Cluster and operations. Good to Have: Good to Have: Have basic experience in Data Preparation Tools Experience with CI/CD build pipelines and toolchain – Git, BitBucket, TeamCity, Artifactory, Jira Experience with testing concepts (TDD, BDD) and frameworks (Cucumber, Selenium, FluentLenium, Junit) Experience with container technologies (Docker, Pivotal Cloud Foundry) and supporting frameworks (Kubernetes, OpenShift, Mesos) Knowledge of Operating Systems and familiar with shell scripting Databricks Certified Associate Developer for Apache Spark, Python Institute - Certified Associate in Python Programming, Oracle Database SQL Certified Associate, Oracle Certified Associate - Java SE 8 Programmer, Cloudera - CDP Data Analyst Have basic experience in Data Preparation Tools Experience with CI/CD build pipelines and toolchain – Git, BitBucket, TeamCity, Artifactory, Jira Experience with testing concepts (TDD, BDD) and frameworks (Cucumber, Selenium, FluentLenium, Junit) Experience with container technologies (Docker, Pivotal Cloud Foundry) and supporting frameworks (Kubernetes, OpenShift, Mesos) Knowledge of Operating Systems and familiar with shell scripting Databricks Certified Associate Developer for Apache Spark, Python Institute - Certified Associate in Python Programming, Oracle Database SQL Certified Associate, Oracle Certified Associate - Java SE 8 Programmer, Cloudera - CDP Data Analyst Job Type: Full-time Experience level: 10 years 11+ years 10 years 11+ years Ability to commute/relocate: Jersey City, NJ 07302: Reliably commute or planning to relocate before starting work (Required) Jersey City, NJ 07302: Reliably commute or planning to relocate before starting work (Required) Experience: Java/J2EE: 8 years (Preferred) PySpark/Apache Spark: 8 years (Preferred) HBase: 10 years (Preferred) Big data: 6 years (Preferred) Agile/Devops Environment: 6 years (Preferred) Java/J2EE: 8 years (Preferred) PySpark/Apache Spark: 8 years (Preferred) HBase: 10 years (Preferred) Big data: 6 years (Preferred) Agile/Devops Environment: 6 years (Preferred) Work Location: In person",4.3,51 to 200 Employees,2007,Company - Private,Information Technology Support Services,Information Technology,$5 to $25 million (USD)
Remote,Senior Data Engineer,N/A,Quadrant Resource,"JOB DESCRIPTION: JOB DESCRIPTION: Need to be expertise Coding on PySpark. Supply Chain and Azure. Experience in working with end-to-end technology implementation project, preferable those related to Data warehousing/Analytics/Reporting Strong communication skills with the ability to collaborate effectively with business owners/users and technical teams. Proven ability to gather product requirements from stakeholders and prioritize feature implementations based on business needs and product strategy. Strong problem-solving skills and attention to detail. Good to have experience with CPG/ Retail industry (preferable CPG companies such as Nestle, Unilever, P&G, Coca coal, PepsiCo etc) Bachelor's/master's degree in business administration, computer science, engineering, or a related field. Need to be expertise Coding on PySpark. Supply Chain and Azure. Experience in working with end-to-end technology implementation project, preferable those related to Data warehousing/Analytics/Reporting Strong communication skills with the ability to collaborate effectively with business owners/users and technical teams. Proven ability to gather product requirements from stakeholders and prioritize feature implementations based on business needs and product strategy. Strong problem-solving skills and attention to detail. Good to have experience with CPG/ Retail industry (preferable CPG companies such as Nestle, Unilever, P&G, Coca coal, PepsiCo etc) Bachelor's/master's degree in business administration, computer science, engineering, or a related field. Job Type: Contract Experience: PySpark: 5 years (Preferred) SQL: 10 years (Preferred) Data analytics: 5 years (Preferred) Data warehouse: 5 years (Preferred) Azure: 5 years (Preferred) Data Bricks: 5 years (Preferred) Data Lake: 5 years (Preferred) Synapse: 5 years (Preferred) Retail Domain: 8 years (Preferred) CPG: 7 years (Preferred) PySpark: 5 years (Preferred) SQL: 10 years (Preferred) Data analytics: 5 years (Preferred) Data warehouse: 5 years (Preferred) Azure: 5 years (Preferred) Data Bricks: 5 years (Preferred) Data Lake: 5 years (Preferred) Synapse: 5 years (Preferred) Retail Domain: 8 years (Preferred) CPG: 7 years (Preferred) Work Location: Remote",4.1,1001 to 5000 Employees,2004,Company - Private,Information Technology Support Services,Information Technology,Unknown / Non-Applicable
Remote,Data Engineer,N/A,clearAvenue,"Data Engineer Job Description The Data Engineer will build, extract, transform, load (ETL) pipelines to enable full spectrum data operations from ingest to query within a team that is responsible for developing and sustaining a cloud-based platform that enables cyber operations across the contract’s agency. This position will support a team to coordinate and integrate work across legacy Big Data Platforms and assist the team with the overall goal of developing new applications, analytics, and services to modernize capabilities through an agile, evolving requirement identification and prioritization process.  Responsibilities: Design and implement data solutions using industry best practices. Performs ETL, ELT operations and administration of data and systems securely and in accordance with enterprise data governance standards. Monitor and maintain data pipelines proactively to ensure high service availability. Works with Data Scientists and ML Engineers to understand mathematical models and optimize data solutions accordingly. Continuous development through training and mentorship programs. Create scripts and programs to automate data operations.  Requirements and Qualifications: Minimum Bachelor’s Degree in Data Science, Engineering, Computer Science, or related fields. 5 years of minimum total relevant experience. Current DoD 8570 training and certifications such as a Security +, CISM, CISP, etc. Experienced working in an Agile Scrum environment. Experienced working in a cloud-based and AI/Machine Learning data technologies. Experienced working in a government contract environment, ideally with the Department of Defense. Thorough understanding of the responsibilities and duties of a data engineer, as well as established industry standards/best practices and documentation guidelines. Excellent problem-solving skills and ability to learn through scattered resources. Outstanding communication skills, and the ability to stay self-motivated and work with little or no supervision. Clearance Requirements: Active Top Secret Security clearance with SCI eligibility Job Types: Full-time, Contract  Other Duties Please note this job description is not designed to cover or contain a comprehensive listing of activities, duties or responsibilities that are required of the employee for this job. Duties, responsibilities and activities may change at any time with or without notice.  clearAvenue, LLC is an equal opportunity employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, or disability status, protected veteran status, or any other characteristic protected by law. Data Engineer Job Description Responsibilities: Design and implement data solutions using industry best practices. Performs ETL, ELT operations and administration of data and systems securely and in accordance with enterprise data governance standards. Monitor and maintain data pipelines proactively to ensure high service availability. Works with Data Scientists and ML Engineers to understand mathematical models and optimize data solutions accordingly. Continuous development through training and mentorship programs. Create scripts and programs to automate data operations. Design and implement data solutions using industry best practices. Performs ETL, ELT operations and administration of data and systems securely and in accordance with enterprise data governance standards. Monitor and maintain data pipelines proactively to ensure high service availability. Works with Data Scientists and ML Engineers to understand mathematical models and optimize data solutions accordingly. Continuous development through training and mentorship programs. Create scripts and programs to automate data operations. Requirements and Qualifications: Minimum Bachelor’s Degree in Data Science, Engineering, Computer Science, or related fields. 5 years of minimum total relevant experience. Current DoD 8570 training and certifications such as a Security +, CISM, CISP, etc. Experienced working in an Agile Scrum environment. Experienced working in a cloud-based and AI/Machine Learning data technologies. Experienced working in a government contract environment, ideally with the Department of Defense. Thorough understanding of the responsibilities and duties of a data engineer, as well as established industry standards/best practices and documentation guidelines. Excellent problem-solving skills and ability to learn through scattered resources. Outstanding communication skills, and the ability to stay self-motivated and work with little or no supervision. Minimum Bachelor’s Degree in Data Science, Engineering, Computer Science, or related fields. 5 years of minimum total relevant experience. Current DoD 8570 training and certifications such as a Security +, CISM, CISP, etc. Experienced working in an Agile Scrum environment. Experienced working in a cloud-based and AI/Machine Learning data technologies. Experienced working in a government contract environment, ideally with the Department of Defense. Thorough understanding of the responsibilities and duties of a data engineer, as well as established industry standards/best practices and documentation guidelines. Excellent problem-solving skills and ability to learn through scattered resources. Outstanding communication skills, and the ability to stay self-motivated and work with little or no supervision. Clearance Requirements: Active Top Secret Security clearance with SCI eligibility Active Top Secret Security clearance with SCI eligibility Job Types: Other Duties clearAvenue, LLC is an equal opportunity employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, or disability status, protected veteran status, or any other characteristic protected by law.",4.0,Unknown,N/A,Company - Private,Software Development,Information Technology,Unknown / Non-Applicable
Remote,Azure Data Engineer,$65.00 - $70.00 Per Hour (Employer est.),Apptad Inc,"Job Title: Azure Data Engineer Job Location: Remote Job Duration: Long-Term Job Title: Job Location: Job Duration: Job Description Job Description Snowflake- data masking Snowflake-security Snowflake- data ingestion from diff sources Snowflake- data model, scalability Fivetran- as orchestration How to bring data from salesforce to snowflake – there is a new connector in preview , explore more How to build data mesh architecture. A strong foundation in designing and building data pipelines, integrating diverse data sources, and implementing efficient data processing frameworks. Proficiency in programming languages Python, SQL, and used snowflake (data masking, security etc) ADF for creating data pipelines. Strong problem-solving abilities, attention to detail in data engineering needs. Snowflake- data masking Snowflake-security Snowflake- data ingestion from diff sources Snowflake- data model, scalability Fivetran- as orchestration How to bring data from salesforce to snowflake – there is a new connector in preview , explore more How to build data mesh architecture. A strong foundation in designing and building data pipelines, integrating diverse data sources, and implementing efficient data processing frameworks. Proficiency in programming languages Python, SQL, and used snowflake (data masking, security etc) ADF for creating data pipelines. Strong problem-solving abilities, attention to detail in data engineering needs. * Job Type: Full-time Salary: $65.00 - $70.00 per hour Experience level: 9 years 9 years Experience: Snowflake: 6 years (Preferred) Azure: 6 years (Preferred) Python: 3 years (Preferred) SQL: 3 years (Preferred) Snowflake: 6 years (Preferred) Azure: 6 years (Preferred) Python: 3 years (Preferred) SQL: 3 years (Preferred) Work Location: Remote",4.4,201 to 500 Employees,2013,Company - Private,Information Technology Support Services,Information Technology,$25 to $100 million (USD)
"Nashville, TN",Data Engineer,$81K - $115K (Glassdoor est.),Wellpath,"You Matter: Make a difference every day in the lives of the underserved Join a mission driven organization with a people first culture Excellent career growth opportunities  Join us and find a career that supports: Caring for overlooked, underserved, and vulnerable patients Diversity, equity, inclusion, and belonging Autonomy in a warm team environment Growth and training  Perks and Benefits In addition to comprehensive benefits including medical, dental, vision, paid time off, and 401k, we foster a work, life balance for team members and their family to support physical, mental, and financial wellbeing including: DailyPay, receive your money as you earn it! Tuition Assistance and dependent Scholarships Employee Assistance Program (EAP) including free counseling and health coaching Company paid life insurance Tax free Health Spending Accounts (HSA) Wellness program featuring fitness memberships and product discounts Preferred banking partnership and discounted rates for home and auto loans Why Us: Now is your moment to make a difference in the lives of the underserved. If there is one unifying characteristic of everyone on our team, it is the deep desire to make a difference by helping society's most vulnerable and often overlooked individuals. Every day we have the distinct honor and responsibility to show up with non-judgmental compassion to provide hope and healing to those who need it most. For those whose calling it is to serve others, now is your moment to join our mission to provide quality care to every patient with compassion, collaboration, and innovation, to live our mantra to “Always Do The Right Thing!”, and to collectively do our part to heal the world, one patient at a time. How you make a difference: The Data Engineer helps cultivate a data-informed culture and discover insights that drive action. The position is responsible for building complex data pipelines within the organization, play a key role in understanding the various pillars of data interpreting that information into agnostic data models and providing the foundation for enriched analytics. This role will work closely with our BI teams synthesizing enterprise data and ensuring the solutions solve business challenges and enable trust with the highest quality. Key Responsibilities: Initiate cross-functional analyses, presenting findings and recommendations organization-wide. Create data integrations, centralizing internal and external sources into a Data Warehouse, while monitoring operations and resolving issues. Profile data sources, map to target formats, and collaborate with Data Governance for quality and integrity solutions. Foster positive relationships, mentor junior team members, and promote teamwork, adaptability, and accountability. Apply critical thinking, sound judgment, and attention to detail in various situations, maintaining professionalism and embracing change positively. Qualifications & Requirements: Education Relevant Bachelor's Degree or equivalent professional work experience. Experience Strong knowledge and understanding of cloud based data warehouses (Snowflake a plus) Strong knowledge of data models (normalized, dimensional) Strong knowledge of SQL (you’ll use it every day) Strong knowledge using ETL tools (SSIS, Snowpipe, etc) Extreme Passion for solving problems Healthcare data experience a plus Licenses/Certifications None We are an Equal Employment Opportunity / Affirmative Action Employer: We celebrate diversity and are committed to creating an inclusive environment for all employees.  We encourage you to apply! If you are excited about a role but your experience doesn’t seem to align perfectly with every element of the job description, we encourage you to apply. You may be just the right candidate for this, or one of our many other roles. Make a difference every day in the lives of the underserved Join a mission driven organization with a people first culture Excellent career growth opportunities Make a difference every day in the lives of the underserved Join a mission driven organization with a people first culture Excellent career growth opportunities Join us and find a career that supports: Caring for overlooked, underserved, and vulnerable patients Diversity, equity, inclusion, and belonging Autonomy in a warm team environment Growth and training Join us and find a career that supports: Caring for overlooked, underserved, and vulnerable patients Diversity, equity, inclusion, and belonging Autonomy in a warm team environment Growth and training Caring for overlooked, underserved, and vulnerable patients Diversity, equity, inclusion, and belonging Autonomy in a warm team environment Growth and training Perks and Benefits In addition to comprehensive benefits including medical, dental, vision, paid time off, and 401k, we foster a work, life balance for team members and their family to support physical, mental, and financial wellbeing including: DailyPay, receive your money as you earn it! Tuition Assistance and dependent Scholarships Employee Assistance Program (EAP) including free counseling and health coaching Company paid life insurance Tax free Health Spending Accounts (HSA) Wellness program featuring fitness memberships and product discounts Preferred banking partnership and discounted rates for home and auto loans Perks and Benefits DailyPay, receive your money as you earn it! Tuition Assistance and dependent Scholarships Employee Assistance Program (EAP) including free counseling and health coaching Company paid life insurance Tax free Health Spending Accounts (HSA) Wellness program featuring fitness memberships and product discounts Preferred banking partnership and discounted rates for home and auto loans DailyPay, receive your money as you earn it! Tuition Assistance and dependent Scholarships Employee Assistance Program (EAP) including free counseling and health coaching Company paid life insurance Tax free Health Spending Accounts (HSA) Wellness program featuring fitness memberships and product discounts Preferred banking partnership and discounted rates for home and auto loans Now is your moment to make a difference in the lives of the underserved. If there is one unifying characteristic of everyone on our team, it is the deep desire to make a difference by helping society's most vulnerable and often overlooked individuals. Every day we have the distinct honor and responsibility to show up with non-judgmental compassion to provide hope and healing to those who need it most. For those whose calling it is to serve others, now is your moment to join our mission to provide quality care to every patient with compassion, collaboration, and innovation, to live our mantra to “Always Do The Right Thing!”, and to collectively do our part to heal the world, one patient at a time. Now is your moment to make a difference in the lives of the underserved. The Data Engineer helps cultivate a data-informed culture and discover insights that drive action. The position is responsible for building complex data pipelines within the organization, play a key role in understanding the various pillars of data interpreting that information into agnostic data models and providing the foundation for enriched analytics. This role will work closely with our BI teams synthesizing enterprise data and ensuring the solutions solve business challenges and enable trust with the highest quality. Initiate cross-functional analyses, presenting findings and recommendations organization-wide. Create data integrations, centralizing internal and external sources into a Data Warehouse, while monitoring operations and resolving issues. Profile data sources, map to target formats, and collaborate with Data Governance for quality and integrity solutions. Foster positive relationships, mentor junior team members, and promote teamwork, adaptability, and accountability. Apply critical thinking, sound judgment, and attention to detail in various situations, maintaining professionalism and embracing change positively. Initiate cross-functional analyses, presenting findings and recommendations organization-wide. Create data integrations, centralizing internal and external sources into a Data Warehouse, while monitoring operations and resolving issues. Profile data sources, map to target formats, and collaborate with Data Governance for quality and integrity solutions. Foster positive relationships, mentor junior team members, and promote teamwork, adaptability, and accountability. Apply critical thinking, sound judgment, and attention to detail in various situations, maintaining professionalism and embracing change positively. Education Relevant Bachelor's Degree or equivalent professional work experience. Relevant Bachelor's Degree or equivalent professional work experience. Experience Strong knowledge and understanding of cloud based data warehouses (Snowflake a plus) Strong knowledge of data models (normalized, dimensional) Strong knowledge of SQL (you’ll use it every day) Strong knowledge using ETL tools (SSIS, Snowpipe, etc) Extreme Passion for solving problems Healthcare data experience a plus Strong knowledge and understanding of cloud based data warehouses (Snowflake a plus) Strong knowledge of data models (normalized, dimensional) Strong knowledge of SQL (you’ll use it every day) Strong knowledge using ETL tools (SSIS, Snowpipe, etc) Extreme Passion for solving problems Healthcare data experience a plus Licenses/Certifications None None We celebrate diversity and are committed to creating an inclusive environment for all employees. We encourage you to apply! If you are excited about a role but your experience doesn’t seem to align perfectly with every element of the job description, we encourage you to apply. You may be just the right candidate for this, or one of our many other roles. We encourage you to apply!",3.2,10000+ Employees,2003,Company - Private,Health Care Services & Hospitals,Healthcare,$1 to $5 billion (USD)
Remote,Senior Data Engineer,N/A,Signify Health,"A Senior Software Engineer - Data develops systems to manage data flow throughout Signify Health's infrastructure. This involves all elements of data engineering, such as ingestion, transformation, and distribution of data. What will you do? Communicate with business leaders to help translate requirements into functional specification Develop broad understanding of business logic and functionality of current systems Analyze and manipulate data by writing and running SQL queries Analyze logs to identify and prevent potential issues from occurring Deliver clean and functional code in accordance with business requirements Consume data from any source, such a flat files, streaming systems, or RESTful APIs Interface with Electronic Health Records Engineer scalable, reliable, and performant systems to manage data Collaborate closely with other Engineers, QA, Scrum master, Product Manager in your team as well as across the organization Build quality systems while expanding offerings to dependent teams Comfortable in multiple roles, from Design and Development to Code Deployment to and monitoring and investigating in production systems. Requirements Bachelors in Computer Science or equivalent Proven ability to complete projects in a timely manner while clearly measuring progress Strong software engineering fundamentals (data structures, algorithms, async programming patterns, object-oriented design, parallel programming) Strong understanding and demonstrated experience with at least one popular programming language (.NET or Java) and SQL constructs. Experience writing and maintaining frontend client applications, Angular preferred Strong experience with revision control (Git) Experience with cloud-based systems (Azure / AWS / GCP). High level understanding of big data design (data lake, data mesh, data warehouse) and data normalization patterns Demonstrated experience with Queuing technologies (Kafka / SNS / RabbitMQ etc) Demonstrated experience with Metrics, Logging, Monitoring and Alerting tools Strong communication skills Strong experience with use of RESTful APIs High level understanding of HL7 V2.x / FHIR based interface messages. High level understanding of system deployment tasks and technologies. (CI/CD Pipeline, K8s, Terraform) About Us: Signify Health is helping build the healthcare system we all want to experience by transforming the home into the healthcare hub. We coordinate care holistically across individuals' clinical, social, and behavioral needs so they can enjoy more healthy days at home. By building strong connections to primary care providers and community resources, we're able to close critical care and social gaps, as well as manage risk for individuals who need help the most. This leads to better outcomes and a better experience for everyone involved. Our high-performance networks are powered by more than 9,000 mobile doctors and nurses covering every county in the U.S., 3,500 healthcare providers and facilities in value-based arrangements, and hundreds of community-based organizations. Signify's intelligent technology and decision-support services enable these resources to radically simplify care coordination for more than 1.5 million individuals each year while helping payers and providers more effectively implement value-based care programs. To learn more about how we're driving outcomes and making healthcare work better, please visit us at www.signifyhealth.com Diversity and Inclusion are core values at Signify Health, and fostering a workplace culture reflective of that is critical to our continued success as an organization. We are committed to equal employment opportunities for employees and job applicants in compliance with applicable law and to an environment where employees are valued for their differences. A Senior Software Engineer - Data develops systems to manage data flow throughout Signify Health's infrastructure. This involves all elements of data engineering, such as ingestion, transformation, and distribution of data. What will you do? What will you do? Communicate with business leaders to help translate requirements into functional specification Develop broad understanding of business logic and functionality of current systems Analyze and manipulate data by writing and running SQL queries Analyze logs to identify and prevent potential issues from occurring Deliver clean and functional code in accordance with business requirements Consume data from any source, such a flat files, streaming systems, or RESTful APIs Interface with Electronic Health Records Engineer scalable, reliable, and performant systems to manage data Collaborate closely with other Engineers, QA, Scrum master, Product Manager in your team as well as across the organization Build quality systems while expanding offerings to dependent teams Comfortable in multiple roles, from Design and Development to Code Deployment to and monitoring and investigating in production systems. Communicate with business leaders to help translate requirements into functional specification Develop broad understanding of business logic and functionality of current systems Analyze and manipulate data by writing and running SQL queries Analyze logs to identify and prevent potential issues from occurring Deliver clean and functional code in accordance with business requirements Consume data from any source, such a flat files, streaming systems, or RESTful APIs Interface with Electronic Health Records Engineer scalable, reliable, and performant systems to manage data Collaborate closely with other Engineers, QA, Scrum master, Product Manager in your team as well as across the organization Build quality systems while expanding offerings to dependent teams Comfortable in multiple roles, from Design and Development to Code Deployment to and monitoring and investigating in production systems. Requirements Requirements Bachelors in Computer Science or equivalent Proven ability to complete projects in a timely manner while clearly measuring progress Strong software engineering fundamentals (data structures, algorithms, async programming patterns, object-oriented design, parallel programming) Strong understanding and demonstrated experience with at least one popular programming language (.NET or Java) and SQL constructs. Experience writing and maintaining frontend client applications, Angular preferred Strong experience with revision control (Git) Experience with cloud-based systems (Azure / AWS / GCP). High level understanding of big data design (data lake, data mesh, data warehouse) and data normalization patterns Demonstrated experience with Queuing technologies (Kafka / SNS / RabbitMQ etc) Demonstrated experience with Metrics, Logging, Monitoring and Alerting tools Strong communication skills Strong experience with use of RESTful APIs High level understanding of HL7 V2.x / FHIR based interface messages. High level understanding of system deployment tasks and technologies. (CI/CD Pipeline, K8s, Terraform) Bachelors in Computer Science or equivalent Proven ability to complete projects in a timely manner while clearly measuring progress Strong software engineering fundamentals (data structures, algorithms, async programming patterns, object-oriented design, parallel programming) Strong understanding and demonstrated experience with at least one popular programming language (.NET or Java) and SQL constructs. Experience writing and maintaining frontend client applications, Angular preferred Strong experience with revision control (Git) Experience with cloud-based systems (Azure / AWS / GCP). High level understanding of big data design (data lake, data mesh, data warehouse) and data normalization patterns Demonstrated experience with Queuing technologies (Kafka / SNS / RabbitMQ etc) Demonstrated experience with Metrics, Logging, Monitoring and Alerting tools Strong communication skills Strong experience with use of RESTful APIs High level understanding of HL7 V2.x / FHIR based interface messages. High level understanding of system deployment tasks and technologies. (CI/CD Pipeline, K8s, Terraform) About Us: Signify Health is helping build the healthcare system we all want to experience by transforming the home into the healthcare hub. We coordinate care holistically across individuals' clinical, social, and behavioral needs so they can enjoy more healthy days at home. By building strong connections to primary care providers and community resources, we're able to close critical care and social gaps, as well as manage risk for individuals who need help the most. This leads to better outcomes and a better experience for everyone involved. Our high-performance networks are powered by more than 9,000 mobile doctors and nurses covering every county in the U.S., 3,500 healthcare providers and facilities in value-based arrangements, and hundreds of community-based organizations. Signify's intelligent technology and decision-support services enable these resources to radically simplify care coordination for more than 1.5 million individuals each year while helping payers and providers more effectively implement value-based care programs. To learn more about how we're driving outcomes and making healthcare work better, please visit us at www.signifyhealth.com About Us: Diversity and Inclusion are core values at Signify Health, and fostering a workplace culture reflective of that is critical to our continued success as an organization. Diversity and Inclusion are core values at Signify Health, and fostering a workplace culture reflective of that is critical to our continued success as an organization. We are committed to equal employment opportunities for employees and job applicants in compliance with applicable law and to an environment where employees are valued for their differences. We are committed to equal employment opportunities for employees and job applicants in compliance with applicable law and to an environment where employees are valued for their differences.",3.2,1001 to 5000 Employees,2017,Company - Public,Health Care Services & Hospitals,Healthcare,Unknown / Non-Applicable
"Brentwood, TN",Azure Data Engineer,$70.00 Per Hour (Employer est.),Vision,"Job Description Seeking an experienced Azure Data Engineer to design and implement data solutions using Azure technologies. The ideal candidate will have a strong background in data modeling, ETL, and data warehousing, and ability in SQL, Python, and Azure data services such as Azure Data Factory, Azure Databricks, and Azure Synapse Analytics. Essential Functions Essential Functions Design and implement data pipelines using Azure Data Factory to move data from various sources to Azure Data Lake Storage and Azure SQL Database. Create and keep data models in Azure Synapse Analytics for reporting and analytics with Power BI. Work with data scientists and analysts to implement data solutions. Collaborate with the DevOps team to automate the deployment of data pipelines using Azure DevOps. Participate in data governance efforts to ensure data quality and compliance. Design and implement data pipelines using Azure Data Factory to move data from various sources to Azure Data Lake Storage and Azure SQL Database. Create and keep data models in Azure Synapse Analytics for reporting and analytics with Power BI. Work with data scientists and analysts to implement data solutions. Collaborate with the DevOps team to automate the deployment of data pipelines using Azure DevOps. Participate in data governance efforts to ensure data quality and compliance. Qualifications Qualifications 2-5+ years of experience as a data engineer or related role. Strong background in data modeling, ETL, and data warehousing. Proficient in SQL, Python, and Azure data services such as Azure Data Factory, Azure Databricks, and Azure Synapse Analytics. Experience using version control software, preferably git. Experience with Visual Studio, Azure DevOps, and Power BI is a plus. Microsoft Certified: Azure Data Engineer Associate or higher is a plus. Education: bachelor’s degree in computer science, data science, or a related field. Strong diligence. Excellent troubleshooting and communication skills Able to work well in a team setting and mentor others. 2-5+ years of experience as a data engineer or related role. Strong background in data modeling, ETL, and data warehousing. Proficient in SQL, Python, and Azure data services such as Azure Data Factory, Azure Databricks, and Azure Synapse Analytics. Experience using version control software, preferably git. Experience with Visual Studio, Azure DevOps, and Power BI is a plus. Microsoft Certified: Azure Data Engineer Associate or higher is a plus. Education: bachelor’s degree in computer science, data science, or a related field. Strong diligence. Excellent troubleshooting and communication skills Able to work well in a team setting and mentor others. Job Types: Full-time, Contract Pay: Up to $70.00 per hour Benefits: Health insurance Health insurance Ability to commute/relocate: Brentwood, TN: Reliably commute or planning to relocate before starting work (Required) Brentwood, TN: Reliably commute or planning to relocate before starting work (Required) Education: Bachelor's (Required) Bachelor's (Required) Experience: SQL: 2 years (Required) Data warehouse: 2 years (Required) Azure Data Engineering: 2 years (Required) design and implement data solutions: 2 years (Required) Azure technologies: 2 years (Required) data modeling, ETL, and data warehousing: 2 years (Required) SQL, Python, and Azure data services: 2 years (Required) Azure Databricks, and Azure Synapse Analytics: 2 years (Required) Azure Data Factory: 2 years (Required) version control software: 2 years (Required) Visual Studio, Azure DevOps, and Power BI: 2 years (Required) SQL: 2 years (Required) Data warehouse: 2 years (Required) Azure Data Engineering: 2 years (Required) design and implement data solutions: 2 years (Required) Azure technologies: 2 years (Required) data modeling, ETL, and data warehousing: 2 years (Required) SQL, Python, and Azure data services: 2 years (Required) Azure Databricks, and Azure Synapse Analytics: 2 years (Required) Azure Data Factory: 2 years (Required) version control software: 2 years (Required) Visual Studio, Azure DevOps, and Power BI: 2 years (Required) License/Certification: Microsoft Certified: Azure Data Engineer Associate or higher (Required) Microsoft Certified: Azure Data Engineer Associate or higher (Required) Work Location: In person",N/A,,,,,,
"Carlsbad, CA",Senior Data Engineer,$103K - $141K (Glassdoor est.),NTENT,"Senior Data Engineer Position: Full time Location: Carlsbad office (Initially Remote) About Us: NTENT provides a Platform-as-a-Service (PaaS), allowing industry partners to customize, localize and integrate Voice Assistant and Search technologies directly into their business-to-consumer offerings. NTENT utilizes a machine learning algorithmic approach to comprehend massive amounts of information across the web. Through the company’s proprietary search engine, ontology and knowledge graph, NTENT makes it easier to decipher meaning, surfacing the most relevant answers, results, content and ads, via conversational interfaces or text.  We are a unique group of brilliant minds intent on discovering, learning and building. We work in a vibrant atmosphere, with an emphasis on personal and professional development. This is an opportunity to tackle complex problems usually reserved for a handful of large companies in the search industry.  About the Opportunity: We are a dynamic company looking for our next great hire to join our team in Carlsbad, California. We are looking for a Senior Data Engineer to join our team to manage and oversea data logging, pipelines, processing and presentation of data related to NTENT's platforms. The right candidate will take a lead role on the design of NTENT's next generation platform utilizing technologies outside of the Hadoop ecosystem.  Duties and Responsibilities: Design, implement, and support ETL data pipelines in a big data environment Maintain and improve existing data logging, pipeline and analytics platform Architect and design next generation data platform with appropriate new technologies Work with executives, product managers, machine learning scientists, and engineers to understand reporting and visualization needs Design, implement, and support a SQL based data warehouse Ensure reliability and accuracy of data logging and processing for all NTENT products Skills and Qualifications: Graduate degree in Computer Science or a similar field, or equivalent work experience 3+ years working in Data Engineering or Data Warehousing Experience with T-SQL, specifically Microsoft SQL Server Experience with Database design and schema definition Experience with Hadoop ecosystem (Hive, Pig, Oozie) Experience with Python Experience with Data Visualization and Reporting platforms The following experience is a plus: Tableau Kafka Spark The ideal candidate will be self-motivated, possess excellent communication skills (both oral and written) and be able to work independently. A keen interest in various aspects of language processing is essential in our multi-disciplinary team. We offer a full comprehensive benefits package including medical, dental and vision. Employees receive a generous time off (PTO) plan and 13 holidays per year. We also offer 401(k) benefits, long term disability benefits and life insurance. Senior Data Engineer Senior Data Engineer Position: Full time Position: Location: Carlsbad office (Initially Remote) Location: About Us: About Us: NTENT provides a Platform-as-a-Service (PaaS), allowing industry partners to customize, localize and integrate Voice Assistant and Search technologies directly into their business-to-consumer offerings. NTENT utilizes a machine learning algorithmic approach to comprehend massive amounts of information across the web. Through the company’s proprietary search engine, ontology and knowledge graph, NTENT makes it easier to decipher meaning, surfacing the most relevant answers, results, content and ads, via conversational interfaces or text.  We are a unique group of brilliant minds intent on discovering, learning and building. We work in a vibrant atmosphere, with an emphasis on personal and professional development. This is an opportunity to tackle complex problems usually reserved for a handful of large companies in the search industry. About the Opportunity: We are a dynamic company looking for our next great hire to join our team in Carlsbad, California. About the Opportunity: We are looking for a Senior Data Engineer to join our team to manage and oversea data logging, pipelines, processing and presentation of data related to NTENT's platforms. The right candidate will take a lead role on the design of NTENT's next generation platform utilizing technologies outside of the Hadoop ecosystem. Senior Data Engineer Duties and Responsibilities: Duties and Responsibilities: Design, implement, and support ETL data pipelines in a big data environment Maintain and improve existing data logging, pipeline and analytics platform Architect and design next generation data platform with appropriate new technologies Work with executives, product managers, machine learning scientists, and engineers to understand reporting and visualization needs Design, implement, and support a SQL based data warehouse Ensure reliability and accuracy of data logging and processing for all NTENT products Design, implement, and support ETL data pipelines in a big data environment Design, implement, and support ETL data pipelines in a big data environment Maintain and improve existing data logging, pipeline and analytics platform Maintain and improve existing data logging, pipeline and analytics platform Architect and design next generation data platform with appropriate new technologies Architect and design next generation data platform with appropriate new technologies Work with executives, product managers, machine learning scientists, and engineers to understand reporting and visualization needs Work with executives, product managers, machine learning scientists, and engineers to understand reporting and visualization needs Design, implement, and support a SQL based data warehouse Design, implement, and support a SQL based data warehouse Ensure reliability and accuracy of data logging and processing for all NTENT products Ensure reliability and accuracy of data logging and processing for all NTENT products Skills and Qualifications: Skills and Qualifications: Graduate degree in Computer Science or a similar field, or equivalent work experience 3+ years working in Data Engineering or Data Warehousing Experience with T-SQL, specifically Microsoft SQL Server Experience with Database design and schema definition Experience with Hadoop ecosystem (Hive, Pig, Oozie) Experience with Python Experience with Data Visualization and Reporting platforms Graduate degree in Computer Science or a similar field, or equivalent work experience Graduate degree in Computer Science or a similar field, or equivalent work experience 3+ years working in Data Engineering or Data Warehousing 3+ years working in Data Engineering or Data Warehousing Experience with T-SQL, specifically Microsoft SQL Server Experience with T-SQL, specifically Microsoft SQL Server Experience with Database design and schema definition Experience with Database design and schema definition Experience with Hadoop ecosystem (Hive, Pig, Oozie) Experience with Hadoop ecosystem (Hive, Pig, Oozie) Experience with Python Experience with Python Experience with Data Visualization and Reporting platforms Experience with Data Visualization and Reporting platforms Experience with Data Visualization and Reporting platforms The following experience is a plus: The following experience is a plus: Tableau Kafka Spark Tableau Kafka Kafka Spark Spark The ideal candidate will be self-motivated, possess excellent communication skills (both oral and written) and be able to work independently. A keen interest in various aspects of language processing is essential in our multi-disciplinary team. We offer a full comprehensive benefits package including medical, dental and vision. Employees receive a generous time off (PTO) plan and 13 holidays per year. We also offer 401(k) benefits, long term disability benefits and life insurance.",2.7,51 to 200 Employees,2010,Company - Private,Enterprise Software & Network Solutions,Information Technology,$5 to $25 million (USD)
"Aurora, CO",Data Engineer,$99K - $124K (Employer est.),HelloFresh,"Data Engineer( can be based in Newark NJ, Newnan GA, Aurora CO, Phoenix AZ, Irving, TX or Swedesboro, NJ) Come see what’s cookin’ at HelloFresh! At HelloFresh, we want to revolutionize the way we eat by making it more convenient and exciting to cook meals from scratch. We have offices all over the world and we deliver delicious meals to millions of people. We are the industry leader in meal-kit subscription services and we’re growing all the time. We have distinct meal-kit services that cater to everyone with the most menu variety in the market, which allows us to reach an incredibly wide population of people. The HelloFresh team is diverse, high-performing, and international, and our work environment is an inspiring space where you can thrive as a result.  Job Description As a Data Engineer, you will work with the Fulfillment Planning Technology team to help build the next generation suite of internal tools that enable Planning and Operations teams to see and act quickly to changing business conditions. The Data Engineer will build scalable data pipelines, infrastructure and tools that power our products and services.  You will … Work with analysts, engineers and planners to design, build, and maintain efficient, scalable and reliable data pipelines to support our business-critical needs in data ingestion, processing, and analysis Develop and maintain efficient, scalable and reliable code in Python and SQL Collaborate with other team members to troubleshoot, perform root cause analysis and optimize existing data pipelines and tools Work with other team members to ensure effective tool integration with other systems and workflows  At a minimum, you have … Bachelor’s or Master’s degree in Computer Science, Engineering or related field 2+ years of data engineering experience in Fulfillment, Logistics, Supply Chain, Production, or related field working with physical goods Strong proficiency in Python and SQL Experience working with distributed systems, building and maintaining pipelines using cloud technologies (AWS, Snowflake) Experience in containerization and orchestration (Docker, Kubernetes, Airflow, GCP) Startup experience a plus You'll get... Competitive Salary & 401k company match that vests immediately upon participation Generous parental leave of 4 weeks & PTO policy, as well as paid holidays off $0 monthly premium and other flexible health plans Amazing discounts, including up to 75% off HelloFresh subscription Flexible shift scheduling & advancement opportunities Emergency child and adult care services Snacks & monthly catered lunches Collaborative, dynamic work environment within a fast-paced, mission-driven company It is the policy of HelloFresh not to discriminate against any employee or applicant for employment because of race, color, religion, sex, sexual orientation, gender identity, national origin, age, marital status, genetic information, disability or because he or she is a protected veteran. Colorado Pay Range $99,200—$124,000 USD Data Engineer( can be based in Newark NJ, Newnan GA, Aurora CO, Phoenix AZ, Irving, TX or Swedesboro, NJ) Data Engineer( can be based in Newark NJ, Newnan GA, Aurora CO, Phoenix AZ, Irving, TX or Swedesboro, NJ) Data Engineer( can be based in Newark NJ, Newnan GA, Aurora CO, Phoenix AZ, Irving, TX or Swedesboro, NJ) Come see what’s cookin’ at HelloFresh! Come see what’s cookin’ at HelloFresh! see what’s cookin’ at HelloFresh! At HelloFresh, we want to revolutionize the way we eat by making it more convenient and exciting to cook meals from scratch. We have offices all over the world and we deliver delicious meals to millions of people. We are the industry leader in meal-kit subscription services and we’re growing all the time. We have distinct meal-kit services that cater to everyone with the most menu variety in the market, which allows us to reach an incredibly wide population of people. The HelloFresh team is diverse, high-performing, and international, and our work environment is an inspiring space where you can thrive as a result. Job Description Job Description As a Data Engineer, you will work with the Fulfillment Planning Technology team to help build the next generation suite of internal tools that enable Planning and Operations teams to see and act quickly to changing business conditions. The Data Engineer will build scalable data pipelines, infrastructure and tools that power our products and services. You will … You will … Work with analysts, engineers and planners to design, build, and maintain efficient, scalable and reliable data pipelines to support our business-critical needs in data ingestion, processing, and analysis Develop and maintain efficient, scalable and reliable code in Python and SQL Collaborate with other team members to troubleshoot, perform root cause analysis and optimize existing data pipelines and tools Work with other team members to ensure effective tool integration with other systems and workflows Work with analysts, engineers and planners to design, build, and maintain efficient, scalable and reliable data pipelines to support our business-critical needs in data ingestion, processing, and analysis Develop and maintain efficient, scalable and reliable code in Python and SQL Collaborate with other team members to troubleshoot, perform root cause analysis and optimize existing data pipelines and tools Work with other team members to ensure effective tool integration with other systems and workflows At a minimum, you have … At a minimum, you have … Bachelor’s or Master’s degree in Computer Science, Engineering or related field 2+ years of data engineering experience in Fulfillment, Logistics, Supply Chain, Production, or related field working with physical goods Strong proficiency in Python and SQL Experience working with distributed systems, building and maintaining pipelines using cloud technologies (AWS, Snowflake) Experience in containerization and orchestration (Docker, Kubernetes, Airflow, GCP) Startup experience a plus Bachelor’s or Master’s degree in Computer Science, Engineering or related field 2+ years of data engineering experience in Fulfillment, Logistics, Supply Chain, Production, or related field working with physical goods Strong proficiency in Python and SQL Experience working with distributed systems, building and maintaining pipelines using cloud technologies (AWS, Snowflake) Experience in containerization and orchestration (Docker, Kubernetes, Airflow, GCP) Startup experience a plus You'll get... You'll get... Competitive Salary & 401k company match that vests immediately upon participation Generous parental leave of 4 weeks & PTO policy, as well as paid holidays off $0 monthly premium and other flexible health plans Amazing discounts, including up to 75% off HelloFresh subscription Flexible shift scheduling & advancement opportunities Emergency child and adult care services Snacks & monthly catered lunches Collaborative, dynamic work environment within a fast-paced, mission-driven company Competitive Salary & 401k company match that vests immediately upon participation Generous parental leave of 4 weeks & PTO policy, as well as paid holidays off $0 monthly premium and other flexible health plans Amazing discounts, including up to 75% off HelloFresh subscription Flexible shift scheduling & advancement opportunities Emergency child and adult care services Snacks & monthly catered lunches Collaborative, dynamic work environment within a fast-paced, mission-driven company It is the policy of HelloFresh not to discriminate against any employee or applicant for employment because of race, color, religion, sex, sexual orientation, gender identity, national origin, age, marital status, genetic information, disability or because he or she is a protected veteran. It is the policy of HelloFresh not to discriminate against any employee or applicant for employment because of race, color, religion, sex, sexual orientation, gender identity, national origin, age, marital status, genetic information, disability or because he or she is a protected veteran. Colorado Pay Range $99,200—$124,000 USD Colorado Pay Range $99,200—$124,000 USD Colorado Pay Range $99,200—$124,000 USD",3.7,5001 to 10000 Employees,2011,Company - Public,Catering & Food Service Contractors,Restaurants & Food Service,Unknown / Non-Applicable
Remote,Data Engineer,$80K - $100K (Employer est.),Tremco Incorporated,"BUILD YOUR FUTURE WHILE YOU HELP BUILD A BETTER WORLD! Tremco CPG is an aggressive, growth-oriented company with revenues of over $1 billion. We are a world leader in solving complex waterproofing and roofing problems for our commercial, institutional, and industrial customers. If you’re looking for a place to build a career, with great benefits, advancement opportunity, technology, people and a commitment to a sustainable future, you’ve found it with us. Tremco CPG is currently searching for a Data Engineer in our Roofing and Building Maintenance Division. *100% REMOTE / TELEWORK* We are looking for a candidate that thrives in a collaborative environment, is a self-starter, and is passionate about data science. Our data science team is the foundation for data-driven business decisions and is leading the way for continued growth in innovative markets within the construction industry. On the Data Science team, the Data Engineer’s purpose is to design, develop, and maintain the company's data infrastructure, pipelines, and workflows. They are responsible for merging predictive and prescriptive modeling to ensure it stays consistent with data flowing across the organization. They work closely with data scientists, analysts, and other stakeholders to ensure the data is properly collected, stored, processed, and analyzed to drive informed business decisions. If you are passionate about data science and want to work with a dynamic team of professionals, please apply today! ESSENTIAL DUTIES AND RESPONSIBILITIES: Design, develop, build, and maintain the company's data infrastructure, pipelines, and workflows and all associated engineering tasks. Develop and maintain ETL (Extract, Transform, Load) processes to collect and integrate data from various sources. Build and maintain data APIs to enable data access across the organization. Develop and implement scalable data solutions to optimize data processing, storage, and retrieval. Develop and maintain documentation for data pipelines, including data dictionaries, standard operating procedures, and data flow diagrams. Work with unstructured data and develop data models to enable data analysis and insights. Identify any hidden patterns or data inconsistencies and work along with similar ad-hoc analysis Ensure data quality, consistency, and accuracy and is properly structured and formatted to support analyses. Ensure data security, integrity, and compliance with data privacy regulations. Troubleshoot and resolve data-related issues, including data quality, integrity, and performance. Continuously monitor, maintain, and optimize the health and performance of the data infrastructure, pipelines, and workflows Collaborate with data scientists, analysts, and other stakeholders to understand data requirements. Stay up to date with the latest advancements in data engineering and recommend new technologies, tools, and processes to improve efficiency and productivity. Design, develop, build, and maintain the company's data infrastructure, pipelines, and workflows and all associated engineering tasks. Develop and maintain ETL (Extract, Transform, Load) processes to collect and integrate data from various sources. Build and maintain data APIs to enable data access across the organization. Develop and implement scalable data solutions to optimize data processing, storage, and retrieval. Develop and maintain documentation for data pipelines, including data dictionaries, standard operating procedures, and data flow diagrams. Work with unstructured data and develop data models to enable data analysis and insights. Identify any hidden patterns or data inconsistencies and work along with similar ad-hoc analysis Ensure data quality, consistency, and accuracy and is properly structured and formatted to support analyses. Ensure data security, integrity, and compliance with data privacy regulations. Troubleshoot and resolve data-related issues, including data quality, integrity, and performance. Continuously monitor, maintain, and optimize the health and performance of the data infrastructure, pipelines, and workflows Collaborate with data scientists, analysts, and other stakeholders to understand data requirements. Stay up to date with the latest advancements in data engineering and recommend new technologies, tools, and processes to improve efficiency and productivity. EDUCATION: Bachelor's or Master's degree in Information Technology, Computer Science, or a related field EXPERIENCE: 3+ years of experience in a data science or related role CERTIFICATES, LICENSES, REGISTRATIONS: Not Required but beneficial: Certified SQL Certified SQL, Advanced Queries Python for Data Science & Machine Learning R for Data Science & Machine Learning Databricks Lakehouse Fundamentals Certified SQL Certified SQL, Advanced Queries Python for Data Science & Machine Learning R for Data Science & Machine Learning Databricks Lakehouse Fundamentals OTHER SKILLS AND ABILITIES: Proficiency in programming languages such as Python, R, and SQL Strong understanding of database technologies and SQL queries Strong experience with ETL processes, data integration, and data modeling Experience with cloud-based data storage and computing services, specifically Azure Excellent problem-solving and analytical skills Experience with data visualization tools such as Tableau or Power BI Experience with data lakehouse tools such as Synapse (data lake) or databricks Excellent communication and collaboration skills Ability to work independently and prioritize tasks in a fast-paced & dynamic environment Proficiency in programming languages such as Python, R, and SQL Strong understanding of database technologies and SQL queries Strong experience with ETL processes, data integration, and data modeling Experience with cloud-based data storage and computing services, specifically Azure Excellent problem-solving and analytical skills Experience with data visualization tools such as Tableau or Power BI Experience with data lakehouse tools such as Synapse (data lake) or databricks Excellent communication and collaboration skills Ability to work independently and prioritize tasks in a fast-paced & dynamic environment Qualified applicants will receive consideration for employment without regard to their race, color, religion, national origin, sex, sexual orientation, gender identity, protected veteran status or disability. The salary range for applicants in this position generally ranges between $79,000 and $99,000. This range is an estimate, based on potential employee qualifications, operations, needs, and other considerations permitted by law. Benefits: Tremco offers a variety of benefits to its employees, including not limited to: health insurance, paid holidays, paid time off, 401(k) with company match, Company Pension Plan, Performance Based Commission, and continuing education. Location: 3735 Green Road, United States Job Type: Full-time Pay: $80,000.00 - $100,000.00 per year Benefits: 401(k) 401(k) matching Dental insurance Employee assistance program Flexible spending account Health insurance Health savings account Life insurance Paid time off Parental leave Professional development assistance Referral program Relocation assistance Retirement plan Tuition reimbursement Vision insurance 401(k) 401(k) matching Dental insurance Employee assistance program Flexible spending account Health insurance Health savings account Life insurance Paid time off Parental leave Professional development assistance Referral program Relocation assistance Retirement plan Tuition reimbursement Vision insurance Schedule: 8 hour shift Monday to Friday 8 hour shift Monday to Friday Supplemental pay types: Bonus pay Bonus pay Work Location: Remote",3.9,1001 to 5000 Employees,1997,Subsidiary or Business Segment,Chemical Manufacturing,Manufacturing,$500 million to $1 billion (USD)
"Chicago, IL",3020 - DATA ENGINEER,$86K - $125K (Glassdoor est.),Sinai Chicago,"About Sinai Chicago Located on Chicago’s West and Southwest Side, Sinai Chicago comprises Mount Sinai Hospital, Holy Cross Hospital, Schwab Rehabilitation Hospital, Sinai Children’s Hospital, Sinai Community Institute, Sinai Medical Group, and Sinai Urban Health Institute.  The entities of Sinai Chicago collectively deliver a full range of quality inpatient and outpatient services, as well as a large number of innovative, community-based health, research and social service programs. We focus our collective depth of expertise and passion to improve the health of the 1.5 million people who live in our diverse service area. With our team of dedicated caregivers, Sinai Chicago is committed to building stronger, healthier communities. A partner with the Jewish United Fund in serving our community General Summary/basic PURPOSE OF JOB: The primary responsibility of this role is to work with other across the organization to produce meaningful information from multiple data sources, including but not limited to EDW data warehouse, EPIC EMR, Meditech, NextGen and other databases. Candidates should be able to work independently, with other technology groups as well as business stakeholders to define and gather data requirements to develop consumable data outputs. Develop and produce datasets routinely for consumption, data extracts for integration with other systems and ad hoc business needs. An understanding of Star Schema, Snowflake and ETL processes to populate data warehouses. Validate code for data accuracy and collaborate with DBAs for database performance and planning. Essential Functions and duties Ability to work independently and as a team to define and build solutions Knowledge or experience working in a Healthcare environment Advanced knowledge with T-SQL, query, stored procedures and Microsoft SSRS, SSIS Work with DBA on database performance and database planning Experience with Microsoft Visual Studio and package creation Collaborate with other data engineers, business analysts and data architects Knowledge of one or more scripting languages such as Python Exposure to EPIC, SlicerDicer, Caboodle and Clarity ideal but not required JOB REQUIREMENTS: MINIMUM Education: Bachelor's Degree in computer science or similar degree MINIMUM WORK EXPERIENCE: 5 or more years’ experience in the same or similar type of role Healthcare industry preferred T-SQL, query, stored procedures and Microsoft SSRS, SSIS Microsoft SQL DBA skills preferred but not required KNOWLEDGE & SKILLS: Expertise query development, T-SQL, ETL, stored procedures and triggers Expertise with Microsoft SQL stack including SSRS, SSIS, SSAS Programming language experience in C#, Java, Python or similar languages Exposure to Microsoft PowerBI (on-site and cloud-based) Excellent communication, organization and documentation skills The ability to transform business requirements into technology solutions Knowledge or exposure to Epic, SlicerDicer, Caboodle, Meditech, or NextGen   Location: SHS · SHS SHS IS BUSINESS SYSTEMS Schedule: Full Time, Days, 8am-5pm About Sinai Chicago About Sinai Chicago Located on Chicago’s West and Southwest Side, Sinai Chicago comprises Mount Sinai Hospital, Holy Cross Hospital, Schwab Rehabilitation Hospital, Sinai Children’s Hospital, Sinai Community Institute, Sinai Medical Group, and Sinai Urban Health Institute.  The entities of Sinai Chicago collectively deliver a full range of quality inpatient and outpatient services, as well as a large number of innovative, community-based health, research and social service programs. We focus our collective depth of expertise and passion to improve the health of the 1.5 million people who live in our diverse service area. With our team of dedicated caregivers, Sinai Chicago is committed to building stronger, healthier communities. A partner with the Jewish United Fund in serving our community A partner with the Jewish United Fund in serving our community General Summary/basic PURPOSE OF JOB: The primary responsibility of this role is to work with other across the organization to produce meaningful information from multiple data sources, including but not limited to EDW data warehouse, EPIC EMR, Meditech, NextGen and other databases. Candidates should be able to work independently, with other technology groups as well as business stakeholders to define and gather data requirements to develop consumable data outputs. Develop and produce datasets routinely for consumption, data extracts for integration with other systems and ad hoc business needs. An understanding of Star Schema, Snowflake and ETL processes to populate data warehouses. Validate code for data accuracy and collaborate with DBAs for database performance and planning. Essential Functions and duties Ability to work independently and as a team to define and build solutions Knowledge or experience working in a Healthcare environment Advanced knowledge with T-SQL, query, stored procedures and Microsoft SSRS, SSIS Work with DBA on database performance and database planning Ability to work independently and as a team to define and build solutions Ability to work independently and as a team to define and build solutions Knowledge or experience working in a Healthcare environment Knowledge or experience working in a Healthcare environment Advanced knowledge with T-SQL, query, stored procedures and Microsoft SSRS, SSIS Advanced knowledge with T-SQL, query, stored procedures and Microsoft SSRS, SSIS Work with DBA on database performance and database planning Work with DBA on database performance and database planning Experience with Microsoft Visual Studio and package creation Collaborate with other data engineers, business analysts and data architects Knowledge of one or more scripting languages such as Python Exposure to EPIC, SlicerDicer, Caboodle and Clarity ideal but not required Experience with Microsoft Visual Studio and package creation Experience with Microsoft Visual Studio and package creation Collaborate with other data engineers, business analysts and data architects Collaborate with other data engineers, business analysts and data architects Knowledge of one or more scripting languages such as Python Knowledge of one or more scripting languages such as Python Exposure to EPIC, SlicerDicer, Caboodle and Clarity ideal but not required Exposure to EPIC, SlicerDicer, Caboodle and Clarity ideal but not required JOB REQUIREMENTS: MINIMUM Education: Bachelor's Degree in computer science or similar degree Bachelor's Degree in computer science or similar degree Bachelor's Degree in computer science or similar degree MINIMUM WORK EXPERIENCE: 5 or more years’ experience in the same or similar type of role Healthcare industry preferred 5 or more years’ experience in the same or similar type of role 5 or more years’ experience in the same or similar type of role Healthcare industry preferred Healthcare industry preferred T-SQL, query, stored procedures and Microsoft SSRS, SSIS Microsoft SQL DBA skills preferred but not required T-SQL, query, stored procedures and Microsoft SSRS, SSIS T-SQL, query, stored procedures and Microsoft SSRS, SSIS Microsoft SQL DBA skills preferred but not required Microsoft SQL DBA skills preferred but not required KNOWLEDGE & SKILLS: Expertise query development, T-SQL, ETL, stored procedures and triggers Expertise with Microsoft SQL stack including SSRS, SSIS, SSAS Expertise query development, T-SQL, ETL, stored procedures and triggers Expertise query development, T-SQL, ETL, stored procedures and triggers Expertise with Microsoft SQL stack including SSRS, SSIS, SSAS Expertise with Microsoft SQL stack including SSRS, SSIS, SSAS Programming language experience in C#, Java, Python or similar languages Exposure to Microsoft PowerBI (on-site and cloud-based) Excellent communication, organization and documentation skills The ability to transform business requirements into technology solutions Knowledge or exposure to Epic, SlicerDicer, Caboodle, Meditech, or NextGen Programming language experience in C#, Java, Python or similar languages Programming language experience in C#, Java, Python or similar languages Exposure to Microsoft PowerBI (on-site and cloud-based) Exposure to Microsoft PowerBI (on-site and cloud-based) Excellent communication, organization and documentation skills Excellent communication, organization and documentation skills The ability to transform business requirements into technology solutions The ability to transform business requirements into technology solutions Knowledge or exposure to Epic, SlicerDicer, Caboodle, Meditech, or NextGen Knowledge or exposure to Epic, SlicerDicer, Caboodle, Meditech, or NextGen",3.9,1001 to 5000 Employees,1918,Nonprofit Organization,Health Care Services & Hospitals,Healthcare,$100 to $500 million (USD)
"Weston, MA",Data Engineer,$98K - $144K (Glassdoor est.),Advisor360,"At Advisor360°, we hire people with all kinds of awesome experiences, backgrounds, and perspectives. We like it that way. So even if you don't meet every single requirement, please consider applying if you like what you see. Though many of our roles are assigned to a particular office location, most are a hybrid of remote and in-person work.  We hire people with all kinds of awesome experiences, backgrounds, and perspectives. We like it that way. So even if you don't meet every single requirement, please consider applying if you like what you see. The Data Engineer role will be part of Advisor360°'s Engineering organization, as a member of our team. This position is responsible for creating and maintaining widespread data capabilities and processes across the Advisor360° platform. The ideal candidate is passionate about leveraging modern technology and building scalable and efficient data pipelines, processes, and capabilities. Key Responsibilities: Actively participates in all SDLC processes including code reviews, deployments, and health checks Adheres to internal SDLC processes including code reviews and change management processes, making suggestions for improvement Collaborate with engineers and architects on effective development practices, communicating with tact and professionalism Create, maintain, and support software and data processes, and capabilities which align to existing standards and requirements Creates end-to-end data solutions that are highly available, performant, scalable, secure, and cost-effective Demonstrate a sound understanding of the business and knowledge of data infrastructure Demonstrates advanced tuning, performance and troubleshooting skills across multiple platforms Demonstrates the ability to adapt theory, design patterns, and best practices to fit the needs of the projects Develop software solutions by analyzing information needs, conferring with users, and examining data usage and workstream processes Develop, build, and implement dependable automated build and deploy processes in our CI/CD process Effectively troubleshoots errors and performance issues and quickly remediates them Plan and implement mid-large-scale projects conception to completion as part of a team that you may lead Produces well-formed artifacts and technical documentation that align to standards and requirements Provide ongoing maintenance, support, and enhancements in existing systems and platforms Provides mentorship and technical leadership to less seasoned engineers Troubleshoots and quickly resolves issues in code, processes, or data sources Preferred Strengths: Knowledge of and experience with Microsoft® Azure platform Knowledge of and experience with Microsoft® SQL Server Working knowledge of Python, .NET, or Powershell Ability to work in a fast-paced Agile/Scrum environment An appetite and aptitude for taking responsibility for technical decisions Good verbal and written communication skills Self-starter who is eager to learn Requirements, skills and knowledge: Ability to work in a fast-paced Agile/Scrum environment Write efficient and maintainable code including SQL stored procedures and APIs Effectively communicate and collaborate with both technical and non-technical teams Experience with SDLC and CI/CD processes Strong coding skills with SQL and at least one programming language such as Python or .NET Demonstrated excellence in troubleshooting, debugging and data performance tuning Demonstrated strength in data modelling, data integration, and analytics Demonstrated strength in documenting, refactoring and optimizing algorithms in code and stored procedures  About Advisor360° Come grow with us! Join a team of inventive, driven professionals! Launched in 2019, Advisor360° combines a start-up's agile and innovative markers with the stability of an established enterprise. Ranked the largest FinTech company in Massachusetts by the Boston Business Journal, Advisor360° was born out of 20 years of feedback from the industry's highest-producing financial advisors and has gone on to build a software that is second to none. Our company specializes in building, delivering, and integrating technology for wealth managers. Our comprehensive platform offers a unified approach to managing wealth, delivering a connected experience to advisors and their clients. Benefits include a competitive bonus plan, unlimited paid time off, paid sick time, volunteer time off, free access to vacation homes, a generous 401(k) employer match, stock appreciation rights, referral bonuses, 80% of either HMO or PPO premiums for individuals and family employer covered, 50% of dental insurance paid for, employer life and AD&D at 2X salary, short- and long-term disability coverage, a health savings account, flexible spending accounts, and paid bonding time for new parents. Advisor360° also provides worthwhile perks, including a hybrid structure with most employees working from the office 2 to 3 times per week, tuition reimbursement, professional development opportunities, employee assistance programs, and more! Join us on this journey. Advisor360° is an equal opportunity employer committed to a diverse workforce. We believe diversity drives innovation and are therefore building a company where people of all backgrounds are truly welcome and included. Everyone is encouraged to bring their unique, authentic selves to work each and every day. The way we see it, we are here to learn from each other. Are you ready to build and discover together? At Advisor360°, we hire people with all kinds of awesome experiences, backgrounds, and perspectives. We like it that way. So even if you don't meet every single requirement, please consider applying if you like what you see. Though many of our roles are assigned to a particular office location, most are a hybrid of remote and in-person work. At Advisor360°, we hire people with all kinds of awesome experiences, backgrounds, and perspectives. We like it that way. So even if you don't meet every single requirement, please consider applying if you like what you see. Though many of our roles are assigned to a particular office location, most are a hybrid of remote and in-person work. We hire people with all kinds of awesome experiences, backgrounds, and perspectives. We like it that way. So even if you don't meet every single requirement, please consider applying if you like what you see. The Data Engineer role will be part of Advisor360°'s Engineering organization, as a member of our team. This position is responsible for creating and maintaining widespread data capabilities and processes across the Advisor360° platform. The ideal candidate is passionate about leveraging modern technology and building scalable and efficient data pipelines, processes, and capabilities. Key Responsibilities: Actively participates in all SDLC processes including code reviews, deployments, and health checks Adheres to internal SDLC processes including code reviews and change management processes, making suggestions for improvement Collaborate with engineers and architects on effective development practices, communicating with tact and professionalism Create, maintain, and support software and data processes, and capabilities which align to existing standards and requirements Creates end-to-end data solutions that are highly available, performant, scalable, secure, and cost-effective Demonstrate a sound understanding of the business and knowledge of data infrastructure Demonstrates advanced tuning, performance and troubleshooting skills across multiple platforms Demonstrates the ability to adapt theory, design patterns, and best practices to fit the needs of the projects Develop software solutions by analyzing information needs, conferring with users, and examining data usage and workstream processes Develop, build, and implement dependable automated build and deploy processes in our CI/CD process Effectively troubleshoots errors and performance issues and quickly remediates them Plan and implement mid-large-scale projects conception to completion as part of a team that you may lead Produces well-formed artifacts and technical documentation that align to standards and requirements Provide ongoing maintenance, support, and enhancements in existing systems and platforms Provides mentorship and technical leadership to less seasoned engineers Troubleshoots and quickly resolves issues in code, processes, or data sources Preferred Strengths: Knowledge of and experience with Microsoft® Azure platform Knowledge of and experience with Microsoft® SQL Server Working knowledge of Python, .NET, or Powershell Ability to work in a fast-paced Agile/Scrum environment An appetite and aptitude for taking responsibility for technical decisions Good verbal and written communication skills Self-starter who is eager to learn Requirements, skills and knowledge: Ability to work in a fast-paced Agile/Scrum environment Write efficient and maintainable code including SQL stored procedures and APIs Effectively communicate and collaborate with both technical and non-technical teams Experience with SDLC and CI/CD processes Strong coding skills with SQL and at least one programming language such as Python or .NET Demonstrated excellence in troubleshooting, debugging and data performance tuning Demonstrated strength in data modelling, data integration, and analytics Demonstrated strength in documenting, refactoring and optimizing algorithms in code and stored procedures We hire people with all kinds of awesome experiences, backgrounds, and perspectives. We like it that way. So even if you don't meet every single requirement, please consider applying if you like what you see. The Data Engineer role will be part of Advisor360°'s Engineering organization, as a member of our team. This position is responsible for creating and maintaining widespread data capabilities and processes across the Advisor360° platform. The ideal candidate is passionate about leveraging modern technology and building scalable and efficient data pipelines, processes, and capabilities. Key Responsibilities: Actively participates in all SDLC processes including code reviews, deployments, and health checks Adheres to internal SDLC processes including code reviews and change management processes, making suggestions for improvement Collaborate with engineers and architects on effective development practices, communicating with tact and professionalism Create, maintain, and support software and data processes, and capabilities which align to existing standards and requirements Creates end-to-end data solutions that are highly available, performant, scalable, secure, and cost-effective Demonstrate a sound understanding of the business and knowledge of data infrastructure Demonstrates advanced tuning, performance and troubleshooting skills across multiple platforms Demonstrates the ability to adapt theory, design patterns, and best practices to fit the needs of the projects Develop software solutions by analyzing information needs, conferring with users, and examining data usage and workstream processes Develop, build, and implement dependable automated build and deploy processes in our CI/CD process Effectively troubleshoots errors and performance issues and quickly remediates them Plan and implement mid-large-scale projects conception to completion as part of a team that you may lead Produces well-formed artifacts and technical documentation that align to standards and requirements Provide ongoing maintenance, support, and enhancements in existing systems and platforms Provides mentorship and technical leadership to less seasoned engineers Troubleshoots and quickly resolves issues in code, processes, or data sources Preferred Strengths: Knowledge of and experience with Microsoft® Azure platform Knowledge of and experience with Microsoft® SQL Server Working knowledge of Python, .NET, or Powershell Ability to work in a fast-paced Agile/Scrum environment An appetite and aptitude for taking responsibility for technical decisions Good verbal and written communication skills Self-starter who is eager to learn Requirements, skills and knowledge: Ability to work in a fast-paced Agile/Scrum environment Write efficient and maintainable code including SQL stored procedures and APIs Effectively communicate and collaborate with both technical and non-technical teams Experience with SDLC and CI/CD processes Strong coding skills with SQL and at least one programming language such as Python or .NET Demonstrated excellence in troubleshooting, debugging and data performance tuning Demonstrated strength in data modelling, data integration, and analytics Demonstrated strength in documenting, refactoring and optimizing algorithms in code and stored procedures We hire people with all kinds of awesome experiences, backgrounds, and perspectives. We like it that way. So even if you don't meet every single requirement, please consider applying if you like what you see. The Data Engineer role will be part of Advisor360°'s Engineering organization, as a member of our team. This position is responsible for creating and maintaining widespread data capabilities and processes across the Advisor360° platform. The ideal candidate is passionate about leveraging modern technology and building scalable and efficient data pipelines, processes, and capabilities. Key Responsibilities: Actively participates in all SDLC processes including code reviews, deployments, and health checks Adheres to internal SDLC processes including code reviews and change management processes, making suggestions for improvement Collaborate with engineers and architects on effective development practices, communicating with tact and professionalism Create, maintain, and support software and data processes, and capabilities which align to existing standards and requirements Creates end-to-end data solutions that are highly available, performant, scalable, secure, and cost-effective Demonstrate a sound understanding of the business and knowledge of data infrastructure Demonstrates advanced tuning, performance and troubleshooting skills across multiple platforms Demonstrates the ability to adapt theory, design patterns, and best practices to fit the needs of the projects Develop software solutions by analyzing information needs, conferring with users, and examining data usage and workstream processes Develop, build, and implement dependable automated build and deploy processes in our CI/CD process Effectively troubleshoots errors and performance issues and quickly remediates them Plan and implement mid-large-scale projects conception to completion as part of a team that you may lead Produces well-formed artifacts and technical documentation that align to standards and requirements Provide ongoing maintenance, support, and enhancements in existing systems and platforms Provides mentorship and technical leadership to less seasoned engineers Troubleshoots and quickly resolves issues in code, processes, or data sources Preferred Strengths: Knowledge of and experience with Microsoft® Azure platform Knowledge of and experience with Microsoft® SQL Server Working knowledge of Python, .NET, or Powershell Ability to work in a fast-paced Agile/Scrum environment An appetite and aptitude for taking responsibility for technical decisions Good verbal and written communication skills Self-starter who is eager to learn Requirements, skills and knowledge: Ability to work in a fast-paced Agile/Scrum environment Write efficient and maintainable code including SQL stored procedures and APIs Effectively communicate and collaborate with both technical and non-technical teams Experience with SDLC and CI/CD processes Strong coding skills with SQL and at least one programming language such as Python or .NET Demonstrated excellence in troubleshooting, debugging and data performance tuning Demonstrated strength in data modelling, data integration, and analytics Demonstrated strength in documenting, refactoring and optimizing algorithms in code and stored procedures We hire people with all kinds of awesome experiences, backgrounds, and perspectives. We like it that way. So even if you don't meet every single requirement, please consider applying if you like what you see. The Data Engineer role will be part of Advisor360°'s Engineering organization, as a member of our team. This position is responsible for creating and maintaining widespread data capabilities and processes across the Advisor360° platform. The ideal candidate is passionate about leveraging modern technology and building scalable and efficient data pipelines, processes, and capabilities. Key Responsibilities: Actively participates in all SDLC processes including code reviews, deployments, and health checks Adheres to internal SDLC processes including code reviews and change management processes, making suggestions for improvement Collaborate with engineers and architects on effective development practices, communicating with tact and professionalism Create, maintain, and support software and data processes, and capabilities which align to existing standards and requirements Creates end-to-end data solutions that are highly available, performant, scalable, secure, and cost-effective Demonstrate a sound understanding of the business and knowledge of data infrastructure Demonstrates advanced tuning, performance and troubleshooting skills across multiple platforms Demonstrates the ability to adapt theory, design patterns, and best practices to fit the needs of the projects Develop software solutions by analyzing information needs, conferring with users, and examining data usage and workstream processes Develop, build, and implement dependable automated build and deploy processes in our CI/CD process Effectively troubleshoots errors and performance issues and quickly remediates them Plan and implement mid-large-scale projects conception to completion as part of a team that you may lead Produces well-formed artifacts and technical documentation that align to standards and requirements Provide ongoing maintenance, support, and enhancements in existing systems and platforms Provides mentorship and technical leadership to less seasoned engineers Troubleshoots and quickly resolves issues in code, processes, or data sources Preferred Strengths: Knowledge of and experience with Microsoft® Azure platform Knowledge of and experience with Microsoft® SQL Server Working knowledge of Python, .NET, or Powershell Ability to work in a fast-paced Agile/Scrum environment An appetite and aptitude for taking responsibility for technical decisions Good verbal and written communication skills Self-starter who is eager to learn Requirements, skills and knowledge: Ability to work in a fast-paced Agile/Scrum environment Write efficient and maintainable code including SQL stored procedures and APIs Effectively communicate and collaborate with both technical and non-technical teams Experience with SDLC and CI/CD processes Strong coding skills with SQL and at least one programming language such as Python or .NET Demonstrated excellence in troubleshooting, debugging and data performance tuning Demonstrated strength in data modelling, data integration, and analytics Demonstrated strength in documenting, refactoring and optimizing algorithms in code and stored procedures We hire people with all kinds of awesome experiences, backgrounds, and perspectives. We like it that way. So even if you don't meet every single requirement, please consider applying if you like what you see. We hire people with all kinds of awesome experiences, backgrounds, and perspectives. We like it that way. So even if you don't meet every single requirement, please consider applying if you like what you see. The Data Engineer role will be part of Advisor360°'s Engineering organization, as a member of our team. This position is responsible for creating and maintaining widespread data capabilities and processes across the Advisor360° platform. The ideal candidate is passionate about leveraging modern technology and building scalable and efficient data pipelines, processes, and capabilities. Key Responsibilities: Key Responsibilities: Actively participates in all SDLC processes including code reviews, deployments, and health checks Adheres to internal SDLC processes including code reviews and change management processes, making suggestions for improvement Collaborate with engineers and architects on effective development practices, communicating with tact and professionalism Create, maintain, and support software and data processes, and capabilities which align to existing standards and requirements Creates end-to-end data solutions that are highly available, performant, scalable, secure, and cost-effective Demonstrate a sound understanding of the business and knowledge of data infrastructure Demonstrates advanced tuning, performance and troubleshooting skills across multiple platforms Demonstrates the ability to adapt theory, design patterns, and best practices to fit the needs of the projects Develop software solutions by analyzing information needs, conferring with users, and examining data usage and workstream processes Develop, build, and implement dependable automated build and deploy processes in our CI/CD process Effectively troubleshoots errors and performance issues and quickly remediates them Plan and implement mid-large-scale projects conception to completion as part of a team that you may lead Produces well-formed artifacts and technical documentation that align to standards and requirements Provide ongoing maintenance, support, and enhancements in existing systems and platforms Provides mentorship and technical leadership to less seasoned engineers Troubleshoots and quickly resolves issues in code, processes, or data sources Actively participates in all SDLC processes including code reviews, deployments, and health checks Adheres to internal SDLC processes including code reviews and change management processes, making suggestions for improvement Collaborate with engineers and architects on effective development practices, communicating with tact and professionalism Create, maintain, and support software and data processes, and capabilities which align to existing standards and requirements Creates end-to-end data solutions that are highly available, performant, scalable, secure, and cost-effective Demonstrate a sound understanding of the business and knowledge of data infrastructure Demonstrates advanced tuning, performance and troubleshooting skills across multiple platforms Demonstrates the ability to adapt theory, design patterns, and best practices to fit the needs of the projects Develop software solutions by analyzing information needs, conferring with users, and examining data usage and workstream processes Develop, build, and implement dependable automated build and deploy processes in our CI/CD process Effectively troubleshoots errors and performance issues and quickly remediates them Plan and implement mid-large-scale projects conception to completion as part of a team that you may lead Produces well-formed artifacts and technical documentation that align to standards and requirements Provide ongoing maintenance, support, and enhancements in existing systems and platforms Provides mentorship and technical leadership to less seasoned engineers Troubleshoots and quickly resolves issues in code, processes, or data sources Preferred Strengths: Preferred Strengths: Knowledge of and experience with Microsoft® Azure platform Knowledge of and experience with Microsoft® SQL Server Working knowledge of Python, .NET, or Powershell Ability to work in a fast-paced Agile/Scrum environment An appetite and aptitude for taking responsibility for technical decisions Good verbal and written communication skills Self-starter who is eager to learn Knowledge of and experience with Microsoft® Azure platform Knowledge of and experience with Microsoft® SQL Server Working knowledge of Python, .NET, or Powershell Ability to work in a fast-paced Agile/Scrum environment An appetite and aptitude for taking responsibility for technical decisions Good verbal and written communication skills Self-starter who is eager to learn Requirements, skills and knowledge: Requirements, skills and knowledge: Ability to work in a fast-paced Agile/Scrum environment Write efficient and maintainable code including SQL stored procedures and APIs Effectively communicate and collaborate with both technical and non-technical teams Experience with SDLC and CI/CD processes Strong coding skills with SQL and at least one programming language such as Python or .NET Demonstrated excellence in troubleshooting, debugging and data performance tuning Demonstrated strength in data modelling, data integration, and analytics Demonstrated strength in documenting, refactoring and optimizing algorithms in code and stored procedures Ability to work in a fast-paced Agile/Scrum environment Write efficient and maintainable code including SQL stored procedures and APIs Effectively communicate and collaborate with both technical and non-technical teams Experience with SDLC and CI/CD processes Strong coding skills with SQL and at least one programming language such as Python or .NET Demonstrated excellence in troubleshooting, debugging and data performance tuning Demonstrated strength in data modelling, data integration, and analytics Demonstrated strength in documenting, refactoring and optimizing algorithms in code and stored procedures About Advisor360° Come grow with us! Join a team of inventive, driven professionals! Launched in 2019, Advisor360° combines a start-up's agile and innovative markers with the stability of an established enterprise. Ranked the largest FinTech company in Massachusetts by the Boston Business Journal, Advisor360° was born out of 20 years of feedback from the industry's highest-producing financial advisors and has gone on to build a software that is second to none. Our company specializes in building, delivering, and integrating technology for wealth managers. Our comprehensive platform offers a unified approach to managing wealth, delivering a connected experience to advisors and their clients. Benefits include a competitive bonus plan, unlimited paid time off, paid sick time, volunteer time off, free access to vacation homes, a generous 401(k) employer match, stock appreciation rights, referral bonuses, 80% of either HMO or PPO premiums for individuals and family employer covered, 50% of dental insurance paid for, employer life and AD&D at 2X salary, short- and long-term disability coverage, a health savings account, flexible spending accounts, and paid bonding time for new parents. Advisor360° also provides worthwhile perks, including a hybrid structure with most employees working from the office 2 to 3 times per week, tuition reimbursement, professional development opportunities, employee assistance programs, and more! Join us on this journey. Advisor360° is an equal opportunity employer committed to a diverse workforce. We believe diversity drives innovation and are therefore building a company where people of all backgrounds are truly welcome and included. Everyone is encouraged to bring their unique, authentic selves to work each and every day. The way we see it, we are here to learn from each other. Are you ready to build and discover together? About Advisor360° About Advisor360° Come grow with us! Join a team of inventive, driven professionals! Launched in 2019, Advisor360° combines a start-up's agile and innovative markers with the stability of an established enterprise. Ranked the largest FinTech company in Massachusetts by the Boston Business Journal, Advisor360° was born out of 20 years of feedback from the industry's highest-producing financial advisors and has gone on to build a software that is second to none. Our company specializes in building, delivering, and integrating technology for wealth managers. Our comprehensive platform offers a unified approach to managing wealth, delivering a connected experience to advisors and their clients. Benefits include a competitive bonus plan, unlimited paid time off, paid sick time, volunteer time off, free access to vacation homes, a generous 401(k) employer match, stock appreciation rights, referral bonuses, 80% of either HMO or PPO premiums for individuals and family employer covered, 50% of dental insurance paid for, employer life and AD&D at 2X salary, short- and long-term disability coverage, a health savings account, flexible spending accounts, and paid bonding time for new parents. Advisor360° also provides worthwhile perks, including a hybrid structure with most employees working from the office 2 to 3 times per week, tuition reimbursement, professional development opportunities, employee assistance programs, and more! Join us on this journey. Advisor360° is an equal opportunity employer committed to a diverse workforce. We believe diversity drives innovation and are therefore building a company where people of all backgrounds are truly welcome and included. Everyone is encouraged to bring their unique, authentic selves to work each and every day. The way we see it, we are here to learn from each other. Are you ready to build and discover together?",3.3,201 to 500 Employees,2019,Company - Private,Enterprise Software & Network Solutions,Information Technology,Unknown / Non-Applicable
"Dallas, TX",Data Engineer,$88K - $119K (Glassdoor est.),Loopback Analytics,"This employer will not sponsor applicants for employment visa status (e.g., H1-B) for this position. All applicants must be currently authorized to work in the United States on a full-time basis.  Come join our Real World Data team at Loopback Analytics, the Loopback platform assembles clinical, pharmacy, enterprise and social data for insight and action across the specialty pharmacy and life sciences value chain. The ideal candidate would be an experienced Data Engineer who will be responsible for building and maintaining data pipelines. The Data Engineer will facilitate deeper analysis and reporting across complex data sets to support customers.  Job Duties to Include Assemble and manage large, complex sets of data to meet functional business and analytical requirements Build required infrastructure, documentation and roadmap for optimal extraction, transformation and loading of data from various data sources Design infrastructure for greater scalability, optimizing data delivery and automating manual processes Plan, coordinate and implement security measures to safeguard data Work with stakeholders including data, product and executive teams and assist with data-related technical issues Develop and maintain processes for data profiling, data documentation, and data quality measurement leveraging both manual and automated data quality testing  Requirements  Technical Experience: 3-5 years of experience to include: Implementing and designing data infrastructure to support data curation and data analysis Orchestrating data transformation through cloud native analytics platforms (Snowflake, Databricks) across cloud environments (Azure, AWS, GCP) Building and modeling data in relational and non-relational data storage technologies including schema design, stored procedure development and performance and optimization techniques (e.g. SQL & NoSQL, C#, Python, etc.) Learning and understanding the various technical domains across the enterprise and able to communicate complex technical and business concepts across the enterprise and various business stakeholders Documenting and testing of designed solutions Writing code that runs in a production system or experience in machine learning  Required Education: Bachelors, masters, or Ph.D. in computer science, software engineering or a related field or equivalent experience  Personal Characteristics: Complex problem solver Excellent program/task organizational skills Detail and results oriented Excellent communication skills  Travel: Minimal  All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, or national origin. For immediate full-time consideration, please forward your resume to Loopback Analytics via email at careers@loopbackanalytics.com.  About Loopback  Founded in 2009, Loopback was rated as one of the best places to work in Dallas by the DBJ. Loopback Analytics is a leading provider of data-driven solutions for hospitals and health systems. The company’s comprehensive analytics platform drives growth for specialty and ambulatory pharmacy programs while connecting pharmacy activities with clinical and economic outcomes. Loopback’s clients include leading academic medical centers, health systems, and life sciences companies. For more information about our company and services please visit our website at www.loopbackanalytics.com. This employer will not sponsor applicants for employment visa status (e.g., H1-B) for this position. All applicants must be currently authorized to work in the United States on a full-time basis. This employer will not sponsor applicants for employment visa status (e.g., H1-B) for this position. All applicants must be currently authorized to work in the United States on a full-time basis. Come join our Real World Data team at Loopback Analytics, the Loopback platform assembles clinical, pharmacy, enterprise and social data for insight and action across the specialty pharmacy and life sciences value chain. The ideal candidate would be an experienced Data Engineer who will be responsible for building and maintaining data pipelines. The Data Engineer will facilitate deeper analysis and reporting across complex data sets to support customers. Job Duties to Include Job Duties to Include Assemble and manage large, complex sets of data to meet functional business and analytical requirements Build required infrastructure, documentation and roadmap for optimal extraction, transformation and loading of data from various data sources Design infrastructure for greater scalability, optimizing data delivery and automating manual processes Plan, coordinate and implement security measures to safeguard data Work with stakeholders including data, product and executive teams and assist with data-related technical issues Develop and maintain processes for data profiling, data documentation, and data quality measurement leveraging both manual and automated data quality testing Assemble and manage large, complex sets of data to meet functional business and analytical requirements Build required infrastructure, documentation and roadmap for optimal extraction, transformation and loading of data from various data sources Design infrastructure for greater scalability, optimizing data delivery and automating manual processes Plan, coordinate and implement security measures to safeguard data Work with stakeholders including data, product and executive teams and assist with data-related technical issues Develop and maintain processes for data profiling, data documentation, and data quality measurement leveraging both manual and automated data quality testing Requirements Requirements Technical Experience: 3-5 years of experience to include: Implementing and designing data infrastructure to support data curation and data analysis Orchestrating data transformation through cloud native analytics platforms (Snowflake, Databricks) across cloud environments (Azure, AWS, GCP) Building and modeling data in relational and non-relational data storage technologies including schema design, stored procedure development and performance and optimization techniques (e.g. SQL & NoSQL, C#, Python, etc.) Learning and understanding the various technical domains across the enterprise and able to communicate complex technical and business concepts across the enterprise and various business stakeholders Documenting and testing of designed solutions Writing code that runs in a production system or experience in machine learning Implementing and designing data infrastructure to support data curation and data analysis Orchestrating data transformation through cloud native analytics platforms (Snowflake, Databricks) across cloud environments (Azure, AWS, GCP) Building and modeling data in relational and non-relational data storage technologies including schema design, stored procedure development and performance and optimization techniques (e.g. SQL & NoSQL, C#, Python, etc.) Learning and understanding the various technical domains across the enterprise and able to communicate complex technical and business concepts across the enterprise and various business stakeholders Documenting and testing of designed solutions Writing code that runs in a production system or experience in machine learning Required Education: Bachelors, masters, or Ph.D. in computer science, software engineering or a related field or equivalent experience Bachelors, masters, or Ph.D. in computer science, software engineering or a related field or equivalent experience Personal Characteristics: Complex problem solver Excellent program/task organizational skills Detail and results oriented Excellent communication skills Complex problem solver Excellent program/task organizational skills Detail and results oriented Excellent communication skills Travel: Minimal Minimal All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, or national origin. For immediate full-time consideration, please forward your resume to Loopback Analytics via email at careers@loopbackanalytics.com. About Loopback About Loopback Founded in 2009, Loopback was rated as one of the best places to work in Dallas by the DBJ. Loopback Analytics is a leading provider of data-driven solutions for hospitals and health systems. The company’s comprehensive analytics platform drives growth for specialty and ambulatory pharmacy programs while connecting pharmacy activities with clinical and economic outcomes. Loopback’s clients include leading academic medical centers, health systems, and life sciences companies. For more information about our company and services please visit our website at www.loopbackanalytics.com.",4.4,51 to 200 Employees,2009,Company - Private,Health Care Services & Hospitals,Healthcare,Unknown / Non-Applicable
Remote,AWS Data Engineer,$90K - $120K (Employer est.),N/A,N/A,N/A,,,,,,
"Rancho Cucamonga, CA",Data Engineer,$91K - $116K (Employer est.),Inland Empire Health Plan,"A reasonable salary expectation is between $91,000.00 and $116,022.40, based upon experience and internal equity. A reasonable salary expectation is between $91,000.00 and $116,022.40, based upon experience and internal equity. A reasonable salary expectation is between $91,000.00 and $116,022.40, based upon experience and internal equity. This position is on a hybrid work schedule. (Mon & Fri - remote, Tues - Thurs onsite in Rancho Cucamonga, CA) This position is on a hybrid work schedule. (Mon & Fri - remote, Tues - Thurs onsite in Rancho Cucamonga, CA) This position is on a hybrid work schedule. (Mon & Fri - remote, Tues - Thurs onsite in Rancho Cucamonga, CA) Position Summary/Position Position Summary/Position The Data Engineer II assists in the implementation of methods to improve data reliability and quality. This role is responsible for combining raw information from different sources to create consistent and machine-readable formats. The Data Engineer II must also develop and test architectures that enable data extraction and transformation for predictive or prescriptive modeling. The Data Engineer II will focus on data accessibility, which will enable the organization to utilize data for performance evaluation and optimization. The data Engineer II is also responsible for managing the entire back-end development life cycle for the company's enterprise data warehouse. In this role the incumbent will handle tasks associated with the implementation of ETL procedures, building warehouse databases, database performance management, and dimensional modeling and design of the table structures. Major Functions (Duties and Responsibilities) Major Functions (Duties and Responsibilities) 1. Design and develop data warehouse Extraction, Transformation and Loading (ETL) solutions using Microsoft SQL Server Integration Services (SSIS), Azure Data Factory, Synapse Analytics, Az Data Bricks, PySpark ETL. 2. Develop and implement data collection processes in conjunction with the data warehouse. Source data from legacy systems supporting a centralized data warehouse and reporting platform. 3. Develop technical solutions to meet the requirements for Data Warehouse, BI & Analytics 4. Work closely with the data engineering and BI & Analytics teams to design data and analytics solutions to increase the usability, completeness, and accuracy of enterprise data 5. Analyze user requirements and translate into database requirements and implement in database code 6. Create and maintain the optimal data pipeline architectures based on micro services based on platform and application requirements 7. Assemble large, complex data sets that meet functional / non-functional business requirements 8. Identify, design, and implement process improvements: automating manual pipeline processes, optimizing data ingestion and consumption, re-designing infrastructure for greater scalability, micro services, etc 9. Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs. 10. Work closely with Data Warehouse Architect and Data Systems Architect to design data and analytics solutions to increase the usability, completeness, and accuracy of enterprise data 11. Create, maintain, and optimize SQL queries and routines 12. Analyze potential data quality issues to determine the root cause and create effective solutions. 13. Develop, adopt, and enforce Data Warehouse and ETL standards and architecture 14. Monitor and support ETL processes ensuring integrity and proper integration of all data sources 15. Create high throughput historical and incremental ETL jobs 16. Facilitate problem management, and communication among data architects, managers, informaticists and analysts 17. Provide detailed analysis of data issues; data mapping; and the process for automation and enhancement of data quality 18. Perform development activities such as source to target mapping validations, identify, document and execute unit test cases/scripts, peer and lead code reviews per code review checklist and document test and review results. 19. Collaborate and contribute to data integration strategies and visions 20. Provide ongoing proactive technical support for ETL and data warehouse system to ensure business continuity. 21. Work with Informaticists and Analysts to translate analytic requirements into technical solutions. Experience Qualifications Experience Qualifications Four (4) years of relevant work experience. Experience and knowledge in logical, rational, dimensional, and physical data modeling. Background in database systems along with a strong knowledge of SQL. Experience with Orchestration tools, Azure DevOps, and CI/CD. Intermediate experience with the following tools and technologies: a. Azure Data Catalogue / Purview b. Azure Cloud c. Databricks d. Power BI Dataflows e. Power Query f. Azure Cosmos g. Azure Monitor h. PowerShell i. Python Preferred Experience Preferred Experience Development experience using PySpark, Spark, Hadoop, Kubernetes, and RDMIS is highly desired. Education Qualifications Education Qualifications Bachelor's degree from an accredited institution required. Preferred Education Preferred Education Master’s degree from an accredited institution preferred. Professional Certification Professional Certification Azure Data Engineering Certification is preferred. Knowledge Requirement Knowledge Requirement Multi-server environment knowledge such as linked servers, data replication, backup/restore with MS SQL Server 2008+. Knowledge of applicable data privacy practices and laws. Skills Requirement Skills Requirement Highly skilled in developing and optimizing T-SQL (DDL, DML, DCL) queries, stored procedures, functions, and views for various applications that involve numerous database tables and complex business logic. Good written and oral communication skills. Strong technical documentation skills. Good interpersonal skills. Abilities Requirement Abilities Requirement Highly self-motivated and directed. Keen attention to detail. Proven analytical and problem-solving abilities. Ability to effectively prioritize and execute tasks in a high-pressure environment. Commitment to Team Culture Commitment to Team Culture The IEHP Team environment requires a Team Member to participate in the IEHP Team Culture. A Team Member demonstrates support of the Culture by developing professional and effective working relationships that include elements of respect and cooperation with Team Members, Members and associates outside of our organization. Working Conditions Working Conditions Word processing and programming involving computer keyboard and screens. Position is eligible for Hybrid work location upon completing the necessary steps and receiving HR approval. All IEHP positions approved for telecommute or hybrid work locations may periodically be required to report to IEHP’s main campus for mandatory in-person meetings or for other business needs as determined by IEHP leadership. Inland Empire Health Plan (IEHP) is the largest not-for-profit Medi-Cal and Medicare health plan in the Inland Empire. We are also one of the largest employers in the region, designated as “Great Place to Work.” With a provider network of more than 5,000 and a team of more than 3,000 employees, IEHP provides quality, accessible healthcare services to more than 1.5 million members. And our Mission, Vision, and Values help guide us in the development of innovative programs and the creation of an award-winning workplace. As the healthcare landscape is transformed, we’re ready to make a difference today and in the years to come. Join our Team and make a difference with us! IEHP offers a competitive salary and stellar benefit package with a value estimated at 35% of the annual salary, including medical, dental, vision, team bonus, and state pension plan. Job Types: Full-time, Permanent Pay: $91,000.00 - $116,022.40 per year Benefits: 401(k) 401(k) matching Dental insurance Flexible spending account Health insurance Life insurance Paid time off Parental leave Retirement plan Tuition reimbursement Vision insurance 401(k) 401(k) matching Dental insurance Flexible spending account Health insurance Life insurance Paid time off Parental leave Retirement plan Tuition reimbursement Vision insurance Experience level: 4 years 4 years Schedule: 8 hour shift Day shift Monday to Friday 8 hour shift Day shift Monday to Friday Ability to commute/relocate: Rancho Cucamonga, CA 91730: Reliably commute or planning to relocate before starting work (Required) Rancho Cucamonga, CA 91730: Reliably commute or planning to relocate before starting work (Required) Education: Bachelor's (Required) Bachelor's (Required) Experience: data engineering: 4 years (Required) data engineering: 4 years (Required) Work Location: Hybrid remote in Rancho Cucamonga, CA 91730",3.7,1001 to 5000 Employees,1996,Company - Public,Health Care Services & Hospitals,Healthcare,$1 to $5 billion (USD)
Remote,Sr. Data Engineer,$120K - $153K (Employer est.),Nava,"About Nava Nava is a consultancy and public benefit corporation working to make government services simple, effective, and accessible to all. Since 2013, federal, state, and local government agencies have trusted Nava to build transformative digital services to help people access public benefits. Meeting our mission is an opportunity to restore trust between people and public institutions. We focus on populations that are the least protected because the stakes are higher.  As a client services company, we work with government agencies to improve how people apply for benefits, navigate their health care, and more. We bill for our time, selling our expertise and problem-solving methodology to government clients. Our clients hire us to help improve their products and services so that their users and beneficiaries have a better customer experience.  These end-users—the humans who benefit from our work—are at the core of everything we do. We research beneficiaries’ needs to help our government clients better deliver on their missions, providing everyone at Nava opportunities to do meaningful, impactful work.  Position Summary Nava is at the forefront of reimagining how our government serves its people, and we are looking for a Data Engineer who will help us to drive our vision forward. Working in a highly-collaborative environment, you’ll create experiences that improve the lives of millions of Americans. We're looking for a Data Engineer who cares deeply about end users and is passionate about crafting simple and usable solutions.  In this role, you'll work with government stakeholders, alongside Nava's engineering, design and product teams, to modernize data architectures and data pipelines in critical government programs. You'll design and implement data models and databases, improve data pipelines, enhance data security, and write code to process data more efficiently and support storage, processing, and analysis of terabytes of critical data. You will provide subject matter expertise and actively contribute in data and architecture review meetings, while supporting the implementation of data integration requirements and developing the pipeline from raw data through curation layers (including data acquisition, cleaning, transformation, derivation, and aggregation) to consumable data for Data Analysts and Data Scientists.  You will deploy and operate mission-critical, highly-available, and scalable systems by adopting and defining standards and best practices in data engineering. You will support our software developers and Infrastructure Engineers on data initiatives and will help ensure data delivery is optimal and consistent throughout ongoing projects. You will support the data needs of multiple teams, systems, and products in addition to creating and optimizing our government partners' data architecture to support their data initiatives and programs.  You will have a significant strategic impact and will be uniquely positioned and empowered to make a huge impact on Nava's mission to reimagine how our government serves its people. The ideal candidate will be comfortable working directly with clients in both a consulting and delivery capacity to tackle complex, enterprise cloud, or on-premises software and technology projects. You'll have an innate desire to learn new technologies and languages as well as be a problem solver who likes and creative thinker. You're also highly organized and works well independently, as part of a team, and with stakeholders. Our ideal candidate will also be an effective oral and written communicator with a strong ability to relay technical concepts clearly and concisely to a wide audience with various levels of technical knowledge and awareness. What you’ll do Document, improve, and maintain data strategies and artifacts, including logical and physical data models, data dictionary, data architecture roadmap, and data security policies, using industry best practices and adhering to federal standards, along with operational runbook procedures Collect data access patterns and review current data models to optimize designs for customer use cases Standardize data ingestion and processing pipelines to scale with increasing utilization Audit and reverse-engineer business rules in legacy systems, and build data connectors for integrating them into a data and analytics platform Implement large-scale data ecosystems within cloud-based platforms that include data management and data governance of structured and unstructured data Leverage and enhance automation to speed development and improve reliability and performance Work with cross-functional project teams to gather business requirements and translate to detailed technical specifications Work with Government partners to assist and develop data engineering applications and pipelines that will enable data services and processing capabilities Design, develop, test, automate, and deploy data engineering solutions in a cloud platforms, such as Azure Participate in software design and code reviews. Develop automated testing, monitoring and alerting, and CI/CD for production systems Maintain security and privacy standards in all aspects of the data pipeline Required skills 7+ years data engineering experience 3+ years of experience in cloud data architecture (AWS preferred) and big data technologies Experience with professional software engineering practices using such tools and methodologies as Agile Software Development, Test Driven Development, CI/CD, and Source Code Management Experience with building ETL pipelines in Python Proficient with relational databases and advanced SQL queries Prior experience with Java, Python, Scala Experience with data cleaning and data modeling while protecting sensitive data Proficient with building data integrations using both API and file-based protocols Proficient in refining high-level goals into high-impact, low-effort tasks and milestones based on human-centered design practices to prioritize options for stakeholders  Other requirements The individual working in this position: Must be legally authorized to work in the United States and meet any other requirements for government contracts for which they are hired. Candidates who are offered a job with Nava must possess work authorization that does not require sponsorship by their employer for a visa now or in the future. May be subject to a government security investigation and must meet eligibility requirements for access to classified information or applicants who are eligible for security clearances.  Perks working with Nava Your well-being is important to us, so we offer highly competitive benefits for medical, dental, and vision 20 days of PTO accrued, 12 days paid federal holidays, 5 floating holidays, unlimited sick leave 16 weeks of fully paid parental leave and weekly meal deliveries during leave Sabbatical Leave 401k contributions match at 4% of your salary Flexible work arrangements $1,000 new home office set up budget, monthly phone allowance Monthly partial reimbursement for utilities (where applicable) $2,000 annual tuition and professional development budget, on top of a LinkedIn Learning license Equity stock options Employee referral program Commuter Benefits Short and Long-Term Disability Insurance Life and Accidental Death Insurance Diverse, inclusive, highly collaborative, and vibrant culture, fostering remote work!  Location You can work in a hybrid work arrangement from one of Nava’s offices in NYC, DC, or San Francisco. We also have fully remote options if you reside in one of the following states:  Alabama, Arizona, California, Colorado, DC, Florida, Georgia, Illinois, Louisiana, Maine, Maryland, Massachusetts, Michigan, Minnesota, North Carolina, New Jersey, New York, Oklahoma, Oregon, Pennsylvania, Rhode Island, South Carolina, Texas, Tennessee, Virginia, Washington, Wisconsin.  If you are not living in one of the states listed above, unfortunately, you will not be considered for a position at this time.  Stay in touch Sign up for our newsletter to find out about career opportunities, new partnerships, and news from the broader civic tech community.  Nava PBC equal opportunity employer that is deeply committed to diversity and inclusion in the workplace. We prohibit discrimination and harassment of any kind based on race, color, age, gender, religious or political beliefs, national origin or heritage, marital status, disability, sex, sexual orientation or gender identity, genetic information, pregnancy, status as a protected veteran or any characteristic protected by federal, state, or local laws. Our commitment to diversity, equity, and inclusion not only reflects our values as a public benefit corporation but also enriches our ability to do our work. Learn more about where we are today and hope to be by 2025.  Please contact the recruiting team at recruiting@navapbc.com if you would like to request reasonable accommodation during the application or interviewing process.  We participate in E-Verify. Upon hire, we will provide the federal government with your Form I-9 information to confirm that you are authorized to work in the U.S. This role is required to work from the contiguous United States. About Nava About Nava Nava is a consultancy and public benefit corporation working to make government services simple, effective, and accessible to all. Since 2013, federal, state, and local government agencies have trusted Nava to build transformative digital services to help people access public benefits. Meeting our mission is an opportunity to restore trust between people and public institutions. We focus on populations that are the least protected because the stakes are higher. As a client services company, we work with government agencies to improve how people apply for benefits, navigate their health care, and more. We bill for our time, selling our expertise and problem-solving methodology to government clients. Our clients hire us to help improve their products and services so that their users and beneficiaries have a better customer experience. These end-users—the humans who benefit from our work—are at the core of everything we do. We research beneficiaries’ needs to help our government clients better deliver on their missions, providing everyone at Nava opportunities to do meaningful, impactful work. Position Summary Position Summary Nava is at the forefront of reimagining how our government serves its people, and we are looking for a Data Engineer who will help us to drive our vision forward. Working in a highly-collaborative environment, you’ll create experiences that improve the lives of millions of Americans. We're looking for a Data Engineer who cares deeply about end users and is passionate about crafting simple and usable solutions. In this role, you'll work with government stakeholders, alongside Nava's engineering, design and product teams, to modernize data architectures and data pipelines in critical government programs. You'll design and implement data models and databases, improve data pipelines, enhance data security, and write code to process data more efficiently and support storage, processing, and analysis of terabytes of critical data. You will provide subject matter expertise and actively contribute in data and architecture review meetings, while supporting the implementation of data integration requirements and developing the pipeline from raw data through curation layers (including data acquisition, cleaning, transformation, derivation, and aggregation) to consumable data for Data Analysts and Data Scientists. You will deploy and operate mission-critical, highly-available, and scalable systems by adopting and defining standards and best practices in data engineering. You will support our software developers and Infrastructure Engineers on data initiatives and will help ensure data delivery is optimal and consistent throughout ongoing projects. You will support the data needs of multiple teams, systems, and products in addition to creating and optimizing our government partners' data architecture to support their data initiatives and programs. You will have a significant strategic impact and will be uniquely positioned and empowered to make a huge impact on Nava's mission to reimagine how our government serves its people. The ideal candidate will be comfortable working directly with clients in both a consulting and delivery capacity to tackle complex, enterprise cloud, or on-premises software and technology projects. You'll have an innate desire to learn new technologies and languages as well as be a problem solver who likes and creative thinker. You're also highly organized and works well independently, as part of a team, and with stakeholders. Our ideal candidate will also be an effective oral and written communicator with a strong ability to relay technical concepts clearly and concisely to a wide audience with various levels of technical knowledge and awareness. What you’ll do What you’ll do Document, improve, and maintain data strategies and artifacts, including logical and physical data models, data dictionary, data architecture roadmap, and data security policies, using industry best practices and adhering to federal standards, along with operational runbook procedures Collect data access patterns and review current data models to optimize designs for customer use cases Standardize data ingestion and processing pipelines to scale with increasing utilization Audit and reverse-engineer business rules in legacy systems, and build data connectors for integrating them into a data and analytics platform Implement large-scale data ecosystems within cloud-based platforms that include data management and data governance of structured and unstructured data Leverage and enhance automation to speed development and improve reliability and performance Work with cross-functional project teams to gather business requirements and translate to detailed technical specifications Work with Government partners to assist and develop data engineering applications and pipelines that will enable data services and processing capabilities Design, develop, test, automate, and deploy data engineering solutions in a cloud platforms, such as Azure Participate in software design and code reviews. Develop automated testing, monitoring and alerting, and CI/CD for production systems Maintain security and privacy standards in all aspects of the data pipeline Document, improve, and maintain data strategies and artifacts, including logical and physical data models, data dictionary, data architecture roadmap, and data security policies, using industry best practices and adhering to federal standards, along with operational runbook procedures Collect data access patterns and review current data models to optimize designs for customer use cases Standardize data ingestion and processing pipelines to scale with increasing utilization Audit and reverse-engineer business rules in legacy systems, and build data connectors for integrating them into a data and analytics platform Implement large-scale data ecosystems within cloud-based platforms that include data management and data governance of structured and unstructured data Leverage and enhance automation to speed development and improve reliability and performance Work with cross-functional project teams to gather business requirements and translate to detailed technical specifications Work with Government partners to assist and develop data engineering applications and pipelines that will enable data services and processing capabilities Design, develop, test, automate, and deploy data engineering solutions in a cloud platforms, such as Azure Participate in software design and code reviews. Develop automated testing, monitoring and alerting, and CI/CD for production systems Maintain security and privacy standards in all aspects of the data pipeline Required skills Required skills 7+ years data engineering experience 3+ years of experience in cloud data architecture (AWS preferred) and big data technologies Experience with professional software engineering practices using such tools and methodologies as Agile Software Development, Test Driven Development, CI/CD, and Source Code Management Experience with building ETL pipelines in Python Proficient with relational databases and advanced SQL queries Prior experience with Java, Python, Scala Experience with data cleaning and data modeling while protecting sensitive data Proficient with building data integrations using both API and file-based protocols Proficient in refining high-level goals into high-impact, low-effort tasks and milestones based on human-centered design practices to prioritize options for stakeholders 7+ years data engineering experience 3+ years of experience in cloud data architecture (AWS preferred) and big data technologies Experience with professional software engineering practices using such tools and methodologies as Agile Software Development, Test Driven Development, CI/CD, and Source Code Management Experience with building ETL pipelines in Python Proficient with relational databases and advanced SQL queries Prior experience with Java, Python, Scala Experience with data cleaning and data modeling while protecting sensitive data Proficient with building data integrations using both API and file-based protocols Proficient in refining high-level goals into high-impact, low-effort tasks and milestones based on human-centered design practices to prioritize options for stakeholders Other requirements Other requirements The individual working in this position: Must be legally authorized to work in the United States and meet any other requirements for government contracts for which they are hired. Candidates who are offered a job with Nava must possess work authorization that does not require sponsorship by their employer for a visa now or in the future. May be subject to a government security investigation and must meet eligibility requirements for access to classified information or applicants who are eligible for security clearances. Must be legally authorized to work in the United States and meet any other requirements for government contracts for which they are hired. Candidates who are offered a job with Nava must possess work authorization that does not require sponsorship by their employer for a visa now or in the future. May be subject to a government security investigation and must meet eligibility requirements for access to classified information or applicants who are eligible for security clearances. Perks working with Nava Perks working with Nava Your well-being is important to us, so we offer highly competitive benefits for medical, dental, and vision 20 days of PTO accrued, 12 days paid federal holidays, 5 floating holidays, unlimited sick leave 16 weeks of fully paid parental leave and weekly meal deliveries during leave Sabbatical Leave 401k contributions match at 4% of your salary Flexible work arrangements $1,000 new home office set up budget, monthly phone allowance Monthly partial reimbursement for utilities (where applicable) $2,000 annual tuition and professional development budget, on top of a LinkedIn Learning license Equity stock options Employee referral program Commuter Benefits Short and Long-Term Disability Insurance Life and Accidental Death Insurance Diverse, inclusive, highly collaborative, and vibrant culture, fostering remote work! Your well-being is important to us, so we offer highly competitive benefits for medical, dental, and vision 20 days of PTO accrued, 12 days paid federal holidays, 5 floating holidays, unlimited sick leave 16 weeks of fully paid parental leave and weekly meal deliveries during leave Sabbatical Leave 401k contributions match at 4% of your salary Flexible work arrangements $1,000 new home office set up budget, monthly phone allowance Monthly partial reimbursement for utilities (where applicable) where applicable) $2,000 annual tuition and professional development budget, on top of a LinkedIn Learning license Equity stock options Employee referral program Commuter Benefits Short and Long-Term Disability Insurance Life and Accidental Death Insurance Diverse, inclusive, highly collaborative, and vibrant culture, fostering remote work! Location Location You can work in a hybrid work arrangement from one of Nava’s offices in NYC, DC, or San Francisco. We also have fully remote options if you reside in one of the following states: Alabama, Arizona, California, Colorado, DC, Florida, Georgia, Illinois, Louisiana, Maine, Maryland, Massachusetts, Michigan, Minnesota, North Carolina, New Jersey, New York, Oklahoma, Oregon, Pennsylvania, Rhode Island, South Carolina, Texas, Tennessee, Virginia, Washington, Wisconsin. If you are not living in one of the states listed above, unfortunately, you will not be considered for a position at this time. If you are not living in one of the states listed above, unfortunately, you will not be considered for a position at this time. If you are not living in one of the states listed above, unfortunately, you will not be considered for a position at this time. Stay in touch Stay in touch Sign up for our newsletter to find out about career opportunities, new partnerships, and news from the broader civic tech community. Nava PBC equal opportunity employer that is deeply committed to diversity and inclusion in the workplace. We prohibit discrimination and harassment of any kind based on race, color, age, gender, religious or political beliefs, national origin or heritage, marital status, disability, sex, sexual orientation or gender identity, genetic information, pregnancy, status as a protected veteran or any characteristic protected by federal, state, or local laws. Our commitment to diversity, equity, and inclusion not only reflects our values as a public benefit corporation but also enriches our ability to do our work. Learn more about where we are today and hope to be by 2025. Please contact the recruiting team at recruiting@navapbc.com if you would like to request reasonable accommodation during the application or interviewing process. Please contact the recruiting team at recruiting@navapbc.com if you would like to request reasonable accommodation during the application or interviewing process. We participate in E-Verify. Upon hire, we will provide the federal government with your Form I-9 information to confirm that you are authorized to work in the U.S. This role is required to work from the contiguous United States. We participate in E-Verify. Upon hire, we will provide the federal government with your Form I-9 information to confirm that you are authorized to work in the U.S. This role is required to work from the contiguous United States.",4.5,1001 to 5000 Employees,1996,Company - Private,Information Technology Support Services,Information Technology,$1 to $5 million (USD)
"Irving, TX",Data Science/ Machine Learning Engineer,$87K - $124K (Glassdoor est.),ICS Global Soft,"Data Science/ Machine Learning Engineer Machine Learning Engineer responsibilities include creating machine learning models and retraining systems. To do this job successfully, you need exceptional skills in statistics and programming. If you also have knowledge of data science and software engineering, we’d like to meet you. Responsibilities: Study and transform data science prototypes Design machine learning systems Research and implement appropriate ML algorithms and tools Develop machine learning applications according to requirements Select appropriate datasets and data representation methods Run machine learning tests and experiments Perform statistical analysis and fine-tuning using test results Requirements: Proven experience as a Machine Learning Engineer or similar role Understanding of data structures, data modeling and software architecture Deep knowledge of math, probability, statistics and algorithms Ability to write robust code in Python, Java and R Familiarity with machine learning frameworks (like Keras or PyTorch) and libraries (like scikit-learn) Excellent communication skills Ability to work in a team Mail Resume to ICS Global Soft, INC, 1231 Greenway Drive STE # 375, Irving TX 75038. Data Science/ Machine Learning Engineer Machine Learning Engineer responsibilities include creating machine learning models and retraining systems. To do this job successfully, you need exceptional skills in statistics and programming. If you also have knowledge of data science and software engineering, we’d like to meet you. Responsibilities: Study and transform data science prototypes Design machine learning systems Research and implement appropriate ML algorithms and tools Develop machine learning applications according to requirements Select appropriate datasets and data representation methods Run machine learning tests and experiments Perform statistical analysis and fine-tuning using test results Requirements: Proven experience as a Machine Learning Engineer or similar role Understanding of data structures, data modeling and software architecture Deep knowledge of math, probability, statistics and algorithms Ability to write robust code in Python, Java and R Familiarity with machine learning frameworks (like Keras or PyTorch) and libraries (like scikit-learn) Excellent communication skills Ability to work in a team Mail Resume to ICS Global Soft, INC, 1231 Greenway Drive STE # 375, Irving TX 75038. Data Science/ Machine Learning Engineer Machine Learning Engineer responsibilities include creating machine learning models and retraining systems. To do this job successfully, you need exceptional skills in statistics and programming. If you also have knowledge of data science and software engineering, we’d like to meet you. Responsibilities: Study and transform data science prototypes Design machine learning systems Research and implement appropriate ML algorithms and tools Develop machine learning applications according to requirements Select appropriate datasets and data representation methods Run machine learning tests and experiments Perform statistical analysis and fine-tuning using test results Requirements: Proven experience as a Machine Learning Engineer or similar role Understanding of data structures, data modeling and software architecture Deep knowledge of math, probability, statistics and algorithms Ability to write robust code in Python, Java and R Familiarity with machine learning frameworks (like Keras or PyTorch) and libraries (like scikit-learn) Excellent communication skills Ability to work in a team Mail Resume to ICS Global Soft, INC, 1231 Greenway Drive STE # 375, Irving TX 75038. Data Science/ Machine Learning Engineer Machine Learning Engineer responsibilities include creating machine learning models and retraining systems. To do this job successfully, you need exceptional skills in statistics and programming. If you also have knowledge of data science and software engineering, we’d like to meet you. Responsibilities: Study and transform data science prototypes Design machine learning systems Research and implement appropriate ML algorithms and tools Develop machine learning applications according to requirements Select appropriate datasets and data representation methods Run machine learning tests and experiments Perform statistical analysis and fine-tuning using test results Requirements: Proven experience as a Machine Learning Engineer or similar role Understanding of data structures, data modeling and software architecture Deep knowledge of math, probability, statistics and algorithms Ability to write robust code in Python, Java and R Familiarity with machine learning frameworks (like Keras or PyTorch) and libraries (like scikit-learn) Excellent communication skills Ability to work in a team Mail Resume to ICS Global Soft, INC, 1231 Greenway Drive STE # 375, Irving TX 75038. Data Science/ Machine Learning Engineer Machine Learning Engineer responsibilities include creating machine learning models and retraining systems. To do this job successfully, you need exceptional skills in statistics and programming. If you also have knowledge of data science and software engineering, we’d like to meet you. Data Science/ Machine Learning Engineer Machine Learning Engineer responsibilities include creating machine learning models and retraining systems. To do this job successfully, you need exceptional skills in statistics and programming. If you also have knowledge of data science and software engineering, we’d like to meet you. Data Science/ Machine Learning Engineer Data Science/ Machine Learning Engineer Machine Learning Engineer responsibilities include creating machine learning models and retraining systems. To do this job successfully, you need exceptional skills in statistics and programming. If you also have knowledge of data science and software engineering, we’d like to meet you. Responsibilities: Responsibilities: Responsibilities: Study and transform data science prototypes Design machine learning systems Research and implement appropriate ML algorithms and tools Develop machine learning applications according to requirements Select appropriate datasets and data representation methods Run machine learning tests and experiments Perform statistical analysis and fine-tuning using test results Study and transform data science prototypes Design machine learning systems Research and implement appropriate ML algorithms and tools Develop machine learning applications according to requirements Select appropriate datasets and data representation methods Run machine learning tests and experiments Perform statistical analysis and fine-tuning using test results Requirements: Requirements: Requirements: Proven experience as a Machine Learning Engineer or similar role Understanding of data structures, data modeling and software architecture Deep knowledge of math, probability, statistics and algorithms Ability to write robust code in Python, Java and R Familiarity with machine learning frameworks (like Keras or PyTorch) and libraries (like scikit-learn) Excellent communication skills Ability to work in a team Proven experience as a Machine Learning Engineer or similar role Understanding of data structures, data modeling and software architecture Deep knowledge of math, probability, statistics and algorithms Ability to write robust code in Python, Java and R Familiarity with machine learning frameworks (like Keras or PyTorch) and libraries (like scikit-learn) Excellent communication skills Ability to work in a team Mail Resume to ICS Global Soft, INC, 1231 Greenway Drive STE # 375, Irving TX 75038. Mail Resume to ICS Global Soft, INC, 1231 Greenway Drive STE # 375, Irving TX 75038. We appraise to boost, inspire to conquer. Join the league, apply with your resume to info@icsglobalsoftinc.com. We appraise to boost, inspire to conquer. Join the league, apply with your resume to info@icsglobalsoftinc.com.",4.1,Unknown,N/A,Company - Private,Staffing & Subcontracting,Human Resources & Staffing,Unknown / Non-Applicable
"South Bend, IN",Data Engineer,$82K - $110K (Glassdoor est.),Aunalytics,"Position Overview At the heart of our Daybreak teams are our super talented Data Engineers. Data Engineers are data experts who dive right into new client projects and make it their job to understand how a client’s data fits into our Industry Intelligent data models. Utilizing this knowledge and the industry’s newest technologie, they create high performance databases that become the very foundation of the work we do. Critical at all stages of the data science process, Data Engineers work cross-functionally with both external and internal teams – from business analysts to data scientists; web app developers to platform engineers; IT teams to high-level executives. Data Engineers also provide valuable feedback to our software team that helps to shape the development of Aunsight, our proprietary end-to-end cloud analytics platform; and the development of our proprietary web applications. The best Data Engineers are patient, persistent, focused, creative, and incredibly curious. They love to learn and seek out opportunities to identify unexpected solutions or develop alternate ways to solve challenging problems.  Essential Duties & Responsibilities: Build and own “one source of truth” data sets to facilitate consistency and efficiency in extracting and analyzing data from disparate data sources Ensure data integrity by developing and executing necessary processes and controls around the flow of data Innovate and improve efficiency of managing data to allow for greater speed and accuracy of producing analyses, metrics, and insights Collaborate with internal and external teams to understand business needs/issues, troubleshoot problems, conduct root cause analysis, and develop cost effective resolutions for data anomalies. Provides input into data governance initiatives to enhance current systems, ensure development of efficient application systems, influence the development of data policy, and support overall corporate and business goals Utilize technology to analyze data from applicable systems to review data processes, identify issues, and determine actions to resolve or escalate problems that require data, system, or process improvement Verify accuracy of table changes and data transformation processes. Test changes prior to deployment as appropriate. Recommend and implement enhancements that standardize and streamline processes, assure data quality and reliability, and reduce processing time to meet client expectations Communicate progress and completion to project team. Escalate roadblocks that may impact delivery schedule Stay up-to-date on data engineering and data science trends and developments Follow company policy and procedures which protect sensitive data and maintain compliance with established security standards and best practices Additional duties as assigned to ensure client and company success  Required Skills: Bachelor’s degree in Information Science, Computer Science, Computer Engineering, Mathematics, or related field, or 3 plus years of relevant work experience Experience working with relational database structures, SQL and/or flat files and performing table joins, web crawling, and web development Proficiency in one or more of the following programming languages: Java, C#, C++, or Python and a familiarity with Node.js Experience working with commercial relational database systems such as electronic medical records or other clinical systems, client relationship management software, or accounting systems a plus Familiar with various data management methodologies, data exploration techniques, data quality assurance practices, and data discovery/ visualization tools Natural curiosity about what’s hidden in the data through exploration, attention to detail, and ability to see the big picture – similar to putting together a 10,000-piece puzzle Ability to communicate your ideas (verbal and written) so that team members and clients can understand them Share our values: growth, relationships, integrity, and true grit  What's in it for You? Opportunity to work with a rapidly expanding tech company in the booming field of data analytics alongside some of the brightest minds in the industry Opportunity to work with cutting-edge technology in a casual, fun environment Opportunity to be a part of a local company committed to making a difference in our community Flexible schedule and paid time off Social events such as happy hours, game nights, holiday parties, birthday celebrations, movie days, ice cream sundae bars, fancy coffee carts, company softball team, etc. Competitive salary and benefits package including health, vision, dental and life insurance and 401(k) plan. Aunalytics is a data platform company that delivers insights as a service to answer a company’s most important IT and business questions. Our cloud-native data platform is built for universal data access, advanced analytics, and AI; unifying disparate data silos into a single golden record of accurate, actionable business information. Through our side-by-side digital transformation model, we provide on-demand scalable access to technology, data science, and AI experts to seamlessly transform a client’s business. Position Overview Position Overview Position Overview At the heart of our Daybreak teams are our super talented Data Engineers. Data Engineers are data experts who dive right into new client projects and make it their job to understand how a client’s data fits into our Industry Intelligent data models. Utilizing this knowledge and the industry’s newest technologie, they create high performance databases that become the very foundation of the work we do. Critical at all stages of the data science process, Data Engineers work cross-functionally with both external and internal teams – from business analysts to data scientists; web app developers to platform engineers; IT teams to high-level executives. Data Engineers also provide valuable feedback to our software team that helps to shape the development of Aunsight, our proprietary end-to-end cloud analytics platform; and the development of our proprietary web applications. The best Data Engineers are patient, persistent, focused, creative, and incredibly curious. They love to learn and seek out opportunities to identify unexpected solutions or develop alternate ways to solve challenging problems. At the heart of our Daybreak teams are our super talented Data Engineers. Data Engineers are data experts who dive right into new client projects and make it their job to understand how a client’s data fits into our Industry Intelligent data models. Utilizing this knowledge and the industry’s newest technologie, they create high performance databases that become the very foundation of the work we do. Critical at all stages of the data science process, Data Engineers work cross-functionally with both external and internal teams – from business analysts to data scientists; web app developers to platform engineers; IT teams to high-level executives. Data Engineers also provide valuable feedback to our software team that helps to shape the development of Aunsight, our proprietary end-to-end cloud analytics platform; and the development of our proprietary web applications. The best Data Engineers are patient, persistent, focused, creative, and incredibly curious. They love to learn and seek out opportunities to identify unexpected solutions or develop alternate ways to solve challenging problems. Essential Duties & Responsibilities: Essential Duties & Responsibilities: Essential Duties & Responsibilities: Build and own “one source of truth” data sets to facilitate consistency and efficiency in extracting and analyzing data from disparate data sources Ensure data integrity by developing and executing necessary processes and controls around the flow of data Innovate and improve efficiency of managing data to allow for greater speed and accuracy of producing analyses, metrics, and insights Collaborate with internal and external teams to understand business needs/issues, troubleshoot problems, conduct root cause analysis, and develop cost effective resolutions for data anomalies. Provides input into data governance initiatives to enhance current systems, ensure development of efficient application systems, influence the development of data policy, and support overall corporate and business goals Build and own “one source of truth” data sets to facilitate consistency and efficiency in extracting and analyzing data from disparate data sources Ensure data integrity by developing and executing necessary processes and controls around the flow of data Innovate and improve efficiency of managing data to allow for greater speed and accuracy of producing analyses, metrics, and insights Collaborate with internal and external teams to understand business needs/issues, troubleshoot problems, conduct root cause analysis, and develop cost effective resolutions for data anomalies. Provides input into data governance initiatives to enhance current systems, ensure development of efficient application systems, influence the development of data policy, and support overall corporate and business goals Build and own “one source of truth” data sets to facilitate consistency and efficiency in extracting and analyzing data from disparate data sources Ensure data integrity by developing and executing necessary processes and controls around the flow of data Innovate and improve efficiency of managing data to allow for greater speed and accuracy of producing analyses, metrics, and insights Collaborate with internal and external teams to understand business needs/issues, troubleshoot problems, conduct root cause analysis, and develop cost effective resolutions for data anomalies. Provides input into data governance initiatives to enhance current systems, ensure development of efficient application systems, influence the development of data policy, and support overall corporate and business goals Build and own “one source of truth” data sets to facilitate consistency and efficiency in extracting and analyzing data from disparate data sources Build and own “one source of truth” data sets to facilitate consistency and efficiency in extracting and analyzing data from disparate data sources Ensure data integrity by developing and executing necessary processes and controls around the flow of data Ensure data integrity by developing and executing necessary processes and controls around the flow of data Innovate and improve efficiency of managing data to allow for greater speed and accuracy of producing analyses, metrics, and insights Innovate and improve efficiency of managing data to allow for greater speed and accuracy of producing analyses, metrics, and insights Collaborate with internal and external teams to understand business needs/issues, troubleshoot problems, conduct root cause analysis, and develop cost effective resolutions for data anomalies. Collaborate with internal and external teams to understand business needs/issues, troubleshoot problems, conduct root cause analysis, and develop cost effective resolutions for data anomalies. Provides input into data governance initiatives to enhance current systems, ensure development of efficient application systems, influence the development of data policy, and support overall corporate and business goals Provides input into data governance initiatives to enhance current systems, ensure development of efficient application systems, influence the development of data policy, and support overall corporate and business goals Utilize technology to analyze data from applicable systems to review data processes, identify issues, and determine actions to resolve or escalate problems that require data, system, or process improvement Verify accuracy of table changes and data transformation processes. Test changes prior to deployment as appropriate. Recommend and implement enhancements that standardize and streamline processes, assure data quality and reliability, and reduce processing time to meet client expectations Communicate progress and completion to project team. Escalate roadblocks that may impact delivery schedule Stay up-to-date on data engineering and data science trends and developments Utilize technology to analyze data from applicable systems to review data processes, identify issues, and determine actions to resolve or escalate problems that require data, system, or process improvement Verify accuracy of table changes and data transformation processes. Test changes prior to deployment as appropriate. Recommend and implement enhancements that standardize and streamline processes, assure data quality and reliability, and reduce processing time to meet client expectations Communicate progress and completion to project team. Escalate roadblocks that may impact delivery schedule Stay up-to-date on data engineering and data science trends and developments Utilize technology to analyze data from applicable systems to review data processes, identify issues, and determine actions to resolve or escalate problems that require data, system, or process improvement Verify accuracy of table changes and data transformation processes. Test changes prior to deployment as appropriate. Recommend and implement enhancements that standardize and streamline processes, assure data quality and reliability, and reduce processing time to meet client expectations Communicate progress and completion to project team. Escalate roadblocks that may impact delivery schedule Stay up-to-date on data engineering and data science trends and developments Utilize technology to analyze data from applicable systems to review data processes, identify issues, and determine actions to resolve or escalate problems that require data, system, or process improvement Utilize technology to analyze data from applicable systems to review data processes, identify issues, and determine actions to resolve or escalate problems that require data, system, or process improvement Verify accuracy of table changes and data transformation processes. Test changes prior to deployment as appropriate. Verify accuracy of table changes and data transformation processes. Test changes prior to deployment as appropriate. Recommend and implement enhancements that standardize and streamline processes, assure data quality and reliability, and reduce processing time to meet client expectations Recommend and implement enhancements that standardize and streamline processes, assure data quality and reliability, and reduce processing time to meet client expectations Communicate progress and completion to project team. Escalate roadblocks that may impact delivery schedule Communicate progress and completion to project team. Escalate roadblocks that may impact delivery schedule Stay up-to-date on data engineering and data science trends and developments Stay up-to-date on data engineering and data science trends and developments Follow company policy and procedures which protect sensitive data and maintain compliance with established security standards and best practices Additional duties as assigned to ensure client and company success  Required Skills: Bachelor’s degree in Information Science, Computer Science, Computer Engineering, Mathematics, or related field, or 3 plus years of relevant work experience Experience working with relational database structures, SQL and/or flat files and performing table joins, web crawling, and web development Follow company policy and procedures which protect sensitive data and maintain compliance with established security standards and best practices Additional duties as assigned to ensure client and company success Follow company policy and procedures which protect sensitive data and maintain compliance with established security standards and best practices Additional duties as assigned to ensure client and company success Follow company policy and procedures which protect sensitive data and maintain compliance with established security standards and best practices Follow company policy and procedures which protect sensitive data and maintain compliance with established security standards and best practices Additional duties as assigned to ensure client and company success Additional duties as assigned to ensure client and company success Required Skills: Required Skills: Required Skills: Bachelor’s degree in Information Science, Computer Science, Computer Engineering, Mathematics, or related field, or 3 plus years of relevant work experience Experience working with relational database structures, SQL and/or flat files and performing table joins, web crawling, and web development Bachelor’s degree in Information Science, Computer Science, Computer Engineering, Mathematics, or related field, or 3 plus years of relevant work experience Experience working with relational database structures, SQL and/or flat files and performing table joins, web crawling, and web development Bachelor’s degree in Information Science, Computer Science, Computer Engineering, Mathematics, or related field, or 3 plus years of relevant work experience Bachelor’s degree in Information Science, Computer Science, Computer Engineering, Mathematics, or related field, or 3 plus years of relevant work experience Experience working with relational database structures, SQL and/or flat files and performing table joins, web crawling, and web development Experience working with relational database structures, SQL and/or flat files and performing table joins, web crawling, and web development Proficiency in one or more of the following programming languages: Java, C#, C++, or Python and a familiarity with Node.js Experience working with commercial relational database systems such as electronic medical records or other clinical systems, client relationship management software, or accounting systems a plus Familiar with various data management methodologies, data exploration techniques, data quality assurance practices, and data discovery/ visualization tools Natural curiosity about what’s hidden in the data through exploration, attention to detail, and ability to see the big picture – similar to putting together a 10,000-piece puzzle Ability to communicate your ideas (verbal and written) so that team members and clients can understand them Proficiency in one or more of the following programming languages: Java, C#, C++, or Python and a familiarity with Node.js Experience working with commercial relational database systems such as electronic medical records or other clinical systems, client relationship management software, or accounting systems a plus Familiar with various data management methodologies, data exploration techniques, data quality assurance practices, and data discovery/ visualization tools Natural curiosity about what’s hidden in the data through exploration, attention to detail, and ability to see the big picture – similar to putting together a 10,000-piece puzzle Ability to communicate your ideas (verbal and written) so that team members and clients can understand them Proficiency in one or more of the following programming languages: Java, C#, C++, or Python and a familiarity with Node.js Experience working with commercial relational database systems such as electronic medical records or other clinical systems, client relationship management software, or accounting systems a plus Familiar with various data management methodologies, data exploration techniques, data quality assurance practices, and data discovery/ visualization tools Natural curiosity about what’s hidden in the data through exploration, attention to detail, and ability to see the big picture – similar to putting together a 10,000-piece puzzle Ability to communicate your ideas (verbal and written) so that team members and clients can understand them Proficiency in one or more of the following programming languages: Java, C#, C++, or Python and a familiarity with Node.js Proficiency in one or more of the following programming languages: Java, C#, C++, or Python and a familiarity with Node.js Experience working with commercial relational database systems such as electronic medical records or other clinical systems, client relationship management software, or accounting systems a plus Experience working with commercial relational database systems such as electronic medical records or other clinical systems, client relationship management software, or accounting systems a plus Familiar with various data management methodologies, data exploration techniques, data quality assurance practices, and data discovery/ visualization tools Familiar with various data management methodologies, data exploration techniques, data quality assurance practices, and data discovery/ visualization tools Natural curiosity about what’s hidden in the data through exploration, attention to detail, and ability to see the big picture – similar to putting together a 10,000-piece puzzle Natural curiosity about what’s hidden in the data through exploration, attention to detail, and ability to see the big picture – similar to putting together a 10,000-piece puzzle Ability to communicate your ideas (verbal and written) so that team members and clients can understand them Ability to communicate your ideas (verbal and written) so that team members and clients can understand them Share our values: growth, relationships, integrity, and true grit  What's in it for You? Opportunity to work with a rapidly expanding tech company in the booming field of data analytics alongside some of the brightest minds in the industry Opportunity to work with cutting-edge technology in a casual, fun environment Opportunity to be a part of a local company committed to making a difference in our community Share our values: growth, relationships, integrity, and true grit Share our values: growth, relationships, integrity, and true grit Share our values: growth, relationships, integrity, and true grit Share our values: growth, relationships, integrity, and true grit What's in it for You? What's in it for You? What's in it for You? Opportunity to work with a rapidly expanding tech company in the booming field of data analytics alongside some of the brightest minds in the industry Opportunity to work with cutting-edge technology in a casual, fun environment Opportunity to be a part of a local company committed to making a difference in our community Opportunity to work with a rapidly expanding tech company in the booming field of data analytics alongside some of the brightest minds in the industry Opportunity to work with cutting-edge technology in a casual, fun environment Opportunity to be a part of a local company committed to making a difference in our community Opportunity to work with a rapidly expanding tech company in the booming field of data analytics alongside some of the brightest minds in the industry Opportunity to work with a rapidly expanding tech company in the booming field of data analytics alongside some of the brightest minds in the industry Opportunity to work with cutting-edge technology in a casual, fun environment Opportunity to work with cutting-edge technology in a casual, fun environment Opportunity to be a part of a local company committed to making a difference in our community Opportunity to be a part of a local company committed to making a difference in our community Flexible schedule and paid time off Social events such as happy hours, game nights, holiday parties, birthday celebrations, movie days, ice cream sundae bars, fancy coffee carts, company softball team, etc. Competitive salary and benefits package including health, vision, dental and life insurance and 401(k) plan. Flexible schedule and paid time off Social events such as happy hours, game nights, holiday parties, birthday celebrations, movie days, ice cream sundae bars, fancy coffee carts, company softball team, etc. Competitive salary and benefits package including health, vision, dental and life insurance and 401(k) plan. Flexible schedule and paid time off Social events such as happy hours, game nights, holiday parties, birthday celebrations, movie days, ice cream sundae bars, fancy coffee carts, company softball team, etc. Competitive salary and benefits package including health, vision, dental and life insurance and 401(k) plan. Flexible schedule and paid time off Flexible schedule and paid time off Social events such as happy hours, game nights, holiday parties, birthday celebrations, movie days, ice cream sundae bars, fancy coffee carts, company softball team, etc. Social events such as happy hours, game nights, holiday parties, birthday celebrations, movie days, ice cream sundae bars, fancy coffee carts, company softball team, etc. Competitive salary and benefits package including health, vision, dental and life insurance and 401(k) plan. Competitive salary and benefits package including health, vision, dental and life insurance and 401(k) plan.",4.2,1 to 50 Employees,N/A,Company - Private,N/A,N/A,Unknown / Non-Applicable
"San Jose, CA",Data Engineer,$107K - $161K (Glassdoor est.),Kodeva,"Location: San Jose CA Duration: 12 Months Job Description : Profile should have more than 8years of experience Hands on experience in designing and executing projects on Google Cloud Platform features like App Engine, Compute, storage, Big Query, Data Proc, Data Flow. Strong Programming Skills in R, Python or Spark. Strong Knowledge on Data Engineering, Simulation and Modelling concepts Proficiency in handling the billions of structured or unstructured transactional data. Proficiency in modeling techniques such linear regression, logistic regression, GLM Knowledge on machine learning techniques such as Decision Trees, xgboost, random forest, PCA etc. Knowledge on unsupervised Machine learning techniques such as Clustering, Segmentation Strong knowledge on Data Manipulation and transformation Knowledge on data loading to GCP services like big query, cloud storage. Knowledge in Hadoop, HIVE and Pig languages. Good communication skills. Location: San Jose CA Duration: 12 Months Job Description : Profile should have more than 8years of experience Hands on experience in designing and executing projects on Google Cloud Platform features like App Engine, Compute, storage, Big Query, Data Proc, Data Flow. Strong Programming Skills in R, Python or Spark. Strong Knowledge on Data Engineering, Simulation and Modelling concepts Proficiency in handling the billions of structured or unstructured transactional data. Proficiency in modeling techniques such linear regression, logistic regression, GLM Knowledge on machine learning techniques such as Decision Trees, xgboost, random forest, PCA etc. Knowledge on unsupervised Machine learning techniques such as Clustering, Segmentation Strong knowledge on Data Manipulation and transformation Knowledge on data loading to GCP services like big query, cloud storage. Knowledge in Hadoop, HIVE and Pig languages. Good communication skills. Location: San Jose CA Location: Duration: 12 Months Duration: Job Description : Job Description : Profile should have more than 8years of experience Hands on experience in designing and executing projects on Google Cloud Platform features like App Engine, Compute, storage, Big Query, Data Proc, Data Flow. Strong Programming Skills in R, Python or Spark. Strong Knowledge on Data Engineering, Simulation and Modelling concepts Proficiency in handling the billions of structured or unstructured transactional data. Proficiency in modeling techniques such linear regression, logistic regression, GLM Knowledge on machine learning techniques such as Decision Trees, xgboost, random forest, PCA etc. Knowledge on unsupervised Machine learning techniques such as Clustering, Segmentation Strong knowledge on Data Manipulation and transformation Knowledge on data loading to GCP services like big query, cloud storage. Knowledge in Hadoop, HIVE and Pig languages. Good communication skills. Profile should have more than 8years of experience Hands on experience in designing and executing projects on Google Cloud Platform features like App Engine, Compute, storage, Big Query, Data Proc, Data Flow. Strong Programming Skills in R, Python or Spark. Strong Knowledge on Data Engineering, Simulation and Modelling concepts Proficiency in handling the billions of structured or unstructured transactional data. Proficiency in modeling techniques such linear regression, logistic regression, GLM Knowledge on machine learning techniques such as Decision Trees, xgboost, random forest, PCA etc. Knowledge on unsupervised Machine learning techniques such as Clustering, Segmentation Strong knowledge on Data Manipulation and transformation Knowledge on data loading to GCP services like big query, cloud storage. Knowledge in Hadoop, HIVE and Pig languages. Good communication skills.",N/A,51 to 200 Employees,N/A,Company - Private,N/A,N/A,Unknown / Non-Applicable
"Baltimore, MD",Data Engineer,$70K - $96K (Glassdoor est.),Baltimore Orioles,"Summary: The Baltimore Orioles are one of Major League Baseball's iconic franchises with a deeply loyal and passionate fan base. We have a long history of success including three World Series Championships in 1966, 1970, and 1983, and have been putting the pieces in place over the past few years to have another long run of success on the field as evidenced by this season's post-season push. We play our home games at Oriole Park at Camden Yards, which opened in 1992, and is consistently ranked as one of the top ballparks in Major League Baseball. It has also been the site for many historic moments such as Hall of Famer Cal Ripken, Jr. breaking the all-time record for consecutive games played, Hall of Famer Eddie Murray hitting his 500th home run, and hosting the 1993 MLB All-Star Game to name a few. The Baltimore Orioles are looking for a Data Engineer to develop and maintain the Business Analytics data warehouse. This includes managing connections to existing data sources, integrating new data sources, modeling data, and developing logic to ensure clean and consistent data. Responsibilities: Design, develop, deploy, and manage data platform architecture and integration strategies. Build and maintain ETL/ELT processes across multiple business data sources that ensure data standardization and integrity. Implement good data hygiene practices and manage issue tracking within data pipelines. Oversee all aspects of data management operations, including defining and resolving database problems, query optimization, and managing storage. Combine, cleanse, and enhance raw data from disparate sources into analytics ready data models. Develop relationships with and gather requirements from non-technical business stakeholders. Partner closely with Business Analytics team to understand data requirements for current and future projects. Qualifications: Bachelor's degree in Computer Science, Data Engineering, or a related field. 3+ years' experience in data engineering or data operations roles managing production data pipelines and modeling. Advanced working experience with SQL and Python languages. Proven experience with data in various forms (data warehouses/relational SQL, NoSQL, JSON, unstructured data environments/PIG, HIVE, Impala) Development and execution of data structures and pipelines to organize, collect and standardize data to generate insights and addresses reporting needs Experience creating data pipelines for sales systems like Salesforce Experience with Google Cloud BigQuery is a plus Experience with Tickets.com ticketing data is a plus End-to-end SDLC experience a plus Motivated team player with a positive attitude and strong work ethic DISCLAIMER: The statements herein are intended to describe the general nature and level of work being performed by the employee in this position. The duties listed do not represent an exhaustive list of all responsibilities, duties, and skills required of a person in this position. EQUAL OPPORTUNITY STATEMENT: The Baltimore Orioles are an Equal Opportunity Employer. It is the policy of the Baltimore Orioles to ensure equal employment opportunity without discrimination or harassment on the basis of race (including hair textures, afro hairstyles, or protective hairstyles), color, national origin or ancestry, religion or creed, gender or sex (including pregnancy), age, disability, citizenship status, marital status, veteran's status, genetic predisposition or carrier status, gender identity, sexual orientation, or any other characteristic protected by law. Summary: Summary The Baltimore Orioles are one of Major League Baseball's iconic franchises with a deeply loyal and passionate fan base. We have a long history of success including three World Series Championships in 1966, 1970, and 1983, and have been putting the pieces in place over the past few years to have another long run of success on the field as evidenced by this season's post-season push. We play our home games at Oriole Park at Camden Yards, which opened in 1992, and is consistently ranked as one of the top ballparks in Major League Baseball. It has also been the site for many historic moments such as Hall of Famer Cal Ripken, Jr. breaking the all-time record for consecutive games played, Hall of Famer Eddie Murray hitting his 500th home run, and hosting the 1993 MLB All-Star Game to name a few. The Baltimore Orioles are looking for a Data Engineer to develop and maintain the Business Analytics data warehouse. This includes managing connections to existing data sources, integrating new data sources, modeling data, and developing logic to ensure clean and consistent data. Responsibilities: Responsibilities Design, develop, deploy, and manage data platform architecture and integration strategies. Build and maintain ETL/ELT processes across multiple business data sources that ensure data standardization and integrity. Implement good data hygiene practices and manage issue tracking within data pipelines. Oversee all aspects of data management operations, including defining and resolving database problems, query optimization, and managing storage. Combine, cleanse, and enhance raw data from disparate sources into analytics ready data models. Develop relationships with and gather requirements from non-technical business stakeholders. Partner closely with Business Analytics team to understand data requirements for current and future projects. Design, develop, deploy, and manage data platform architecture and integration strategies. Build and maintain ETL/ELT processes across multiple business data sources that ensure data standardization and integrity. Implement good data hygiene practices and manage issue tracking within data pipelines. Oversee all aspects of data management operations, including defining and resolving database problems, query optimization, and managing storage. Combine, cleanse, and enhance raw data from disparate sources into analytics ready data models. Develop relationships with and gather requirements from non-technical business stakeholders. Partner closely with Business Analytics team to understand data requirements for current and future projects. Qualifications: Qualifications Bachelor's degree in Computer Science, Data Engineering, or a related field. 3+ years' experience in data engineering or data operations roles managing production data pipelines and modeling. Advanced working experience with SQL and Python languages. Proven experience with data in various forms (data warehouses/relational SQL, NoSQL, JSON, unstructured data environments/PIG, HIVE, Impala) Development and execution of data structures and pipelines to organize, collect and standardize data to generate insights and addresses reporting needs Experience creating data pipelines for sales systems like Salesforce Experience with Google Cloud BigQuery is a plus Experience with Tickets.com ticketing data is a plus End-to-end SDLC experience a plus Motivated team player with a positive attitude and strong work ethic Bachelor's degree in Computer Science, Data Engineering, or a related field. 3+ years' experience in data engineering or data operations roles managing production data pipelines and modeling. Advanced working experience with SQL and Python languages. Proven experience with data in various forms (data warehouses/relational SQL, NoSQL, JSON, unstructured data environments/PIG, HIVE, Impala) Development and execution of data structures and pipelines to organize, collect and standardize data to generate insights and addresses reporting needs Experience creating data pipelines for sales systems like Salesforce Experience with Google Cloud BigQuery is a plus Experience with Tickets.com ticketing data is a plus End-to-end SDLC experience a plus Motivated team player with a positive attitude and strong work ethic DISCLAIMER: The statements herein are intended to describe the general nature and level of work being performed by the employee in this position. The duties listed do not represent an exhaustive list of all responsibilities, duties, and skills required of a person in this position. EQUAL OPPORTUNITY STATEMENT: The Baltimore Orioles are an Equal Opportunity Employer. It is the policy of the Baltimore Orioles to ensure equal employment opportunity without discrimination or harassment on the basis of race (including hair textures, afro hairstyles, or protective hairstyles), color, national origin or ancestry, religion or creed, gender or sex (including pregnancy), age, disability, citizenship status, marital status, veteran's status, genetic predisposition or carrier status, gender identity, sexual orientation, or any other characteristic protected by law. DISCLAIMER: DISCLAIMER: The statements herein are intended to describe the general nature and level of work being performed by the employee in this position. The duties listed do not represent an exhaustive list of all responsibilities, duties, and skills required of a person in this position. EQUAL OPPORTUNITY STATEMENT: EQUAL OPPORTUNITY STATEMENT: The Baltimore Orioles are an Equal Opportunity Employer. It is the policy of the Baltimore Orioles to ensure equal employment opportunity without discrimination or harassment on the basis of race (including hair textures, afro hairstyles, or protective hairstyles), color, national origin or ancestry, religion or creed, gender or sex (including pregnancy), age, disability, citizenship status, marital status, veteran's status, genetic predisposition or carrier status, gender identity, sexual orientation, or any other characteristic protected by law.",4.1,201 to 500 Employees,1993,Company - Private,Sports & Recreation,"Arts, Entertainment & Recreation",$100 to $500 million (USD)
"Palo Alto, CA",Associate Data Engineer,$46.36 - $60.27 Per Hour (Employer est.),Stanford Health Care,"If you're ready to be part of our legacy of hope and innovation, we encourage you to take the first step and explore our current job openings. Your best is waiting to be discovered.  Day - 08 Hour (United States of America) This is a Stanford Health Care job.  A Brief Overview The Associate Data Architect is a Level I Analyst role responsible for providing analytics supporting improvement efforts, generally within a defined value stream or strategic domain. This includes developing data architecture and solutions to sustain improvement efforts and provide ongoing support for existing applications.  Locations Stanford Health Care  What you will do Builds effective working relationships with cross discipline team members, including hospital staff, new users, line management, on/offshore team members, etc. Guides user leadership in formulation of project plan, regular status reporting to IT and user management, issue management and escalation. Develops business cases for new technology solutions. Confidently and professionally presents and defends recommended solution, approach, timeline, etc. to IT and user leadership and manages expectations accordingly. Organizes and conducts regular project status meetings and design reviews sessions leveraging appropriate project artifacts. With little supervision, performs analysis of the scope and requirements for projects. Prepares specifications, designs, data models and diagrams from which databases can be developed. Develops, tests, and deploys data structures using Entity Relationship Diagramming, SQL Server tools, and data modeling tools. Troubleshoots incidents surrounding supported databases and solutions. Tunes performance of databases, ETL processes and queries.  Education Qualifications BS/BA Degree in information technology, information systems, business management, business analytics, business administration or a directly-related field from an accredited college or university. Required  Experience Qualifications Zero (0) to Two (2) years of experience in analytics, business intelligence or healthcare technology Required  Required Knowledge, Skills and Abilities Understanding of components of high-quality Reporting & Analytics solutions that meet user requirements with minimal on-going maintenance and a low volume of production incidents (production failures, help desk calls, etc.). Understanding of best practices and common processes for developing solutions with SHC tools (i.e., SSIS, Crystal Reports, WebI, Qlikview, etc.) utilized in role. Troubleshoots incidents and enhancement requests surrounding supported applications. Basic working knowledge of SQL in an Oracle or SQL Server environment. Proficient with Select queries with inner/outer joins and common text and numeric functions. Creates moderately complex Reports or Visualizations with SHC standard tools. Effectively implements these, scheduling, and user admin. Effectively takes direction from supervisors to complete assigned tasks. Reactive interaction up to Tier 4 levels of the organization Demonstrates ability to manage assigned tasks on basic projects. Seeks and embraces coaching and mentoring from team members in order to develop skills and integrate with the team. Understands basic tenants of SHC vision and communicates them to others. Developing expertise in a single domain. Limited ability to anticipate problems. Effective verbal, written, and interpersonal communication skills  Licenses and Certifications None .  These principles apply to ALL employees:  SHC Commitment to Providing an Exceptional Patient & Family Experience  Stanford Health Care sets a high standard for delivering value and an exceptional experience for our patients and families. Candidates for employment and existing employees must adopt and execute C-I-CARE standards for all of patients, families and towards each other. C-I-CARE is the foundation of Stanford’s patient-experience and represents a framework for patient-centered interactions. Simply put, we do what it takes to enable and empower patients and families to focus on health, healing and recovery.  You will do this by executing against our three experience pillars, from the patient and family’s perspective: Know Me: Anticipate my needs and status to deliver effective care Show Me the Way: Guide and prompt my actions to arrive at better outcomes and better health Coordinate for Me: Own the complexity of my care through coordination Equal Opportunity Employer Stanford Health Care (SHC) strongly values diversity and is committed to equal opportunity and non-discrimination in all of its policies and practices, including the area of employment. Accordingly, SHC does not discriminate against any person on the basis of race, color, sex, sexual orientation or gender identity and/or expression, religion, age, national or ethnic origin, political beliefs, marital status, medical condition, genetic information, veteran status, or disability, or the perception of any of the above. People of all genders, members of all racial and ethnic groups, people with disabilities, and veterans are encouraged to apply. Qualified applicants with criminal convictions will be considered after an individualized assessment of the conviction and the job requirements. Base Pay Scale: Generally starting at $46.36 - $60.27 per hour The salary of the finalist selected for this role will be set based on a variety of factors, including but not limited to, internal equity, experience, education, specialty and training. This pay scale is not a promise of a particular wage. If you're ready to be part of our legacy of hope and innovation, we encourage you to take the first step and explore our current job openings. Your best is waiting to be discovered. This is a Stanford Health Care job.  A Brief Overview The Associate Data Architect is a Level I Analyst role responsible for providing analytics supporting improvement efforts, generally within a defined value stream or strategic domain. This includes developing data architecture and solutions to sustain improvement efforts and provide ongoing support for existing applications.  Locations Stanford Health Care  What you will do This is a Stanford Health Care job. A Brief Overview Locations What you will do Builds effective working relationships with cross discipline team members, including hospital staff, new users, line management, on/offshore team members, etc. Guides user leadership in formulation of project plan, regular status reporting to IT and user management, issue management and escalation. Develops business cases for new technology solutions. Confidently and professionally presents and defends recommended solution, approach, timeline, etc. to IT and user leadership and manages expectations accordingly. Organizes and conducts regular project status meetings and design reviews sessions leveraging appropriate project artifacts. With little supervision, performs analysis of the scope and requirements for projects. Prepares specifications, designs, data models and diagrams from which databases can be developed. Develops, tests, and deploys data structures using Entity Relationship Diagramming, SQL Server tools, and data modeling tools. Troubleshoots incidents surrounding supported databases and solutions. Tunes performance of databases, ETL processes and queries. Builds effective working relationships with cross discipline team members, including hospital staff, new users, line management, on/offshore team members, etc. Guides user leadership in formulation of project plan, regular status reporting to IT and user management, issue management and escalation. Develops business cases for new technology solutions. Confidently and professionally presents and defends recommended solution, approach, timeline, etc. to IT and user leadership and manages expectations accordingly. Organizes and conducts regular project status meetings and design reviews sessions leveraging appropriate project artifacts. With little supervision, performs analysis of the scope and requirements for projects. Prepares specifications, designs, data models and diagrams from which databases can be developed. Develops, tests, and deploys data structures using Entity Relationship Diagramming, SQL Server tools, and data modeling tools. Troubleshoots incidents surrounding supported databases and solutions. Tunes performance of databases, ETL processes and queries. Education Qualifications Education Qualifications BS/BA Degree in information technology, information systems, business management, business analytics, business administration or a directly-related field from an accredited college or university. Required BS/BA Degree in information technology, information systems, business management, business analytics, business administration or a directly-related field from an accredited college or university. Required Experience Qualifications Experience Qualifications Zero (0) to Two (2) years of experience in analytics, business intelligence or healthcare technology Required Zero (0) to Two (2) years of experience in analytics, business intelligence or healthcare technology Required Required Knowledge, Skills and Abilities Required Knowledge, Skills and Abilities Understanding of components of high-quality Reporting & Analytics solutions that meet user requirements with minimal on-going maintenance and a low volume of production incidents (production failures, help desk calls, etc.). Understanding of best practices and common processes for developing solutions with SHC tools (i.e., SSIS, Crystal Reports, WebI, Qlikview, etc.) utilized in role. Troubleshoots incidents and enhancement requests surrounding supported applications. Basic working knowledge of SQL in an Oracle or SQL Server environment. Proficient with Select queries with inner/outer joins and common text and numeric functions. Creates moderately complex Reports or Visualizations with SHC standard tools. Effectively implements these, scheduling, and user admin. Effectively takes direction from supervisors to complete assigned tasks. Reactive interaction up to Tier 4 levels of the organization Demonstrates ability to manage assigned tasks on basic projects. Seeks and embraces coaching and mentoring from team members in order to develop skills and integrate with the team. Understands basic tenants of SHC vision and communicates them to others. Developing expertise in a single domain. Limited ability to anticipate problems. Effective verbal, written, and interpersonal communication skills Understanding of components of high-quality Reporting & Analytics solutions that meet user requirements with minimal on-going maintenance and a low volume of production incidents (production failures, help desk calls, etc.). Understanding of best practices and common processes for developing solutions with SHC tools (i.e., SSIS, Crystal Reports, WebI, Qlikview, etc.) utilized in role. Troubleshoots incidents and enhancement requests surrounding supported applications. Basic working knowledge of SQL in an Oracle or SQL Server environment. Proficient with Select queries with inner/outer joins and common text and numeric functions. Creates moderately complex Reports or Visualizations with SHC standard tools. Effectively implements these, scheduling, and user admin. Effectively takes direction from supervisors to complete assigned tasks. Reactive interaction up to Tier 4 levels of the organization Demonstrates ability to manage assigned tasks on basic projects. Seeks and embraces coaching and mentoring from team members in order to develop skills and integrate with the team. Understands basic tenants of SHC vision and communicates them to others. Developing expertise in a single domain. Limited ability to anticipate problems. Effective verbal, written, and interpersonal communication skills Licenses and Certifications Licenses and Certifications None . None . These principles apply to ALL employees:  SHC Commitment to Providing an Exceptional Patient & Family Experience  Stanford Health Care sets a high standard for delivering value and an exceptional experience for our patients and families. Candidates for employment and existing employees must adopt and execute C-I-CARE standards for all of patients, families and towards each other. C-I-CARE is the foundation of Stanford’s patient-experience and represents a framework for patient-centered interactions. Simply put, we do what it takes to enable and empower patients and families to focus on health, healing and recovery.  You will do this by executing against our three experience pillars, from the patient and family’s perspective: These principles apply to ALL employees: SHC Commitment to Providing an Exceptional Patient & Family Experience Stanford Health Care sets a high standard for delivering value and an exceptional experience for our patients and families. Candidates for employment and existing employees must adopt and execute C-I-CARE standards for all of patients, families and towards each other. C-I-CARE is the foundation of Stanford’s patient-experience and represents a framework for patient-centered interactions. Simply put, we do what it takes to enable and empower patients and families to focus on health, healing and recovery. You will do this by executing against our three experience pillars, from the patient and family’s perspective: Know Me: Anticipate my needs and status to deliver effective care Show Me the Way: Guide and prompt my actions to arrive at better outcomes and better health Coordinate for Me: Own the complexity of my care through coordination Know Me: Anticipate my needs and status to deliver effective care Show Me the Way: Guide and prompt my actions to arrive at better outcomes and better health Coordinate for Me: Own the complexity of my care through coordination Equal Opportunity Employer Stanford Health Care (SHC) strongly values diversity and is committed to equal opportunity and non-discrimination in all of its policies and practices, including the area of employment. Accordingly, SHC does not discriminate against any person on the basis of race, color, sex, sexual orientation or gender identity and/or expression, religion, age, national or ethnic origin, political beliefs, marital status, medical condition, genetic information, veteran status, or disability, or the perception of any of the above. People of all genders, members of all racial and ethnic groups, people with disabilities, and veterans are encouraged to apply. Qualified applicants with criminal convictions will be considered after an individualized assessment of the conviction and the job requirements. Equal Opportunity Employer Stanford Health Care (SHC) strongly values diversity and is committed to equal opportunity and non-discrimination in all of its policies and practices, including the area of employment. Accordingly, SHC does not discriminate against any person on the basis of race, color, sex, sexual orientation or gender identity and/or expression, religion, age, national or ethnic origin, political beliefs, marital status, medical condition, genetic information, veteran status, or disability, or the perception of any of the above. People of all genders, members of all racial and ethnic groups, people with disabilities, and veterans are encouraged to apply. Qualified applicants with criminal convictions will be considered after an individualized assessment of the conviction and the job requirements. The salary of the finalist selected for this role will be set based on a variety of factors, including but not limited to, internal equity, experience, education, specialty and training. This pay scale is not a promise of a particular wage.",3.9,10000+ Employees,1957,Hospital,Health Care Services & Hospitals,Healthcare,$1 to $5 billion (USD)
Remote,Data Engineer,$95K - $115K (Employer est.),Everside Health,"ABOUT THE JOB Data is one of the foundations of Everside Health and plays an integral role in delivering first-class healthcare services to our patient population. We utilize a wide variety of tools to produce valuable data and provide better patient care as a result. As a Data Engineer, you will collaborate with product owners, senior data engineers, data architects and partnerships with expert resources to design, develop, and implement data assets for a wide range of new initiatives at Everside Health. The role involves heavy data exploration, proficiency with SQL, ETL, knowledge of service-based deployments and APIs, and the ability to discover and learn quickly through collaboration. There is a need to think analytically and outside of the box while questioning current processes and continuing to build your business acumen. There will be a combination of team collaboration and independent work efforts. This role involves interaction with the Analytics team as well as a wide range of business areas across Everside. We seek candidates with a strong quantitative background and excellent analytical and problem-solving skills. This position combines business and technical skills involving interaction with business customers, Analytics partners, internal and external data suppliers, and information technology partners. ESSENTIAL DUTIES & RESPONSIBILITIES Deliver data warehouse and analytic solutions by ingesting, integrating, and curating data to deliver information to the business and stakeholders Conduct ETL design, development, and maintenance including data extraction, manipulation, analysis, source-target mapping, change data capture, code performance Ensure seamless integration of data across the enterprise and drive automation of common and repeated tasks Conduct performance tuning and optimization of all processes executed across the data platform Develop large scale data structures and pipelines to organize, collect, and standardize data that helps generate insights and addresses reporting needs Collaborate closely with business users and stakeholders to define data for the design, development, and deployment of new solutions that support strategic business priorities Develop comprehensive documentation and facilitate knowledge transfer to Production Support team Work with Production Support to analyze and fix the production issues QUALIFICATIONS Bachelor’s degree in Computer Science or related field 3 + years of Data Engineering work experience or equivalent combination of education and experience Experience designing and implementing ETL pipelines, working with a variety of data warehousing models and design fundamentals and experience writing optimized code for the business scenario Experience working with Matillion, Azure Data Factory, Data Bricks, or similar ETL tool Experience working in Snowflake, Synapse, or similar MPP platform. Experience in DataOps/DevOps and agile methodologies Experience with messaging streaming systems (e.g., Kafka, Azure Event Hubs) Experience in hybrid data processing methods (batch and streaming) Experience with AWS or Azure application deployment Experience with API integration Pay Range: $95,000 - $115,000/yr The actual offer may vary dependent upon geographic location and the candidate’s years of experience and/or skill level. Everside Benefits Summary We believe in empowering teammates to do their best work and build better healthcare. Below are some of our benefit offerings. Eligibility is based on 24/hr week. Health and Well-Being: Free Everside membership for in person and virtual care, employer paid life and disability insurance, and choice in medical/dental plans, vision, employer funded HSA, FSA, and voluntary illness, accident and hospitalization plans. Benefits are effective on the first of the month following date of hire. Financial Support: Competitive compensation, 401k match, access to financial coaching through our Employee Assistance Program Lifestyle: Paid time off for vacation, sick leave, and more, holiday schedule Learn more at https://www.eversidehealth.com/careers/ ABOUT THE JOB Data is one of the foundations of Everside Health and plays an integral role in delivering first-class healthcare services to our patient population. We utilize a wide variety of tools to produce valuable data and provide better patient care as a result. As a Data Engineer, you will collaborate with product owners, senior data engineers, data architects and partnerships with expert resources to design, develop, and implement data assets for a wide range of new initiatives at Everside Health. The role involves heavy data exploration, proficiency with SQL, ETL, knowledge of service-based deployments and APIs, and the ability to discover and learn quickly through collaboration. There is a need to think analytically and outside of the box while questioning current processes and continuing to build your business acumen. There will be a combination of team collaboration and independent work efforts. This role involves interaction with the Analytics team as well as a wide range of business areas across Everside. We seek candidates with a strong quantitative background and excellent analytical and problem-solving skills. This position combines business and technical skills involving interaction with business customers, Analytics partners, internal and external data suppliers, and information technology partners. ESSENTIAL DUTIES & RESPONSIBILITIES Deliver data warehouse and analytic solutions by ingesting, integrating, and curating data to deliver information to the business and stakeholders Conduct ETL design, development, and maintenance including data extraction, manipulation, analysis, source-target mapping, change data capture, code performance Ensure seamless integration of data across the enterprise and drive automation of common and repeated tasks Conduct performance tuning and optimization of all processes executed across the data platform Develop large scale data structures and pipelines to organize, collect, and standardize data that helps generate insights and addresses reporting needs Collaborate closely with business users and stakeholders to define data for the design, development, and deployment of new solutions that support strategic business priorities Develop comprehensive documentation and facilitate knowledge transfer to Production Support team Work with Production Support to analyze and fix the production issues Deliver data warehouse and analytic solutions by ingesting, integrating, and curating data to deliver information to the business and stakeholders Deliver data warehouse and analytic solutions by ingesting, integrating, and curating data to deliver information to the business and stakeholders Conduct ETL design, development, and maintenance including data extraction, manipulation, analysis, source-target mapping, change data capture, code performance Conduct ETL design, development, and maintenance including data extraction, manipulation, analysis, source-target mapping, change data capture, code performance Ensure seamless integration of data across the enterprise and drive automation of common and repeated tasks Ensure seamless integration of data across the enterprise and drive automation of common and repeated tasks Conduct performance tuning and optimization of all processes executed across the data platform Conduct performance tuning and optimization of all processes executed across the data platform Develop large scale data structures and pipelines to organize, collect, and standardize data that helps generate insights and addresses reporting needs Develop large scale data structures and pipelines to organize, collect, and standardize data that helps generate insights and addresses reporting needs Collaborate closely with business users and stakeholders to define data for the design, development, and deployment of new solutions that support strategic business priorities Collaborate closely with business users and stakeholders to define data for the design, development, and deployment of new solutions that support strategic business priorities Develop comprehensive documentation and facilitate knowledge transfer to Production Support team Develop comprehensive documentation and facilitate knowledge transfer to Production Support team Work with Production Support to analyze and fix the production issues Work with Production Support to analyze and fix the production issues QUALIFICATIONS Bachelor’s degree in Computer Science or related field 3 + years of Data Engineering work experience or equivalent combination of education and experience Experience designing and implementing ETL pipelines, working with a variety of data warehousing models and design fundamentals and experience writing optimized code for the business scenario Experience working with Matillion, Azure Data Factory, Data Bricks, or similar ETL tool Experience working in Snowflake, Synapse, or similar MPP platform. Experience in DataOps/DevOps and agile methodologies Experience with messaging streaming systems (e.g., Kafka, Azure Event Hubs) Experience in hybrid data processing methods (batch and streaming) Experience with AWS or Azure application deployment Experience with API integration Bachelor’s degree in Computer Science or related field Bachelor’s degree in Computer Science or related field 3 + years of Data Engineering work experience or equivalent combination of education and experience 3 + years of Data Engineering work experience or equivalent combination of education and experience Experience designing and implementing ETL pipelines, working with a variety of data warehousing models and design fundamentals and experience writing optimized code for the business scenario Experience designing and implementing ETL pipelines, working with a variety of data warehousing models and design fundamentals and experience writing optimized code for the business scenario Experience working with Matillion, Azure Data Factory, Data Bricks, or similar ETL tool Experience working with Matillion, Azure Data Factory, Data Bricks, or similar ETL tool Experience working in Snowflake, Synapse, or similar MPP platform. Experience working in Snowflake, Synapse, or similar MPP platform. Experience in DataOps/DevOps and agile methodologies Experience in DataOps/DevOps and agile methodologies Experience with messaging streaming systems (e.g., Kafka, Azure Event Hubs) Experience with messaging streaming systems (e.g., Kafka, Azure Event Hubs) Experience in hybrid data processing methods (batch and streaming) Experience in hybrid data processing methods (batch and streaming) Experience with AWS or Azure application deployment Experience with AWS or Azure application deployment Experience with API integration Experience with API integration Pay Range: $95,000 - $115,000/yr The actual offer may vary dependent upon geographic location and the candidate’s years of experience and/or skill level. The actual offer may vary dependent upon geographic location and the candidate’s years of experience and/or skill level. Everside Benefits Summary We believe in empowering teammates to do their best work and build better healthcare. Below are some of our benefit offerings. Eligibility is based on 24/hr week. Health and Well-Being: Free Everside membership for in person and virtual care, employer paid life and disability insurance, and choice in medical/dental plans, vision, employer funded HSA, FSA, and voluntary illness, accident and hospitalization plans. Benefits are effective on the first of the month following date of hire. Financial Support: Competitive compensation, 401k match, access to financial coaching through our Employee Assistance Program Lifestyle: Paid time off for vacation, sick leave, and more, holiday schedule Health and Well-Being: Free Everside membership for in person and virtual care, employer paid life and disability insurance, and choice in medical/dental plans, vision, employer funded HSA, FSA, and voluntary illness, accident and hospitalization plans. Benefits are effective on the first of the month following date of hire. Health and Well-Being: Free Everside membership for in person and virtual care, employer paid life and disability insurance, and choice in medical/dental plans, vision, employer funded HSA, FSA, and voluntary illness, accident and hospitalization plans. Benefits are effective on the first of the month following date of hire. Financial Support: Competitive compensation, 401k match, access to financial coaching through our Employee Assistance Program Financial Support: Competitive compensation, 401k match, access to financial coaching through our Employee Assistance Program Lifestyle: Paid time off for vacation, sick leave, and more, holiday schedule Lifestyle: Paid time off for vacation, sick leave, and more, holiday schedule Learn more at https://www.eversidehealth.com/careers/ https://www.eversidehealth.com/careers/",3.2,1001 to 5000 Employees,2001,Company - Private,Health Care Services & Hospitals,Healthcare,Unknown / Non-Applicable
"San Jose, CA",USA - Infrastructure Data Engineer (AWS),$104K - $148K (Glassdoor est.),Avestacs,"Job Title: Infrastructure Data Engineer (AWS) Location: San Jose, California, United States Type: Fulltime  The candidate has to be in San Jose, CA and it is 100% onsite job”.  Roles & Responsibilities: Design, build and maintain data platform infrastructure on AWS environment. Oversee design, build, and maintain data platform infrastructure on AWS environment. Develop data pipelines to collect the metrics that is required to monitor data refreshes, reports deliveries and track SLAs. Work with the DWH development team and business users in establishing SLAs for data refreshes and reports. Build continuous integration/deployment (CI/CD) pipelines to accelerate development and improve team agility. Oversee project. Monitor all aspects of data platform system security, performance, storage, incidents, and usage for databases, data pipelines, applications, and infrastructure on AWS. Escalate to respective teams for fixes. Ensure data pipelines meet intraday and daily SLAs, as per documented SLA definitions and escalate accordingly. develop appropriate instrumentation to collect metrics on system performance, cost, data ingress/egress /storage processes. Have a clear understanding of the reports/analyses/insights to be driven by data and build data driven solutions to optimally support the operational analytics needs. Documents user stories, epics, and reports Coordinate infrastructure enhancements and maintenance with the system/network engineering teams Work with DWH development team and analytics team to do manual releases where required. Onboard users to data analytics systems with appropriate approvals Conduct system performance tests and collect metrics. Tune/add capacity. Complete knowledge management processes Own strategy and communicate potential major shifts in expected workload based on business, market, or operational changes. Operate ongoing business relationship management sessions to review operational metrics, understand pain points, identify upcoming projects and engagement efforts. Job Title: Infrastructure Data Engineer (AWS) Job Title: Location: San Jose, California, United States Location: Type: Fulltime Type: The candidate has to be in San Jose, CA and it is 100% onsite job”. The candidate has to be in San Jose, CA and it is 100% onsite job”. Roles & Responsibilities: Roles & Responsibilities: Design, build and maintain data platform infrastructure on AWS environment. Oversee design, build, and maintain data platform infrastructure on AWS environment. Develop data pipelines to collect the metrics that is required to monitor data refreshes, reports deliveries and track SLAs. Work with the DWH development team and business users in establishing SLAs for data refreshes and reports. Build continuous integration/deployment (CI/CD) pipelines to accelerate development and improve team agility. Oversee project. Monitor all aspects of data platform system security, performance, storage, incidents, and usage for databases, data pipelines, applications, and infrastructure on AWS. Escalate to respective teams for fixes. Ensure data pipelines meet intraday and daily SLAs, as per documented SLA definitions and escalate accordingly. develop appropriate instrumentation to collect metrics on system performance, cost, data ingress/egress /storage processes. Have a clear understanding of the reports/analyses/insights to be driven by data and build data driven solutions to optimally support the operational analytics needs. Documents user stories, epics, and reports Coordinate infrastructure enhancements and maintenance with the system/network engineering teams Work with DWH development team and analytics team to do manual releases where required. Onboard users to data analytics systems with appropriate approvals Conduct system performance tests and collect metrics. Tune/add capacity. Complete knowledge management processes Own strategy and communicate potential major shifts in expected workload based on business, market, or operational changes. Operate ongoing business relationship management sessions to review operational metrics, understand pain points, identify upcoming projects and engagement efforts. Design, build and maintain data platform infrastructure on AWS environment. Design, build and maintain data platform infrastructure on AWS environment. Oversee design, build, and maintain data platform infrastructure on AWS environment. infrastructure on AWS environment Develop data pipelines to collect the metrics that is required to monitor data refreshes, reports deliveries and track SLAs. Work with the DWH development team and business users in establishing SLAs for data refreshes and reports. Build continuous integration/deployment (CI/CD) pipelines to accelerate development and improve team agility. continuous integration/deployment (CI/CD) Oversee project. Monitor all aspects of data platform system security, performance, storage, incidents, and usage for databases, data pipelines, applications, and infrastructure on AWS. Escalate to respective teams for fixes. data platform system security, performance, storage, incidents usage Ensure data pipelines meet intraday and daily SLAs, as per documented SLA definitions and escalate accordingly. develop appropriate instrumentation to collect metrics on system performance, cost, data ingress/egress /storage processes. Have a clear understanding of the reports/analyses/insights to be driven by data and build data driven solutions to optimally support the operational analytics needs. Documents user stories, epics, and reports Coordinate infrastructure enhancements and maintenance with the system/network engineering teams Work with DWH development team and analytics team to do manual releases where required. DWH Onboard users to data analytics systems with appropriate approvals Conduct system performance tests and collect metrics. Tune/add capacity. Complete knowledge management processes Own strategy and communicate potential major shifts in expected workload based on business, market, or operational changes. Operate ongoing business relationship management sessions to review operational metrics, understand pain points, identify upcoming projects and engagement efforts.",N/A,51 to 200 Employees,N/A,Company - Public,N/A,N/A,Unknown / Non-Applicable
"New York, NY",Data Engineer,$105K - $148K (Glassdoor est.),Garda Capital Partners,"Garda Capital Partners (Garda) is a multi-billion dollar alternative investment firm with over 19 years of experience deploying relative value strategies across fixed income markets for institutional investors. Garda has offices in Wayzata, New York City, West Palm Beach, Geneva, Zug, Copenhagen, and Singapore.  Garda is looking to hire a Data Engineer to be a part of our Research and Technology (R&T) team based out of our New York office. The R&T team is responsible for all of the firm's applications & infrastructure including data management, analytics, portfolio and risk management. The selected candidate will be experienced in processing and manipulating data, developing scalable and performant data delivery systems, data APIs and platforms. This position requires proficiency and experience in software development using Python. While prior business knowledge is not mandatory, capital markets related experience will be a plus. Position Responsibilities Assist in developing and maintaining stable data pipeline to support Garda's trading and business activities Aid in the onboarding and processing of interesting new data sets, working with developers, traders, and portfolio managers across the organization Develop a keen understanding of the data and how it is being utilized in our systems, analytics, and investment process Qualifications & Desired Skills Bachelor's Degree in Computer Science, Engineering, Mathematics, or Finance Minimum 2 years' of development experience using either Python, C#/Java or C++ Minimum 2 years' years of database/SQL experience Familiarity with object-oriented programming concepts Must be comfortable with development across the application stack Ability to complete complex projects independently Detail-oriented with strong verbal and written communication skills Ability to work effectively in a high-energy, time sensitive team environment Interest or prior experience in the financial trading industry (particularly in fixed income) a plus This role is also eligible for other forms of compensation and benefits, such as a discretionary bonus, healthcare plan, 401(k) matching program, etc. Within the range, individual pay is determined by work location and additional factors, such as job-related skills, experience, and relevant education. Base Salary for this role is expected to be between: $110,000—$150,000 USD Garda Capital Partners (Garda) is a multi-billion dollar alternative investment firm with over 19 years of experience deploying relative value strategies across fixed income markets for institutional investors. Garda has offices in Wayzata, New York City, West Palm Beach, Geneva, Zug, Copenhagen, and Singapore. Garda Capital Partners (Garda) is a multi-billion dollar alternative investment firm with over 19 years of experience deploying relative value strategies across fixed income markets for institutional investors. Garda has offices in Wayzata, New York City, West Palm Beach, Geneva, Zug, Copenhagen, and Singapore. Garda is looking to hire a Data Engineer to be a part of our Research and Technology (R&T) team based out of our New York office. Data Engineer The R&T team is responsible for all of the firm's applications & infrastructure including data management, analytics, portfolio and risk management. The selected candidate will be experienced in processing and manipulating data, developing scalable and performant data delivery systems, data APIs and platforms. This position requires proficiency and experience in software development using Python. While prior business knowledge is not mandatory, capital markets related experience will be a plus. Position Responsibilities Position Responsibilities Assist in developing and maintaining stable data pipeline to support Garda's trading and business activities Aid in the onboarding and processing of interesting new data sets, working with developers, traders, and portfolio managers across the organization Develop a keen understanding of the data and how it is being utilized in our systems, analytics, and investment process Assist in developing and maintaining stable data pipeline to support Garda's trading and business activities Aid in the onboarding and processing of interesting new data sets, working with developers, traders, and portfolio managers across the organization Develop a keen understanding of the data and how it is being utilized in our systems, analytics, and investment process Qualifications & Desired Skills Qualifications & Desired Skills Bachelor's Degree in Computer Science, Engineering, Mathematics, or Finance Minimum 2 years' of development experience using either Python, C#/Java or C++ Minimum 2 years' years of database/SQL experience Familiarity with object-oriented programming concepts Must be comfortable with development across the application stack Ability to complete complex projects independently Detail-oriented with strong verbal and written communication skills Ability to work effectively in a high-energy, time sensitive team environment Interest or prior experience in the financial trading industry (particularly in fixed income) a plus Bachelor's Degree in Computer Science, Engineering, Mathematics, or Finance Minimum 2 years' of development experience using either Python, C#/Java or C++ Minimum 2 years' years of database/SQL experience Familiarity with object-oriented programming concepts Must be comfortable with development across the application stack Ability to complete complex projects independently Detail-oriented with strong verbal and written communication skills Ability to work effectively in a high-energy, time sensitive team environment Interest or prior experience in the financial trading industry (particularly in fixed income) a plus This role is also eligible for other forms of compensation and benefits, such as a discretionary bonus, healthcare plan, 401(k) matching program, etc. Within the range, individual pay is determined by work location and additional factors, such as job-related skills, experience, and relevant education. Base Salary for this role is expected to be between: $110,000—$150,000 USD This role is also eligible for other forms of compensation and benefits, such as a discretionary bonus, healthcare plan, 401(k) matching program, etc. Within the range, individual pay is determined by work location and additional factors, such as job-related skills, experience, and relevant education. This role is also eligible for other forms of compensation and benefits, such as a discretionary bonus, healthcare plan, 401(k) matching program, etc. Within the range, individual pay is determined by work location and additional factors, such as job-related skills, experience, and relevant education. Base Salary for this role is expected to be between: Base Salary for this role is expected to be between: $110,000—$150,000 USD",4.4,51 to 200 Employees,N/A,Company - Private,Investment & Asset Management,Financial Services,$5 to $25 million (USD)
"Santa Clara, CA",Data Center Engineer,$45.00 - $47.00 Per Hour (Employer est.),Cloud destinations,"Job Title: Data Center Engineer w/ Lab experience Job Title: Number of roles open: 3 Number of roles open: Location: Santa Clara, CA (100% onsite) Location: Contract Duration: 6 months to start – ongoing support past 6 months Contract Duration: Job Description Job Description Our client is working on state-of-the-srt data center equipment and they need technical help to test, troubleshoot, repair and optimize their next generation gear. Our client is working on state-of-the-srt data center equipment and they need technical help to test, troubleshoot, repair and optimize their next generation gear. Qualifications include: Qualifications include: Strong data center lab experience Minimum of 5 years on-prem large scale data center experience supporting the Data Center equipment and/or infrastructure (Power, Space, Cooling, Equipment) Strong attention to detail, independent and out of box thinking Solid skills in UNIX (Ubuntu or RedHat) administration and knowledge of basic commands including logging into the server’s to monitor for disk failures, etc. Linux scripting experience also a MUST Ability to debug server hardware Break/fix server expertise Build up prototype open board systems on the benchtop and debugging issues reported by users. Triage and debug systems and get systems to pass base line diagnostics. Strong data center lab experience lab Minimum of 5 years on-prem large scale data center experience supporting the Data Center equipment and/or infrastructure (Power, Space, Cooling, Equipment) Strong attention to detail, independent and out of box thinking Solid skills in UNIX (Ubuntu or RedHat) administration and knowledge of basic commands including logging into the server’s to monitor for disk failures, etc. Linux scripting experience also a MUST MUST Ability to debug server hardware Break/fix server expertise Build up prototype open board systems on the benchtop and debugging issues reported by users. Triage and debug systems and get systems to pass base line diagnostics. Job Type: Contract Salary: $45.00 - $47.00 per hour Work Location: On the road",4.8,201 to 500 Employees,2016,Company - Private,Enterprise Software & Network Solutions,Information Technology,$5 to $25 million (USD)
Remote,Python Data Engineer,N/A,Curotec,"About Curotec We have a really interesting team of people that span many different skill sets, cultures, and backgrounds. We are a melting pot of sorts. One of the things our team members enjoy the most about working at Curotec is their ability to learn new things every day, not only about technology but also about their fellow team members and their cultures.  About the Job We are looking for a highly-skilled remote data engineer that has experience with scripting in Python to pull in and normalize data from multiple varying APIs. Candidates should have experience with Python scripting, working with databases, working within the AWS cloud data tools, and working with standard developer tools. We are looking for a Senior Python Data Engineer; however, we're open to hiring an advanced mid-level engineer who is showing promising signs of being on their way to Senior.  Job Requirements 5+ years of experience with Python and data engineering. 3+ years of experience with AWS cloud data tools. Must be able to work EDT business hours. 8am - 5pm New York time. Must be able to speak and write fluent English. Must be dedicated, passionate, and hard-working. Must be able to work with a team and collaborate remotely daily. We are open to considering candidates with cloud experience outside of AWS. They must be open to learning AWS and can quickly comprehend how to navigate and leverage components within a different cloud platform. Technical Requirements Experience leading the development and maintenance of large-scale ETL environments. Advanced experience with using Python to parse and normalize complex data sets. Solid working experience building APIs, including REST and SOAP. Experience parsing XML and JSON data. Experience working with flat file data imports. Ability to recognize and solve issues in a Python script. Experience with testing and debugging ETL integration issues. Active experience integrating custom code with 3rd party web services. Experience leading complicated data migrations. Hands-on experience with tools such as Git and Jira. Experience working in the AWS (Amazon Web Services) ecosystem, primarily data-related AWS products. Experience working within Agile development environments. Experience with database profiling and query optimization is a plus.  Job Duties Work with a team of data engineers and integration team leaders to create and maintain dozens of ETL scripts and data pipeline tooling. Monitor and debug issues with existing data integration scripts. Review API changes and adjust code accordingly. Document your development process and development components. Collaborate daily with the team to communicate your tasks, challenges, and progress. Raise the flag quickly to team leads when issues occur that fall on your plate.  Why Work for Curotec Competitive salary Additional benefits offered Ability to grow and advance your career Attend virtual developer conferences Work on cutting-edge and exciting projects 2kp5tPAsVj About Curotec About Curotec We have a really interesting team of people that span many different skill sets, cultures, and backgrounds. We are a melting pot of sorts. One of the things our team members enjoy the most about working at Curotec is their ability to learn new things every day, not only about technology but also about their fellow team members and their cultures. About the Job About the Job We are looking for a highly-skilled remote data engineer that has experience with scripting in Python to pull in and normalize data from multiple varying APIs. Candidates should have experience with Python scripting, working with databases, working within the AWS cloud data tools, and working with standard developer tools. We are looking for a Senior Python Data Engineer; however, we're open to hiring an advanced mid-level engineer who is showing promising signs of being on their way to Senior. Job Requirements Job Requirements 5+ years of experience with Python and data engineering. 3+ years of experience with AWS cloud data tools. Must be able to work EDT business hours. 8am - 5pm New York time. Must be able to speak and write fluent English. Must be dedicated, passionate, and hard-working. Must be able to work with a team and collaborate remotely daily. 5+ years of experience with Python and data engineering. 3+ years of experience with AWS cloud data tools. Must be able to work EDT business hours. 8am - 5pm New York time. Must be able to speak and write fluent English. Must be dedicated, passionate, and hard-working. Must be able to work with a team and collaborate remotely daily. We are open to considering candidates with cloud experience outside of AWS. They must be open to learning AWS and can quickly comprehend how to navigate and leverage components within a different cloud platform. Technical Requirements Technical Requirements Experience leading the development and maintenance of large-scale ETL environments. Advanced experience with using Python to parse and normalize complex data sets. Solid working experience building APIs, including REST and SOAP. Experience parsing XML and JSON data. Experience working with flat file data imports. Ability to recognize and solve issues in a Python script. Experience with testing and debugging ETL integration issues. Active experience integrating custom code with 3rd party web services. Experience leading complicated data migrations. Hands-on experience with tools such as Git and Jira. Experience working in the AWS (Amazon Web Services) ecosystem, primarily data-related AWS products. Experience working within Agile development environments. Experience with database profiling and query optimization is a plus. Experience leading the development and maintenance of large-scale ETL environments. Advanced experience with using Python to parse and normalize complex data sets. Solid working experience building APIs, including REST and SOAP. Experience parsing XML and JSON data. Experience working with flat file data imports. Ability to recognize and solve issues in a Python script. Experience with testing and debugging ETL integration issues. Active experience integrating custom code with 3rd party web services. Experience leading complicated data migrations. Hands-on experience with tools such as Git and Jira. Experience working in the AWS (Amazon Web Services) ecosystem, primarily data-related AWS products. Experience working within Agile development environments. Experience with database profiling and query optimization is a plus. Job Duties Job Duties Work with a team of data engineers and integration team leaders to create and maintain dozens of ETL scripts and data pipeline tooling. Monitor and debug issues with existing data integration scripts. Review API changes and adjust code accordingly. Document your development process and development components. Collaborate daily with the team to communicate your tasks, challenges, and progress. Raise the flag quickly to team leads when issues occur that fall on your plate. Work with a team of data engineers and integration team leaders to create and maintain dozens of ETL scripts and data pipeline tooling. Monitor and debug issues with existing data integration scripts. Review API changes and adjust code accordingly. Document your development process and development components. Collaborate daily with the team to communicate your tasks, challenges, and progress. Raise the flag quickly to team leads when issues occur that fall on your plate. Why Work for Curotec Why Work for Curotec Competitive salary Additional benefits offered Ability to grow and advance your career Attend virtual developer conferences Work on cutting-edge and exciting projects Competitive salary Additional benefits offered Ability to grow and advance your career Attend virtual developer conferences Work on cutting-edge and exciting projects 2kp5tPAsVj",5.0,1 to 50 Employees,2010,Company - Private,Enterprise Software & Network Solutions,Information Technology,Unknown / Non-Applicable
"Washington, DC",Data Engineer,$112K - $127K (Employer est.),Smithsonian Institution,"Description  OPEN DATE: 8/11/2023 CLOSING DATE: 9/8/2023 SALARY RANGE: $112,015 - 126,949 POSITION TYPE: Trust Fund APPOINTMENT TYPE: Temporary SCHEDULE: Full Time DUTY LOCATION: Washington DC Non-Critical Sensitive/Moderate Risk Open to all qualified applicants  What are Trust Fund Positions? Trust Fund positions are unique to the Smithsonian. They are paid for from a variety of sources, including the Smithsonian endowment, revenue from our business activities, donations, grants and contracts. Trust employees are not part of the civil service, nor does trust fund employment lead to Federal status. The salary ranges for trust positions are generally the same as for federal positions and in many cases trust and federal employees work side by side. Trust employees have their own benefit program and may include Health, Dental & Vision Insurance, Life Insurance, Transit/Commuter Benefits, Accidental Death and Dismemberment Insurance, Annual and Sick Leave, Family Friendly Leave, 403b Retirement Plan, Discounts for Smithsonian Memberships, Museum Stores and Restaurants, Credit Union, Smithsonian Early Enrichment Center (Child Care), Flexible Spending Account (Health & Dependent Care). Conditions of Employment Pass Pre-employment Background Check and Subsequent Background Investigation for position designated as low risk. Complete a Probationary Period Maintain a Bank Account for Direct Deposit/Electronic Transfer. The position is open to all candidates eligible to work in the United States. Proof of eligibility to work in U.S. is not required to apply.  OVERVIEW  Come join a team of dedicated staff at an exceptional time in the Smithsonian’s history as we prepare for the Smithsonian Campaign for Our Shared Future. This ambitious fundraising campaign’s public phase will run from 2024 to 2026, culminating with the nation’s 250th anniversary. It will expand the Smithsonian’s reach and impact, empowering the institution to find solutions to today’s most pressing challenges. The Smithsonian is also planning two exciting new museums. The National Museum of the American Latino and the Smithsonian American Women's History Museum share the experiences and perspectives of Latinos and women across history and deepen our shared understanding of what it means to be an American. The Smithsonian has built a model fundraising organization, driven by talented staff across our many museums, research centers and cultural centers. This position offers exciting opportunities for the successful candidate to make a significant impact on the future of the Smithsonian. There is no better time to join this amazing Institution.  DUTIES AND RESPONSIBILITIES  The Smithsonian Institution’s Office of Advancement seeks a Data Engineer to serve as a key member of the Advancement Technology Services team as we transform our data management approach. The data engineer reports to the Systems Manager and works with the team in developing and supporting high-quality, secure systems and data pipelines. The Data Engineer: Designs, develops, documents, and implements data solutions (ELT/ETL pipelines, tools, datasets, etc.) that support a variety of products. Performs data profiling and analysis, data cleansing and validation in support of data solutions. Builds solutions that support real-time and batch data processing and adhere to data management, security, and privacy standards. Tests and promotes work packages between non-production and production environments both independently, and with team members. Collaborates with stakeholders (internal technical and business teams) to understand processes and data needs. Monitors and troubleshoots solutions - correcting failures, security, and performance issues in a timely manner. Creates and manages incident reports as they pertain to data management processes. Provides advanced technical support for production applications and dataflows.  QUALIFICATION REQUIREMENTS  Successful candidates will have: Demonstrated expertise in data management with large/complex databases using SQL and PL/SQL, stored procedures, indexing, triggers, and performance tuning. Demonstrated expertise building resilient data pipelines for system integration and for reporting/analytics purposes. Experience with Oracle, MS SQL Server, Hive, and cloud/serverless databases. Experience with one or more of the following data management and orchestration technologies – Azure Data Factory, SSIS, AWS Glue, Airflow, JSON, and APIs. Experience modeling datasets for, managing the configuration of, and supporting development in, business intelligence tools (e.g., Tableau, PowerBI). Experience with CRM-like and transactional datasets. Exposure to cloud services (e.g., AWS, Azure) and familiarity with serverless data architectures using those services. Proficiency in programming languages for data and analytics (e.g., SQL, Python and R). Strong analytical and problem-solving skills. Strong technical writing skills and experience creating procedural and audit documentation. Strong communication skills, presentation skills, and the ability to build relationships in a hybrid setting. Demonstrated ability to innovate and drive assigned tasks to successful completion. Curious, creative, and collaborative approach to challenges. Applicants, who wish to qualify based on education completed outside the United States, must be deemed equivalent to higher education programs of U.S. Institutions by an organization that specializes in the interpretation of foreign educational credentials. This documentation is the responsibility of the applicant and should be included as part of your application package. Any false statement in your application may result in your application being rejected and may also result in termination after employment begins. The Smithsonian Institution values and seeks a diverse workforce. Join us in ""Inspiring Generations through Knowledge and Discovery.""  Resumes should include a description of your paid and non-paid work experience that is related to this job; starting and ending dates of job (month and year); and average number of hours worked per week.  What To Expect Next: Once the vacancy announcement closes, a review of your resume will be compared against the qualification and experience requirements related to this job. After review of applicant resumes is complete, qualified candidates will be referred to the hiring manager. Relocation expenses are not paid.  The Smithsonian Institution provides reasonable accommodation to applicants with disabilities where appropriate. Applicants requiring reasonable accommodation should contact oastaffing@si.edu. Determinations on requests for reasonable accommodation will be made on a case-by-case basis. To learn more, please review the Smithsonian’s Accommodation Procedures. The Smithsonian Institution is an Equal Opportunity Employer. We believe that a workforce comprising a variety of educational, cultural, and experiential backgrounds support and enhance our daily work life and contribute to the richness of our exhibitions and programs. See Smithsonian EEO program information: www.si.edu/oeema.  About Smithsonian Institution Founded in 1846, the Smithsonian is the world’s largest museum and research complex of 19 museums and galleries, the National Zoological Park and nine research facilities. There are 6,000 Smithsonian employees, including approximately 500 scientists. The total number of objects, works of art and specimens at the Smithsonian is estimated at nearly 137 million. Description Description OPEN DATE: 8/11/2023 OPEN DATE: 8/11/2023 CLOSING DATE: 9/8/2023 CLOSING DATE: 9/8/2023 SALARY RANGE: $112,015 - 126,949 SALARY RANGE: $112,015 - 126,949 POSITION TYPE: Trust Fund POSITION TYPE: Trust Fund APPOINTMENT TYPE: Temporary APPOINTMENT TYPE: Temporary SCHEDULE: Full Time SCHEDULE: Full Time DUTY LOCATION: Washington DC Non-Critical Sensitive/Moderate Risk Open to all qualified applicants  What are Trust Fund Positions? DUTY LOCATION: Washington DC Non-Critical Sensitive/Moderate Risk Open to all qualified applicants What are Trust Fund Positions? Trust Fund positions are unique to the Smithsonian. They are paid for from a variety of sources, including the Smithsonian endowment, revenue from our business activities, donations, grants and contracts. Trust employees are not part of the civil service, nor does trust fund employment lead to Federal status. The salary ranges for trust positions are generally the same as for federal positions and in many cases trust and federal employees work side by side. Trust employees have their own benefit program and may include Health, Dental & Vision Insurance, Life Insurance, Transit/Commuter Benefits, Accidental Death and Dismemberment Insurance, Annual and Sick Leave, Family Friendly Leave, 403b Retirement Plan, Discounts for Smithsonian Memberships, Museum Stores and Restaurants, Credit Union, Smithsonian Early Enrichment Center (Child Care), Flexible Spending Account (Health & Dependent Care). Conditions of Employment Conditions of Employment Pass Pre-employment Background Check and Subsequent Background Investigation for position designated as low risk. Complete a Probationary Period Maintain a Bank Account for Direct Deposit/Electronic Transfer. The position is open to all candidates eligible to work in the United States. Proof of eligibility to work in U.S. is not required to apply. Pass Pre-employment Background Check and Subsequent Background Investigation for position designated as low risk. Complete a Probationary Period Maintain a Bank Account for Direct Deposit/Electronic Transfer. The position is open to all candidates eligible to work in the United States. Proof of eligibility to work in U.S. is not required to apply. OVERVIEW OVERVIEW Come join a team of dedicated staff at an exceptional time in the Smithsonian’s history as we prepare for the Smithsonian Campaign for Our Shared Future. This ambitious fundraising campaign’s public phase will run from 2024 to 2026, culminating with the nation’s 250th anniversary. It will expand the Smithsonian’s reach and impact, empowering the institution to find solutions to today’s most pressing challenges. The Smithsonian is also planning two exciting new museums. The National Museum of the American Latino and the Smithsonian American Women's History Museum share the experiences and perspectives of Latinos and women across history and deepen our shared understanding of what it means to be an American. The Smithsonian has built a model fundraising organization, driven by talented staff across our many museums, research centers and cultural centers. This position offers exciting opportunities for the successful candidate to make a significant impact on the future of the Smithsonian. There is no better time to join this amazing Institution. DUTIES AND RESPONSIBILITIES DUTIES AND RESPONSIBILITIES The Smithsonian Institution’s Office of Advancement seeks a Data Engineer to serve as a key member of the Advancement Technology Services team as we transform our data management approach. The data engineer reports to the Systems Manager and works with the team in developing and supporting high-quality, secure systems and data pipelines. The Data Engineer: Designs, develops, documents, and implements data solutions (ELT/ETL pipelines, tools, datasets, etc.) that support a variety of products. Performs data profiling and analysis, data cleansing and validation in support of data solutions. Builds solutions that support real-time and batch data processing and adhere to data management, security, and privacy standards. Tests and promotes work packages between non-production and production environments both independently, and with team members. Collaborates with stakeholders (internal technical and business teams) to understand processes and data needs. Monitors and troubleshoots solutions - correcting failures, security, and performance issues in a timely manner. Creates and manages incident reports as they pertain to data management processes. Provides advanced technical support for production applications and dataflows. Designs, develops, documents, and implements data solutions (ELT/ETL pipelines, tools, datasets, etc.) that support a variety of products. Performs data profiling and analysis, data cleansing and validation in support of data solutions. Builds solutions that support real-time and batch data processing and adhere to data management, security, and privacy standards. Tests and promotes work packages between non-production and production environments both independently, and with team members. Collaborates with stakeholders (internal technical and business teams) to understand processes and data needs. Monitors and troubleshoots solutions - correcting failures, security, and performance issues in a timely manner. Creates and manages incident reports as they pertain to data management processes. Provides advanced technical support for production applications and dataflows. QUALIFICATION REQUIREMENTS QUALIFICATION REQUIREMENTS Successful candidates will have: Demonstrated expertise in data management with large/complex databases using SQL and PL/SQL, stored procedures, indexing, triggers, and performance tuning. Demonstrated expertise building resilient data pipelines for system integration and for reporting/analytics purposes. Experience with Oracle, MS SQL Server, Hive, and cloud/serverless databases. Experience with one or more of the following data management and orchestration technologies – Azure Data Factory, SSIS, AWS Glue, Airflow, JSON, and APIs. Experience modeling datasets for, managing the configuration of, and supporting development in, business intelligence tools (e.g., Tableau, PowerBI). Experience with CRM-like and transactional datasets. Exposure to cloud services (e.g., AWS, Azure) and familiarity with serverless data architectures using those services. Proficiency in programming languages for data and analytics (e.g., SQL, Python and R). Strong analytical and problem-solving skills. Strong technical writing skills and experience creating procedural and audit documentation. Strong communication skills, presentation skills, and the ability to build relationships in a hybrid setting. Demonstrated ability to innovate and drive assigned tasks to successful completion. Curious, creative, and collaborative approach to challenges. Demonstrated expertise in data management with large/complex databases using SQL and PL/SQL, stored procedures, indexing, triggers, and performance tuning. Demonstrated expertise building resilient data pipelines for system integration and for reporting/analytics purposes. Experience with Oracle, MS SQL Server, Hive, and cloud/serverless databases. Experience with one or more of the following data management and orchestration technologies – Azure Data Factory, SSIS, AWS Glue, Airflow, JSON, and APIs. Experience modeling datasets for, managing the configuration of, and supporting development in, business intelligence tools (e.g., Tableau, PowerBI). Experience with CRM-like and transactional datasets. Exposure to cloud services (e.g., AWS, Azure) and familiarity with serverless data architectures using those services. Proficiency in programming languages for data and analytics (e.g., SQL, Python and R). Strong analytical and problem-solving skills. Strong technical writing skills and experience creating procedural and audit documentation. Strong communication skills, presentation skills, and the ability to build relationships in a hybrid setting. Demonstrated ability to innovate and drive assigned tasks to successful completion. Curious, creative, and collaborative approach to challenges. Applicants, who wish to qualify based on education completed outside the United States, must be deemed equivalent to higher education programs of U.S. Institutions by an organization that specializes in the interpretation of foreign educational credentials. This documentation is the responsibility of the applicant and should be included as part of your application package. Any false statement in your application may result in your application being rejected and may also result in termination after employment begins. The Smithsonian Institution values and seeks a diverse workforce. Join us in ""Inspiring Generations through Knowledge and Discovery."" Resumes should include a description of your paid and non-paid work experience that is related to this job; starting and ending dates of job (month and year); and average number of hours worked per week.  What To Expect Next: Once the vacancy announcement closes, a review of your resume will be compared against the qualification and experience requirements related to this job. After review of applicant resumes is complete, qualified candidates will be referred to the hiring manager. Relocation expenses are not paid.  The Smithsonian Institution provides reasonable accommodation to applicants with disabilities where appropriate. Applicants requiring reasonable accommodation should contact oastaffing@si.edu. Determinations on requests for reasonable accommodation will be made on a case-by-case basis. To learn more, please review the Smithsonian’s Accommodation Procedures. The Smithsonian Institution is an Equal Opportunity Employer. We believe that a workforce comprising a variety of educational, cultural, and experiential backgrounds support and enhance our daily work life and contribute to the richness of our exhibitions and programs. See Smithsonian EEO program information: www.si.edu/oeema. About Smithsonian Institution About Smithsonian Institution Founded in 1846, the Smithsonian is the world’s largest museum and research complex of 19 museums and galleries, the National Zoological Park and nine research facilities. There are 6,000 Smithsonian employees, including approximately 500 scientists. The total number of objects, works of art and specimens at the Smithsonian is estimated at nearly 137 million.",3.9,5001 to 10000 Employees,1846,Nonprofit Organization,Civic & Social Services,Nonprofit & NGO,$1 to $5 billion (USD)
"Chicago, IL",Data Engineer,$83K - $115K (Glassdoor est.),Prescient,"TITLE: Data Engineer  PRACTICE AREA: Cyber  LOCATION: All Offices or Remote  REPORTS TO: Managing Director & Practice Lead  FLSA: Exempt  WHY PRESCIENT?  Prescient is a global risk management and intelligence services firm. Our Due Diligence, Investigations, Cyber, and Intelligence Practices help Fortune 500 companies, law firms, and financial institutions mitigate risk and uncover mission-critical information. Headquartered in Chicago, IL with offices in Arlington, VA; New York, NY; Los Angeles, CA and Dublin, Ireland, Prescient’s team of former military personnel, intelligence officers, law enforcement agents, and corporate investigators is proficient in multiple foreign languages and has decades of experience conducting due diligence, corporate investigations, and intelligence collection operations in over 110 countries. We provide stakeholders with actionable information to address challenges related to compliance, investments, physical & cyber security, and litigation, among others.  POSITION SUMMARY  The Data Engineer will play a critical role in designing, building, and maintaining our in-house dark web and open-source data repositories, search applications, and APIs. They will also work closely with a cross-functional team to improve and maintain the reliability, scalability, and performance of these tools and technologies. Expertise in data modeling, infrastructure design, and data integration will be essential in driving our search application development and enhancing our data.  ESSENTIAL JOB FUNCTIONS  Data Engineers will generally:  Design and develop scalable search applications to enable efficient data retrieval and indexing from the deep and dark web. Work with internal and external stakeholders to optimize data infrastructure and identify cost savings, where possible. Build and maintain data pipelines to ingest, transform, and process large volumes of open, deep, and dark web data from diverse sources. Develop and maintain API endpoints for querying dark web data, ensuring efficient and reliable access to our systems. Monitor and optimize search performance, address bottlenecks, and implement enhancements. Implement data quality and validation measures to ensure accuracy and integrity of indexed data. Identify and integrate optimal database solutions.  DESIRED QUALIFICATIONS  Proficiency in working with large-scale data processing frameworks, especially Elasticsearch Solid understanding of data modeling and schema design principles for efficient search and retrieval Familiarity with open source intelligence (OSINT) and/or dark web intelligence collection and/or processes Experience with API development and management, including authentication, versioning, and performance optimization Proven experience as a Data Engineer, preferably in the development and management of search engines and APIs Demonstrated knowledge of cloud platforms, especially AWS and Azure Excellent communication and collaboration skills to work effectively in a cross-functional team environment Familiarity with building CI/CD pipelines to ensure the delivery of high-quality software Knowledge of data privacy and security considerations when working with sensitive data Strong programming skills in Python  EDUCATION & EXPERIENCE REQUIREMENTS  To perform this job successfully, an individual must be able to perform each essential duty satisfactorily. Reasonable accommodations may be made to enable individuals with disabilities to perform essential functions.  While not required, candidates will typically have a Bachelor's or Master's degree in Computer Science, Data Science, or a related field.  PHYSICAL & SAFETY EXPECTATIONS  When physical requirements are not essential job functions, reasonable accommodations may be made for individuals with disabilities.  Prolonged periods of sitting at a desk and working on a computer. Must be able to lift up to 5 pounds at times. Ability to see, hear and speak continuously at a level to meet all essential functions of the job. Physical work is of light demand. Work is of high attention and mental demands including the ability to prioritize and process with accuracy.  HOW TO APPLY  Resume Cover Letter  You will receive an auto reply confirming that we’ve received your application. NO FOLLOW UP PHONE CALLS OR EMAILS, PLEASE. We pride ourselves on responding to every applicant, but that sort of diligent review takes time. If you receive a confirmation message from our system, we’ll get back to you soon.  Prescient provides equal employment opportunities to all employees and applicants. TITLE: Data Engineer TITLE PRACTICE AREA: Cyber PRACTICE AREA: LOCATION: All Offices or Remote LOCATION: REPORTS TO: Managing Director & Practice Lead REPORTS TO: FLSA: Exempt FLSA: WHY PRESCIENT? WHY PRESCIENT? Prescient is a global risk management and intelligence services firm. Our Due Diligence, Investigations, Cyber, and Intelligence Practices help Fortune 500 companies, law firms, and financial institutions mitigate risk and uncover mission-critical information. Headquartered in Chicago, IL with offices in Arlington, VA; New York, NY; Los Angeles, CA and Dublin, Ireland, Prescient’s team of former military personnel, intelligence officers, law enforcement agents, and corporate investigators is proficient in multiple foreign languages and has decades of experience conducting due diligence, corporate investigations, and intelligence collection operations in over 110 countries. We provide stakeholders with actionable information to address challenges related to compliance, investments, physical & cyber security, and litigation, among others. POSITION SUMMARY POSITION SUMMARY The Data Engineer will play a critical role in designing, building, and maintaining our in-house dark web and open-source data repositories, search applications, and APIs. They will also work closely with a cross-functional team to improve and maintain the reliability, scalability, and performance of these tools and technologies. Expertise in data modeling, infrastructure design, and data integration will be essential in driving our search application development and enhancing our data. ESSENTIAL JOB FUNCTIONS ESSENTIAL JOB FUNCTIONS Data Engineers will generally: Design and develop scalable search applications to enable efficient data retrieval and indexing from the deep and dark web. Work with internal and external stakeholders to optimize data infrastructure and identify cost savings, where possible. Build and maintain data pipelines to ingest, transform, and process large volumes of open, deep, and dark web data from diverse sources. Develop and maintain API endpoints for querying dark web data, ensuring efficient and reliable access to our systems. Monitor and optimize search performance, address bottlenecks, and implement enhancements. Implement data quality and validation measures to ensure accuracy and integrity of indexed data. Identify and integrate optimal database solutions. Design and develop scalable search applications to enable efficient data retrieval and indexing from the deep and dark web. Work with internal and external stakeholders to optimize data infrastructure and identify cost savings, where possible. Build and maintain data pipelines to ingest, transform, and process large volumes of open, deep, and dark web data from diverse sources. Develop and maintain API endpoints for querying dark web data, ensuring efficient and reliable access to our systems. Monitor and optimize search performance, address bottlenecks, and implement enhancements. Implement data quality and validation measures to ensure accuracy and integrity of indexed data. Identify and integrate optimal database solutions. DESIRED QUALIFICATIONS DESIRED QUALIFICATIONS Proficiency in working with large-scale data processing frameworks, especially Elasticsearch Solid understanding of data modeling and schema design principles for efficient search and retrieval Proficiency in working with large-scale data processing frameworks, especially Elasticsearch Solid understanding of data modeling and schema design principles for efficient search and retrieval Familiarity with open source intelligence (OSINT) and/or dark web intelligence collection and/or processes Familiarity with open source intelligence (OSINT) and/or dark web intelligence collection and/or processes Experience with API development and management, including authentication, versioning, and performance optimization Proven experience as a Data Engineer, preferably in the development and management of search engines and APIs Demonstrated knowledge of cloud platforms, especially AWS and Azure Excellent communication and collaboration skills to work effectively in a cross-functional team environment Familiarity with building CI/CD pipelines to ensure the delivery of high-quality software Knowledge of data privacy and security considerations when working with sensitive data Strong programming skills in Python Experience with API development and management, including authentication, versioning, and performance optimization Proven experience as a Data Engineer, preferably in the development and management of search engines and APIs Demonstrated knowledge of cloud platforms, especially AWS and Azure Excellent communication and collaboration skills to work effectively in a cross-functional team environment Familiarity with building CI/CD pipelines to ensure the delivery of high-quality software Knowledge of data privacy and security considerations when working with sensitive data Strong programming skills in Python EDUCATION & EXPERIENCE REQUIREMENTS EDUCATION & EXPERIENCE REQUIREMENTS To perform this job successfully, an individual must be able to perform each essential duty satisfactorily. Reasonable accommodations may be made to enable individuals with disabilities to perform essential functions. While not required, candidates will typically have a Bachelor's or Master's degree in Computer Science, Data Science, or a related field. PHYSICAL & SAFETY EXPECTATIONS PHYSICAL & SAFETY EXPECTATIONS When physical requirements are not essential job functions, reasonable accommodations may be made for individuals with disabilities. Prolonged periods of sitting at a desk and working on a computer. Must be able to lift up to 5 pounds at times. Ability to see, hear and speak continuously at a level to meet all essential functions of the job. Physical work is of light demand. Work is of high attention and mental demands including the ability to prioritize and process with accuracy. Prolonged periods of sitting at a desk and working on a computer. Must be able to lift up to 5 pounds at times. Ability to see, hear and speak continuously at a level to meet all essential functions of the job. Physical work is of light demand. Work is of high attention and mental demands including the ability to prioritize and process with accuracy. HOW TO APPLY HOW TO APPLY Resume Cover Letter You will receive an auto reply confirming that we’ve received your application. NO FOLLOW UP PHONE CALLS OR EMAILS, PLEASE. We pride ourselves on responding to every applicant, but that sort of diligent review takes time. If you receive a confirmation message from our system, we’ll get back to you soon. Prescient provides equal employment opportunities to all employees and applicants. Prescient provides equal employment opportunities to all employees and applicants. Prescient provides equal employment opportunities to all employees and applicants.",4.4,51 to 200 Employees,1996,Company - Private,Information Technology Support Services,Information Technology,Unknown / Non-Applicable
Remote,Senior Data Engineer,$75.00 Per Hour (Employer est.),hire IT people,"Position- Data Engineer Location- San Francisco, CA- Remote Duration- 7 Months Position- Data Engineer Location- San Francisco, CA- Remote Duration- 7 Months MUST HAVE MUST HAVE Kubernetes – Very Strong and #1 – 4 to 5 years Data Pipelines – ETL Preferred – Bring data to send back to other team Understanding of Python is good and will code in Python – Not working on API’s Good Understanding of Machine Learning Pipelines Argo WorkFlow Experience Docker and Jenkins – Would be good Workflow Experience – Would be good Job Description: Job Description: Can you please provide a summary of the project/initiatives which describes what’s being done? Will be part of Data Engineering Software Product development team and will be playing a role in Designing and developing high-volume, low-latency stream applications for mission-critical systems. Can you please provide a summary of the project/initiatives which describes what’s being done? Will be part of Data Engineering Software Product development team and will be playing a role in Designing and developing high-volume, low-latency stream applications for mission-critical systems. • What are the top 5-10 responsibilities for this position? (Please be detailed as to what the candidate is expected to do or complete on a daily basis) o Responsible for developing software product which requires Spark on K8s with scala & Python programming skills. o Responsible for building test automation suite o Refactoring of existing code base to the high standards and detect hotspots. o Responsible for enhancing the code base and meet the code coverage of 80% o Responsible for supporting end customer use cases, identify gaps, bugs and new features required to fulfil end customer requirement. o Responsible for building automation utilities to reduce redundant work. • What skills/technologies are required (please include the number of years of experience required)? o Spark on K8s o Cloud knowledge o Spark with scala/Java o Python to support/contribute to Airflow orchestration. o ETL knowledge o Spark structured streaming o Knowledge on Kafka o Knowledge on Splunk • What are the top 5-10 responsibilities for this position? (Please be detailed as to what the candidate is expected to do or complete on a daily basis) • What skills/technologies are required (please include the number of years of experience required)? What skills/attributes are preferred (these are a desired, not required)? What skills/attributes are preferred (these are a desired, not required)? o Apache Flink for real-time streaming. o Azure knowledge o Springboot microservice knowledge What does the interview process look like? What does the interview process look like? o How many rounds? 1 o Video, phone, or in person? Video o How technical will the interviews be? Will focus on all the required skills shared above. Participates in the technical design of application systems. Develops and implements application systems by participating through the software development lifecycle from inception to delivery and beyond. The role is high touch position with a notable amount of collaboration across product teams and stakeholders to define requirements and understand how they fit into the end objective. ESSENTIAL FUNCTIONS: Designs and writes complex code in several languages relevant to our existing product stack, with a focus on automation Configures, tunes, maintains and installs applications systems and validates system functionality Installs new software releases and application system upgrades. Evaluates and installs software patches Monitors and fine tunes applications system to achieve optimum performance levels and works with hardware teams to resolve issues with hardware and software Assists with application system problem resolution by working with application developers, vendors, and internal infrastructure teams member to troubleshoot Addresses product backlog and provide continuous delivery of high-quality features Maintains a comprehensive operating system hardware and software configuration database/library of all supporting documentation to ensure data integrity Acts to improve the overall reliability of systems and to increase efficiency Works collaboratively with cross functional teams, using Agile / DevOps principles to bring products to life, achieve business objectives and serve customer needs ESSENTIAL FUNCTIONS: Job Type: Full-time Salary: From $75.00 per hour Experience level: 10 years 8 years 9 years 10 years 8 years 9 years Schedule: 8 hour shift 8 hour shift Experience: Informatica: 1 year (Preferred) SQL: 1 year (Preferred) Data warehouse: 1 year (Preferred) Informatica: 1 year (Preferred) SQL: 1 year (Preferred) Data warehouse: 1 year (Preferred) Work Location: Remote",4.4,51 to 200 Employees,1998,Company - Private,HR Consulting,Human Resources & Staffing,Unknown / Non-Applicable
